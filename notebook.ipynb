{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad860f77",
   "metadata": {},
   "source": [
    "## PRIMER PASO: BARAJEAR DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ksk6quf69i8",
   "metadata": {},
   "source": [
    "## SETUP - Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1wi59fg8n9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "!pip install deep-translator pandas tqdm seaborn nltk scikit-learn gensim matplotlib numpy transformers torch\n",
    "\n",
    "print(\"‚úì Todas las dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_folders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpetas para organizar los archivos del proyecto\n",
    "import os\n",
    "\n",
    "folders = ['data', 'data_processed', 'models', 'charts']\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "print(\"‚úì Estructura de carpetas creada:\")\n",
    "print(\"  üìÅ data/              - Datasets originales\")\n",
    "print(\"  üìÅ data_processed/    - Datos procesados (CSV)\")\n",
    "print(\"  üìÅ models/            - Modelos entrenados\")\n",
    "print(\"  üìÅ charts/            - Gr√°ficos generados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0d82a",
   "metadata": {},
   "source": [
    "## Mezclado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa38f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Cargar dataset original\n",
    "df = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "print(f\"Dataset original cargado: {len(df)} noticias\")\n",
    "print(f\"Distribuci√≥n de sentimientos:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Crear copia para noticias correctas (50%)\n",
    "df_correctas = df.copy()\n",
    "df_correctas['Etiqueta'] = 'correcta'\n",
    "\n",
    "# Separar noticias por sentimiento\n",
    "df_positive = df[df['Sentiment'] == 'positive']\n",
    "df_negative = df[df['Sentiment'] == 'negative']\n",
    "\n",
    "print(f\"\\nNoticias positivas disponibles: {len(df_positive)}\")\n",
    "print(f\"Noticias negativas disponibles: {len(df_negative)}\")\n",
    "\n",
    "# Extraer fragmentos de una noticia dividiendo por comas o puntos\n",
    "def extraer_fragmentos(noticia):\n",
    "    fragmentos = [p.strip() for p in noticia.split(',') if p.strip()]\n",
    "    \n",
    "    if len(fragmentos) <= 1:\n",
    "        fragmentos = [p.strip() for p in noticia.split('.') if p.strip()]\n",
    "    \n",
    "    return fragmentos\n",
    "\n",
    "# Crear noticias incorrectas mezclando fragmentos de sentimientos opuestos\n",
    "def crear_mezclas_opuestas(df_positive, df_negative, num_mezclas):\n",
    "    noticias_mezcladas = []\n",
    "    \n",
    "    print(f\"\\nCreando {num_mezclas} mezclas de sentimientos opuestos...\")\n",
    "    \n",
    "    for i in range(num_mezclas):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Procesadas {i + 1}/{num_mezclas}...\")\n",
    "        \n",
    "        # Seleccionar una noticia positiva y una negativa aleatoriamente\n",
    "        idx_pos = random.randint(0, len(df_positive) - 1)\n",
    "        idx_neg = random.randint(0, len(df_negative) - 1)\n",
    "        \n",
    "        noticia_pos = df_positive.iloc[idx_pos]['Sentence']\n",
    "        noticia_neg = df_negative.iloc[idx_neg]['Sentence']\n",
    "        \n",
    "        # Extraer fragmentos de cada noticia\n",
    "        frag_pos = extraer_fragmentos(noticia_pos)\n",
    "        frag_neg = extraer_fragmentos(noticia_neg)\n",
    "        \n",
    "        # Seleccionar 1-2 fragmentos de cada noticia\n",
    "        if len(frag_pos) > 0 and len(frag_neg) > 0:\n",
    "            num_pos = min(random.randint(1, 2), len(frag_pos))\n",
    "            num_neg = min(random.randint(1, 2), len(frag_neg))\n",
    "            \n",
    "            fragmentos_seleccionados = (\n",
    "                random.sample(frag_pos, num_pos) + \n",
    "                random.sample(frag_neg, num_neg)\n",
    "            )\n",
    "            \n",
    "            # Mezclar el orden de los fragmentos\n",
    "            random.shuffle(fragmentos_seleccionados)\n",
    "            \n",
    "            noticia_mezclada = ', '.join(fragmentos_seleccionados)\n",
    "            if noticia_mezclada and noticia_mezclada[-1] not in '.!?':\n",
    "                noticia_mezclada += '.'\n",
    "            \n",
    "            # Asignar sentimiento aleatorio\n",
    "            sentiment = random.choice(['positive', 'negative'])\n",
    "            \n",
    "            noticias_mezcladas.append({\n",
    "                'Sentence': noticia_mezclada,\n",
    "                'Sentiment': sentiment,\n",
    "                'Etiqueta': 'incorrecta'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(noticias_mezcladas)\n",
    "\n",
    "# Crear el mismo n√∫mero de noticias incorrectas que correctas\n",
    "num_mezclas = len(df_correctas)\n",
    "df_incorrectas = crear_mezclas_opuestas(df_positive, df_negative, num_mezclas)\n",
    "\n",
    "# Combinar datasets\n",
    "df_final = pd.concat([df_correctas, df_incorrectas], ignore_index=True)\n",
    "\n",
    "# Mezclar aleatoriamente\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Mostrar estad√≠sticas\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ESTAD√çSTICAS DEL DATASET FINAL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de filas: {len(df_final)}\")\n",
    "print(f\"Noticias correctas: {len(df_final[df_final['Etiqueta'] == 'correcta'])} ({len(df_final[df_final['Etiqueta'] == 'correcta'])/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Noticias incorrectas: {len(df_final[df_final['Etiqueta'] == 'incorrecta'])} ({len(df_final[df_final['Etiqueta'] == 'incorrecta'])/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de sentimientos en noticias CORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'correcta']['Sentiment'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de sentimientos en noticias INCORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'incorrecta']['Sentiment'].value_counts())\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EJEMPLOS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n[NOTICIA CORRECTA]\")\n",
    "ejemplo_correcta = df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_correcta[:300] + (\"...\" if len(ejemplo_correcta) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 1]\")\n",
    "ejemplo_inc1 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_inc1[:300] + (\"...\" if len(ejemplo_inc1) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 2]\")\n",
    "ejemplo_inc2 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentiment']}\")\n",
    "print(ejemplo_inc2[:300] + (\"...\" if len(ejemplo_inc2) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 3]\")\n",
    "ejemplo_inc3 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentiment']}\")\n",
    "print(ejemplo_inc3[:300] + (\"...\" if len(ejemplo_inc3) > 300 else \"\"))\n",
    "\n",
    "# Guardar dataset final\n",
    "output_path = 'data/dataset_mezclado_final.csv'\n",
    "df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Dataset guardado como '{output_path}'\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bee4a",
   "metadata": {},
   "source": [
    "## TRADUCCI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir el dataset a m√∫ltiples idiomas usando Google Translator\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv('data/dataset_mezclado_final.csv')\n",
    "\n",
    "# Idiomas disponibles\n",
    "idiomas = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'eu', 'gl']\n",
    "\n",
    "nombres_idiomas = {\n",
    "    'es': 'espa√±ol',\n",
    "    'en': 'ingl√©s',\n",
    "    'fr': 'franc√©s',\n",
    "    'de': 'alem√°n',\n",
    "    'it': 'italiano',\n",
    "    'pt': 'portugu√©s',\n",
    "    'ca': 'catal√°n',\n",
    "    'eu': 'euskera',\n",
    "    'gl': 'gallego'\n",
    "}\n",
    "\n",
    "df_traducido = df.copy()\n",
    "df_traducido['Idioma'] = ''\n",
    "\n",
    "# Traducir texto con manejo de errores\n",
    "def traducir_texto(texto, idioma_destino):\n",
    "    try:\n",
    "        if idioma_destino == 'es':\n",
    "            return texto\n",
    "        translator = GoogleTranslator(source='es', target=idioma_destino)\n",
    "        traduccion = translator.translate(texto)\n",
    "        time.sleep(0.5)\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        print(f\"Error traduciendo a {idioma_destino}: {e}\")\n",
    "        return texto\n",
    "\n",
    "# Traducir solo la columna 'Sentence' a un idioma aleatorio por fila\n",
    "print(\"Iniciando traducci√≥n del dataset...\")\n",
    "print(f\"Total de filas a procesar: {len(df_traducido)}\")\n",
    "\n",
    "for idx in tqdm(range(len(df_traducido))):\n",
    "    # Seleccionar idioma aleatorio (mayor probabilidad para espa√±ol)\n",
    "    idioma_elegido = random.choice(idiomas + ['es', 'es', 'es'])\n",
    "    \n",
    "    # Traducir la columna 'Sentence'\n",
    "    texto_original = df_traducido.loc[idx, 'Sentence']\n",
    "    df_traducido.loc[idx, 'Sentence'] = traducir_texto(texto_original, idioma_elegido)\n",
    "    df_traducido.loc[idx, 'Idioma'] = nombres_idiomas[idioma_elegido]\n",
    "\n",
    "# Guardar dataset traducido\n",
    "output_path = 'data/dataset_multiidioma.csv'\n",
    "df_traducido.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úì Dataset traducido guardado en: {output_path}\")\n",
    "print(f\"Total de filas: {len(df_traducido)}\")\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "print(df_traducido.head())\n",
    "print(f\"\\nDistribuci√≥n de idiomas:\")\n",
    "print(df_traducido['Idioma'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549c3f4",
   "metadata": {},
   "source": [
    "## 1. An√°lisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Iniciando An√°lisis Exploratorio de Datos...\")\n",
    "try:\n",
    "    df = pd.read_csv('data/dataset_multiidioma.csv', encoding='utf-8')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: No se encontr√≥ 'dataset_multiidioma.csv'.\")\n",
    "    exit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" RESUMEN DEL DATASET \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica del dataset\n",
    "print(\"\\n1. ESTRUCTURA Y CONTEO DE VALORES NO NULOS:\")\n",
    "df.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n2. ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "print(\"-\" * 60)\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Distribuciones de variables categ√≥ricas\n",
    "print(\"\\n3. DISTRIBUCIONES CLAVE (Conteo y Porcentaje):\")\n",
    "for col in ['Sentiment', 'Etiqueta', 'Idioma']:\n",
    "    counts = df[col].value_counts().rename('Conteo')\n",
    "    percents = df[col].value_counts(normalize=True).mul(100).round(2).rename('Porcentaje (%)')\n",
    "    print(f\"\\n--- Distribuci√≥n de: {col} ---\")\n",
    "    print(pd.concat([counts, percents], axis=1))\n",
    "\n",
    "# Calcular longitud de oraciones en palabras\n",
    "df['sentence_length'] = df['Sentence'].str.split().str.len()\n",
    "print(\"\\n4. ESTAD√çSTICAS DE LONGITUD DE ORACIONES (En palabras):\")\n",
    "print(\"-\" * 60)\n",
    "print(df['sentence_length'].describe().round(2))\n",
    "\n",
    "# An√°lisis de valores nulos\n",
    "print(\"\\n5. AN√ÅLISIS DE VALORES NULOS:\")\n",
    "print(\"-\" * 60)\n",
    "nulos = df.isnull().sum()\n",
    "if nulos.sum() == 0:\n",
    "    print(\"‚úì No hay valores nulos en el dataset\")\n",
    "else:\n",
    "    print(nulos[nulos > 0])\n",
    "\n",
    "# An√°lisis de duplicados\n",
    "print(\"\\n6. AN√ÅLISIS DE DUPLICADOS:\")\n",
    "print(\"-\" * 60)\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {duplicados} ({duplicados/len(df)*100:.2f}%)\")\n",
    "if duplicados > 0:\n",
    "    print(f\"Filas √∫nicas: {len(df) - duplicados}\")\n",
    "\n",
    "# An√°lisis cruzado entre variables\n",
    "print(\"\\n7. AN√ÅLISIS CRUZADO: Sentiment vs Etiqueta\")\n",
    "print(\"-\" * 60)\n",
    "crosstab = pd.crosstab(df['Sentiment'], df['Etiqueta'], margins=True)\n",
    "print(\"\\nConteo absoluto:\")\n",
    "print(crosstab)\n",
    "print(\"\\nPorcentaje por fila:\")\n",
    "crosstab_pct = pd.crosstab(df['Sentiment'], df['Etiqueta'], normalize='index') * 100\n",
    "print(crosstab_pct.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd023f0",
   "metadata": {},
   "source": [
    "## 2. Visualizaciones del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones del dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZACIONES DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Distribuciones principales\n",
    "print(\"\\n1. DISTRIBUCIONES PRINCIPALES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Distribuciones del Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Gr√°fico de sentimientos\n",
    "sns.countplot(x='Sentiment', data=df, ax=axes[0], order=df['Sentiment'].value_counts().index)\n",
    "axes[0].set_title('Distribuci√≥n de Sentimientos')\n",
    "axes[0].set_xlabel('Sentimiento')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Gr√°fico de idiomas (top 5)\n",
    "top_idiomas = df['Idioma'].value_counts().head(5).index\n",
    "df_top_idiomas = df[df['Idioma'].isin(top_idiomas)]\n",
    "sns.countplot(data=df_top_idiomas, y='Idioma', order=top_idiomas, ax=axes[1], palette='viridis')\n",
    "axes[1].set_title('Top 5 Idiomas')\n",
    "axes[1].set_xlabel('Frecuencia')\n",
    "\n",
    "# Gr√°fico de etiquetas\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "sns.countplot(data=df, x='Etiqueta', ax=axes[2], palette=colors)\n",
    "axes[2].set_title('Distribuci√≥n de Etiquetas')\n",
    "axes[2].set_xlabel('Etiqueta')\n",
    "axes[2].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/01_distribuciones.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 01_distribuciones.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. An√°lisis cruzado\n",
    "print(\"\\n2. AN√ÅLISIS CRUZADO\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "fig.suptitle('Relaci√≥n Sentimiento vs Etiqueta', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Crear heatmap de porcentajes\n",
    "pivot_pct = pd.crosstab(df['Sentiment'], df['Etiqueta'], normalize='index') * 100\n",
    "sns.heatmap(pivot_pct, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax, \n",
    "            cbar_kws={'label': 'Porcentaje (%)'})\n",
    "ax.set_title('Porcentaje por Sentimiento')\n",
    "ax.set_xlabel('Etiqueta')\n",
    "ax.set_ylabel('Sentimiento')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/02_heatmap_sentiment_etiqueta.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 02_heatmap_sentiment_etiqueta.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. An√°lisis de longitud de oraciones\n",
    "print(\"\\n3. AN√ÅLISIS DE LONGITUD\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('An√°lisis de Longitud de Oraciones', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histograma de longitudes\n",
    "sns.histplot(df['sentence_length'], bins=30, kde=True, ax=axes[0])\n",
    "media = df['sentence_length'].mean()\n",
    "axes[0].axvline(media, color='red', linestyle='--', label=f'Media: {media:.1f}')\n",
    "axes[0].set_title('Distribuci√≥n de Longitud')\n",
    "axes[0].set_xlabel('N√∫mero de Palabras')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot por sentimiento\n",
    "sns.boxplot(x='Sentiment', y='sentence_length', data=df, ax=axes[1])\n",
    "axes[1].set_title('Longitud por Sentimiento')\n",
    "axes[1].set_xlabel('Sentimiento')\n",
    "axes[1].set_ylabel('N√∫mero de Palabras')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/03_analisis_longitud.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 03_analisis_longitud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN DE VISUALIZACIONES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì 01_distribuciones.png\")\n",
    "print(\"‚úì 02_heatmap_sentiment_etiqueta.png\")\n",
    "print(\"‚úì 03_analisis_longitud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34d558",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos - Tokenizaci√≥n y Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "try:\n",
    "    print(\"Verificando recursos NLTK esenciales...\")\n",
    "    nltk.download('punkt', quiet=True, raise_on_error=False)\n",
    "    nltk.download('stopwords', quiet=True, raise_on_error=False)\n",
    "    print(\"‚úì Recursos NLTK (punkt, stopwords) listos.\")\n",
    "except Exception:\n",
    "    print(\"ATENCI√ìN: La descarga de recursos de NLTK fall√≥. Usaremos tokenizaci√≥n simple.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Tokenizaci√≥n simple como alternativa\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data = 'data/dataset_multiidioma.csv'\n",
    "\n",
    "# Cargar stopwords en m√∫ltiples idiomas\n",
    "try:\n",
    "    STOPWORDS_ES = set(stopwords.words('spanish'))\n",
    "    STOPWORDS_EN = set(stopwords.words('english'))\n",
    "    STOPWORDS_ALL = STOPWORDS_ES.union(STOPWORDS_EN) \n",
    "except LookupError:\n",
    "    STOPWORDS_ALL = set()\n",
    "\n",
    "df_processed = pd.read_csv(data).copy()\n",
    "\n",
    "# Preprocesar texto: limpieza, tokenizaci√≥n y eliminaci√≥n de stopwords\n",
    "def preprocess_text(text, stopwords_set, min_len=2):\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return []\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Eliminar URLs y menciones\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenizar y convertir a min√∫sculas\n",
    "    try:\n",
    "        tokens = word_tokenize(text.lower()) \n",
    "    except LookupError:\n",
    "        tokens = simple_tokenize(text)\n",
    "\n",
    "    # Eliminar puntuaci√≥n, stopwords y tokens cortos\n",
    "    tokens_final = [\n",
    "        token for token in tokens\n",
    "        if token not in string.punctuation \n",
    "        and token not in stopwords_set\n",
    "        and len(token) > min_len\n",
    "    ]\n",
    "    \n",
    "    return tokens_final\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "print(\"\\n1. APLICANDO PROCESO DE LIMPIEZA...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_processed['tokens_processed'] = df_processed['Sentence'].apply(\n",
    "    lambda x: preprocess_text(x, STOPWORDS_ALL)\n",
    ")\n",
    "\n",
    "df_processed['tokens_original'] = df_processed['Sentence'].apply(\n",
    "     lambda x: simple_tokenize(str(x)) if not pd.isna(x) else []\n",
    ")\n",
    "\n",
    "print(\"‚úì Preprocesamiento completado en la columna 'tokens_processed'\")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DEL PREPROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Columna de tokens procesados (Primeras 5 filas):\")\n",
    "print(df_processed[['Sentence', 'tokens_processed']].head())\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ejemplo de transformaci√≥n\n",
    "if len(df_processed) > 0:\n",
    "    print(\"\\nEjemplo de transformaci√≥n de la primera fila:\")\n",
    "    print(f\"  Texto Original: {df['Sentence'].iloc[0]}\")\n",
    "    print(f\"  Tokens Finales: {df_processed['tokens_processed'].iloc[0]}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular estad√≠sticas de reducci√≥n\n",
    "df_processed['num_tokens_original'] = df_processed['tokens_original'].apply(len)\n",
    "df_processed['num_tokens_processed'] = df_processed['tokens_processed'].apply(len)\n",
    "\n",
    "mean_original = df_processed['num_tokens_original'].mean()\n",
    "mean_processed = df_processed['num_tokens_processed'].mean()\n",
    "reduction_percentage = ((mean_original - mean_processed) / mean_original * 100) if mean_original > 0 else 0\n",
    "\n",
    "print(f\"\\nEstad√≠sticas de Longitud:\")\n",
    "print(f\"  Promedio de tokens originales: {mean_original:.2f}\")\n",
    "print(f\"  Promedio de tokens procesados: {mean_processed:.2f}\")\n",
    "print(f\"  Reducci√≥n de ruido promedio: **{reduction_percentage:.2f}%**\")\n",
    "\n",
    "# Guardar resultado\n",
    "df_processed.to_csv('data_processed/datos_preprocesados_simple.csv', index=False)\n",
    "print(f\"\\n‚úì Datos guardados en 'data_processed/datos_preprocesados_simple.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca1727",
   "metadata": {},
   "source": [
    "## 4. Lemmatization y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c11c73",
   "metadata": {},
   "outputs": [],
   "source": "from nltk.stem import PorterStemmer, WordNetLemmatizer\nimport pandas as pd\nimport nltk\nimport numpy as np\nimport ast\n\n# Descargar recursos de NLTK\ntry:\n    print(\"Verificando recursos NLTK esenciales (wordnet)...\")\n    nltk.download('wordnet', quiet=True, raise_on_error=False)\n    print(\"‚úì Recurso 'wordnet' listo.\")\nexcept Exception:\n    print(\"ATENCI√ìN: El recurso 'wordnet' de NLTK no est√° disponible.\")\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"LEMMATIZATION Y STEMMING\")\nprint(\"=\" * 60)\n\n# Cargar datos\ndf_processed = pd.read_csv('data_processed/datos_preprocesados_simple.csv').copy()\n\n# Convertir strings de listas a listas reales\nprint(\"\\n1. CONVIRTIENDO TOKENS (STRING ‚Üí LISTA)\")\nprint(\"-\" * 60)\n\ndef convert_to_list(value):\n    if isinstance(value, list):\n        return value\n    if isinstance(value, str):\n        try:\n            result = ast.literal_eval(value)\n            if isinstance(result, list):\n                return result\n        except:\n            return value.split()\n    return []\n\n# Aplicar conversi√≥n a las columnas de tokens\ndf_processed['tokens_processed'] = df_processed['tokens_processed'].apply(convert_to_list)\ndf_processed['tokens_original'] = df_processed['tokens_original'].apply(convert_to_list)\n\n# Verificar conversi√≥n\nsample = df_processed['tokens_processed'].iloc[0]\nif isinstance(sample, list) and len(sample) > 0 and isinstance(sample[0], str):\n    vocab_real = len(set(token for tokens in df_processed['tokens_processed'] for token in tokens))\n    print(f\"‚úì Conversi√≥n exitosa\")\n    print(f\"  Tipo: {type(sample)}\")\n    print(f\"  Ejemplo: {sample[:5]}\")\n    print(f\"  Vocabulario real: {vocab_real} palabras\")\nelse:\n    print(\"‚ö†Ô∏è  ADVERTENCIA: La conversi√≥n puede tener problemas\")\n\n# Inicializar herramientas de stemming y lemmatization\nporter_stemmer = PorterStemmer()\nword_lemmatizer = WordNetLemmatizer()\n\n# Aplicar stemming\nprint(\"\\n2. STEMMING (Porter Stemmer)\")\nprint(\"-\" * 60)\n\ndf_processed['tokens_stemmed'] = df_processed['tokens_processed'].apply(\n    lambda tokens: [porter_stemmer.stem(token) for token in tokens] if isinstance(tokens, list) else []\n)\nprint(\"‚úì Stemming completado\")\n\n# Aplicar lemmatization\nprint(\"\\n3. LEMMATIZATION (WordNet Lemmatizer)\")\nprint(\"-\" * 60)\n\ndf_processed['tokens_lemmatized'] = df_processed['tokens_processed'].apply(\n    lambda tokens: [word_lemmatizer.lemmatize(token, pos='v') for token in tokens] if isinstance(tokens, list) else []\n)\nprint(\"‚úì Lemmatization completada\")\n\n# Comparar resultados\nprint(\"\\n4. COMPARACI√ìN DE RESULTADOS\")\nprint(\"-\" * 60)\n\nif not df_processed.empty and len(df_processed['tokens_processed'].iloc[0]) > 0:\n    original = df_processed['tokens_processed'].iloc[0][:3]\n    stemmed = df_processed['tokens_stemmed'].iloc[0][:3]\n    lemmatized = df_processed['tokens_lemmatized'].iloc[0][:3]\n    \n    print(f\"{'Token Original':<20} {'Stemming':<20} {'Lemmatization':<20}\")\n    print(\"-\" * 60)\n    for i in range(min(len(original), 3)):\n        print(f\"{original[i]:<20} {stemmed[i]:<20} {lemmatized[i]:<20}\")\n\n# Calcular tama√±o de vocabulario despu√©s de cada t√©cnica\nvocab_original = set(token for tokens in df_processed['tokens_processed'] for token in tokens if isinstance(tokens, list))\nvocab_stemmed = set(token for tokens in df_processed['tokens_stemmed'] for token in tokens if isinstance(tokens, list))\nvocab_lemmatized = set(token for tokens in df_processed['tokens_lemmatized'] for token in tokens if isinstance(tokens, list))\n\nprint(f\"\\nTama√±o del vocabulario base: {len(vocab_original)}\")\nprint(f\"Tama√±o despu√©s de Stemming: {len(vocab_stemmed)} ({((len(vocab_original) - len(vocab_stemmed)) / len(vocab_original) * 100):.2f}% reducci√≥n)\")\nprint(f\"Tama√±o despu√©s de Lemmatization: {len(vocab_lemmatized)} ({((len(vocab_original) - len(vocab_lemmatized)) / len(vocab_original) * 100):.2f}% reducci√≥n)\")\n\n# Convertir tokens a texto para compatibilidad\ndf_processed['text_stemmed'] = df_processed['tokens_stemmed'].apply(\n    lambda x: ' '.join(x) if isinstance(x, list) else ''\n)\ndf_processed['text_lemmatized'] = df_processed['tokens_lemmatized'].apply(\n    lambda x: ' '.join(x) if isinstance(x, list) else ''\n)\ndf_processed['text_processed_base'] = df_processed['tokens_processed'].apply(\n    lambda x: ' '.join(x) if isinstance(x, list) else ''\n)\n\n# Crear columna de tokens limpios SIN lematizar (para embeddings pre-entrenados)\ndf_processed['tokens_clean'] = df_processed['tokens_processed'].copy()\n\n# Convertir a texto tambi√©n\ndf_processed['text_clean'] = df_processed['tokens_clean'].apply(\n    lambda x: ' '.join(x) if isinstance(x, list) else ''\n)\n\nprint(\"\\n‚úì Columna 'tokens_clean' creada para embeddings (sin lematizar)\")\n\n# Guardar\ndf_processed.to_csv('data_processed/datos_preprocesados_completo.csv', index=False)\nprint(f\"\\n‚úì Datos guardados en 'data_processed/datos_preprocesados_completo.csv'\")"
  },
  {
   "cell_type": "markdown",
   "id": "20abc521",
   "metadata": {},
   "source": [
    "## 5. Representaci√≥n Tradicional: Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACI√ìN: BAG OF WORDS (BoW)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar columna de texto procesado\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# Crear vectorizador con par√°metros para reducir ruido\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=5000, \n",
    "    min_df=2, \n",
    "    max_df=0.8, \n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Crear matriz BoW\n",
    "X_bow = bow_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úì Matriz BoW creada. Forma: {X_bow.shape}\")\n",
    "print(f\"Tama√±o del Vocabulario Final: {X_bow.shape[1]}\")\n",
    "\n",
    "# Calcular frecuencias de t√©rminos\n",
    "print(\"\\n2. AN√ÅLISIS DE FRECUENCIAS (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "word_freq = np.asarray(X_bow.sum(axis=0)).flatten()\n",
    "freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False).head(15)\n",
    "\n",
    "print(freq_df)\n",
    "\n",
    "# Visualizar t√©rminos m√°s frecuentes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='frequency', y='word', data=freq_df)\n",
    "plt.title(f'Top 15 T√©rminos M√°s Frecuentes (BoW)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/bow_top_terms.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analizar t√©rminos clave por sentimiento\n",
    "print(\"\\n3. T√âRMINOS CLAVE POR SENTIMIENTO (Top 5 por Clase)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sentiment in df_processed['Sentiment'].unique():\n",
    "    # Filtrar documentos por sentimiento\n",
    "    docs_sentiment = df_processed[df_processed['Sentiment'] == sentiment][TEXT_COLUMN]\n",
    "    \n",
    "    # Vectorizar documentos\n",
    "    X_sentiment = bow_vectorizer.transform(docs_sentiment)\n",
    "    \n",
    "    # Calcular frecuencias\n",
    "    freq_sentiment = np.asarray(X_sentiment.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Mostrar top 5 t√©rminos\n",
    "    freq_df_sentiment = pd.DataFrame({\n",
    "        'word': feature_names,\n",
    "        'frequency': freq_sentiment\n",
    "    }).sort_values('frequency', ascending=False).head(5)\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(freq_df_sentiment)\n",
    "\n",
    "# Preparar datos finales para el modelo\n",
    "print(\"\\n4. PREPARACI√ìN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Convertir matriz dispersa a DataFrame\n",
    "X_dense = X_bow.toarray()\n",
    "bow_final_df = pd.DataFrame(X_dense, columns=feature_names)\n",
    "\n",
    "# A√±adir columnas de etiquetas\n",
    "bow_final_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "bow_final_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "# Guardar\n",
    "bow_final_df.to_csv('data_processed/datos_vectorizados_final.csv', index=False)\n",
    "print(f\"‚úì Matriz de caracter√≠sticas guardada en 'data_processed/datos_vectorizados_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f2760",
   "metadata": {},
   "source": [
    "## 6. Representaci√≥n Tradicional: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACI√ìN: TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar columna de texto procesado\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# Crear vectorizador TF-IDF\n",
    "print(\"\\n1. CREANDO MATRIZ TF-IDF\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Crear matriz TF-IDF\n",
    "try:\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "except NameError:\n",
    "    print(\"ERROR: El DataFrame 'df_processed' no est√° definido. Aseg√∫rate de cargar los datos antes.\")\n",
    "    sys.exit()\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úì Modelo TF-IDF creado. Forma: {X_tfidf.shape}\")\n",
    "print(f\"Tama√±o del Vocabulario Final: {X_tfidf.shape[1]}\")\n",
    "\n",
    "# Analizar t√©rminos con mayor peso TF-IDF\n",
    "print(\"\\n2. T√âRMINOS CON MAYOR PESO TF-IDF PROMEDIO (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular peso promedio de cada t√©rmino\n",
    "tfidf_means = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "tfidf_df = pd.DataFrame({\n",
    "    'term': tfidf_feature_names,\n",
    "    'tfidf_mean': tfidf_means\n",
    "}).sort_values('tfidf_mean', ascending=False).head(15)\n",
    "\n",
    "print(tfidf_df)\n",
    "\n",
    "# Preparar datos finales\n",
    "print(\"\\n3. PREPARACI√ìN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Convertir matriz dispersa a DataFrame\n",
    "tfidf_matrix_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_feature_names)\n",
    "tfidf_matrix_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "\n",
    "# Guardar\n",
    "tfidf_matrix_df.to_csv('data_processed/datos_tfidf_final.csv', index=False)\n",
    "print(f\"‚úì Matriz de caracter√≠sticas guardada en 'data_processed/datos_tfidf_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d0e81",
   "metadata": {},
   "source": [
    "## 7. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95779b",
   "metadata": {},
   "outputs": [],
   "source": "# Crear embeddings usando Word2Vec y FastText\nfrom gensim.models import Word2Vec, FastText\nimport pandas as pd\nimport numpy as np\n\nprint(\"=\" * 60)\nprint(\"WORD EMBEDDINGS NO CONTEXTUALES\")\nprint(\"NOTA: Usamos tokens_clean (sin lematizar) porque Word2Vec/FastText\")\nprint(\"      fueron entrenados con texto natural y esperan palabras en su forma original\")\nprint(\"=\" * 60)\n\n# Cargar datos procesados\ndf_processed = pd.read_csv('data_processed/datos_preprocesados_completo.csv')\n\n# Convertir tokens a listas\nimport ast\n\ndef ensure_list(value):\n    if isinstance(value, list):\n        return value\n    if isinstance(value, str):\n        try:\n            return ast.literal_eval(value)\n        except:\n            return value.split()\n    return []\n\ndf_processed['tokens_clean'] = df_processed['tokens_clean'].apply(ensure_list)\nsentences = df_processed['tokens_clean'].tolist()\n\nsample = sentences[0]\nprint(f\"\\nVerificaci√≥n inicial:\")\nprint(f\"  Tipo: {type(sample)}\")\nprint(f\"  Ejemplo: {sample[:5] if len(sample) > 0 else 'vac√≠o'}\")\n\n# WORD2VEC\nprint(\"\\n\" + \"=\" * 60)\nprint(\"1. WORD2VEC\")\nprint(\"=\" * 60)\n\nprint(\"\\nEntrenando modelo Word2Vec...\")\nw2v_model = Word2Vec(\n    sentences=sentences,\n    vector_size=100,\n    window=5,\n    min_count=2,\n    workers=4,\n    sg=1,\n    epochs=10\n)\n\nvocab_size = len(w2v_model.wv)\nprint(f\"‚úì Modelo entrenado\")\nprint(f\"  Vocabulario: {vocab_size} palabras\")\nprint(f\"  Dimensi√≥n: {w2v_model.wv.vector_size}\")\n\n# Calcular palabras fuera de vocabulario (OOV)\noov_count = sum(1 for tokens in sentences for token in tokens if token not in w2v_model.wv)\ntotal_tokens = sum(len(tokens) for tokens in sentences)\noov_percentage = (oov_count / total_tokens * 100) if total_tokens > 0 else 0\n\nprint(f\"\\nAn√°lisis OOV:\")\nprint(f\"  Palabras fuera de vocabulario: {oov_count:,}/{total_tokens:,}\")\nprint(f\"  Porcentaje OOV: {oov_percentage:.2f}%\")\n\n# Generar vectores de documentos promediando los vectores de palabras\ndef get_document_vector_w2v(tokens, model):\n    vectors = [model.wv[word] for word in tokens if word in model.wv]\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(model.wv.vector_size)\n\nprint(\"\\nGenerando vectores de documentos...\")\ndoc_vectors_w2v = df_processed['tokens_clean'].apply(\n    lambda tokens: get_document_vector_w2v(tokens, w2v_model)\n)\n\nX_w2v = np.vstack(doc_vectors_w2v.values)\nw2v_df = pd.DataFrame(X_w2v, columns=[f'w2v_{i}' for i in range(X_w2v.shape[1])])\nw2v_df['Sentiment'] = df_processed['Sentiment'].values\nw2v_df['Idioma'] = df_processed['Idioma'].values\n\nw2v_df.to_csv('data_processed/datos_word2vec.csv', index=False)\nw2v_model.save('models/word2vec_model.model')\nprint(\"‚úì Vectores Word2Vec guardados\")\n\n# Ejemplo de palabras similares\nif vocab_size > 1000:\n    try:\n        test_words = ['company', 'profit', 'loss', 'market']\n        print(\"\\nEjemplos de palabras similares:\")\n        for word in test_words:\n            if word in w2v_model.wv:\n                similar = w2v_model.wv.most_similar(word, topn=3)\n                print(f\"  {word}: {[w for w, s in similar]}\")\n                break\n    except:\n        pass\n\n# FASTTEXT\nprint(\"\\n\" + \"=\" * 60)\nprint(\"2. FASTTEXT\")\nprint(\"=\" * 60)\n\nprint(\"\\nEntrenando modelo FastText...\")\nft_model = FastText(\n    sentences=sentences,\n    vector_size=100,\n    window=5,\n    min_count=2,\n    workers=4,\n    sg=1,\n    epochs=10\n)\n\nvocab_size_ft = len(ft_model.wv)\nprint(f\"‚úì Modelo entrenado\")\nprint(f\"  Vocabulario: {vocab_size_ft} palabras\")\nprint(f\"  Dimensi√≥n: {ft_model.wv.vector_size}\")\n\nprint(f\"\\nVentaja de FastText:\")\nprint(f\"  ‚úì Todas las palabras tienen representaci√≥n (incluso OOV)\")\nprint(f\"  ‚úì Usa subwords para palabras desconocidas\")\nprint(f\"  Total de tokens procesados: {total_tokens:,}\")\n\n# Generar vectores de documentos (FastText no tiene problema con OOV)\ndef get_document_vector_ft(tokens, model):\n    vectors = [model.wv[word] for word in tokens]\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(model.wv.vector_size)\n\nprint(\"\\nGenerando vectores de documentos...\")\ndoc_vectors_ft = df_processed['tokens_clean'].apply(\n    lambda tokens: get_document_vector_ft(tokens, ft_model)\n)\n\nX_ft = np.vstack(doc_vectors_ft.values)\nft_df = pd.DataFrame(X_ft, columns=[f'ft_{i}' for i in range(X_ft.shape[1])])\nft_df['Sentiment'] = df_processed['Sentiment'].values\nft_df['Idioma'] = df_processed['Idioma'].values\n\nft_df.to_csv('data_processed/datos_fasttext.csv', index=False)\nft_model.save('models/fasttext_model.model')\nprint(\"‚úì Vectores FastText guardados\")\n\n# Ejemplo de palabras similares\nif vocab_size_ft > 1000:\n    try:\n        test_words = ['company', 'profit', 'loss', 'market']\n        print(\"\\nEjemplos de palabras similares:\")\n        for word in test_words:\n            similar = ft_model.wv.most_similar(word, topn=3)\n            print(f\"  {word}: {[w for w, s in similar]}\")\n            break\n    except:\n        pass\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RESUMEN DE EMBEDDINGS NO CONTEXTUALES\")\nprint(\"=\" * 60)\nprint(f\"‚úì Word2Vec: {vocab_size:,} palabras, {oov_percentage:.2f}% OOV\")\nprint(f\"‚úì FastText: {vocab_size_ft:,} palabras, 0% OOV (usa subwords)\")"
  },
  {
   "cell_type": "markdown",
   "id": "bert_section",
   "metadata": {},
   "source": [
    "## 8. Word Embeddings Contextuales con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings contextuales usando BERT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORD EMBEDDINGS CONTEXTUALES - BERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar datos\n",
    "df_processed = pd.read_csv('data_processed/datos_preprocesados_completo.csv')\n",
    "\n",
    "# Cargar modelo BERT multiling√ºe\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "print(f\"\\nCargando modelo: {model_name}\")\n",
    "print(\"(Este proceso puede tardar unos minutos la primera vez)\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Usar GPU si est√° disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Modelo cargado en: {device}\")\n",
    "print(f\"  Vocabulario: {len(tokenizer)} tokens\")\n",
    "print(f\"  Dimensi√≥n de embeddings: {model.config.hidden_size}\")\n",
    "\n",
    "# Obtener embedding de BERT usando el token [CLS]\n",
    "def get_bert_embedding(text, tokenizer, model, device, max_length=128):\n",
    "    # Tokenizar texto\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                      padding=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Obtener embeddings sin calcular gradientes\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usar el embedding del token [CLS] que representa toda la oraci√≥n\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embedding.flatten()\n",
    "\n",
    "# Generar embeddings para todos los documentos\n",
    "print(f\"\\nGenerando embeddings para {len(df_processed)} documentos...\")\n",
    "print(\"(Esto puede tardar varios minutos)\")\n",
    "\n",
    "embeddings_list = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(df_processed), batch_size)):\n",
    "    batch = df_processed['Sentence'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    for text in batch:\n",
    "        embedding = get_bert_embedding(str(text), tokenizer, model, device)\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "# Crear DataFrame con embeddings\n",
    "X_bert = np.vstack(embeddings_list)\n",
    "bert_df = pd.DataFrame(X_bert, columns=[f'bert_{i}' for i in range(X_bert.shape[1])])\n",
    "bert_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "bert_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "# Guardar\n",
    "bert_df.to_csv('data_processed/datos_bert.csv', index=False)\n",
    "print(f\"\\n‚úì Embeddings BERT guardados\")\n",
    "print(f\"  Forma de la matriz: {X_bert.shape}\")\n",
    "print(f\"  Archivo: datos_bert.csv\")\n",
    "\n",
    "print(f\"\\nAn√°lisis OOV:\")\n",
    "print(f\"  ‚úì BERT no tiene palabras OOV\")\n",
    "print(f\"  ‚úì Usa subword tokenization (WordPiece)\")\n",
    "print(f\"  ‚úì Embeddings contextuales (var√≠a seg√∫n contexto)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARACI√ìN: NO CONTEXTUALES vs CONTEXTUALES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWord2Vec/FastText (No Contextuales):\")\n",
    "print(\"  ‚Ä¢ Cada palabra tiene UN SOLO vector\")\n",
    "print(\"  ‚Ä¢ No considera contexto\")\n",
    "print(\"  ‚Ä¢ Vocabulario limitado (problemas OOV)\")\n",
    "print(\"\\nBERT (Contextual):\")\n",
    "print(\"  ‚Ä¢ Cada palabra tiene M√öLTIPLES vectores seg√∫n contexto\")\n",
    "print(\"  ‚Ä¢ Considera contexto completo de la oraci√≥n\")\n",
    "print(\"  ‚Ä¢ Sin problemas OOV (subword tokenization)\")\n",
    "print(\"  ‚Ä¢ M√°s costoso computacionalmente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2vir64zy58",
   "source": "## 10. TAREA 1: Divisi√≥n Train/Validation/Test\n\n**Objetivo**: Crear splits estratificados para ambas tareas (Consistencia y Sentimiento)\n\nEn esta secci√≥n vamos a dividir el dataset en conjuntos de entrenamiento, validaci√≥n y prueba de forma estratificada para garantizar que las proporciones de clases se mantengan en cada split.\n\n**Splits a crear**:\n- Train: 70%\n- Validation: 15%\n- Test: 15%\n\n**Tareas**:\n1. Detecci√≥n de Consistencia (target: Etiqueta - correcta/incorrecta)\n2. An√°lisis de Sentimiento (target: Sentiment - positive/negative/neutral)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8otcnda8va5",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Establecer semilla para reproducibilidad\nnp.random.seed(42)\n\nprint(\"=\" * 80)\nprint(\"TAREA 1: DIVISI√ìN DE DATOS EN TRAIN/VALIDATION/TEST\")\nprint(\"=\" * 80)\n\n# Cargar dataset multiidioma\nprint(\"\\n1. CARGANDO DATASET PRINCIPAL\")\nprint(\"-\" * 80)\n\ndf_main = pd.read_csv('data/initial_data.csv')\nprint(f\"‚úì Dataset cargado: {len(df_main)} noticias\")\nprint(f\"\\nColumnas disponibles: {list(df_main.columns)}\")\n\n# Verificar si existe la columna 'Etiqueta', si no, crearla\nif 'Etiqueta' not in df_main.columns:\n    print(\"\\nNOTA: Columna 'Etiqueta' no encontrada. Creando todas las noticias como 'correcta'\")\n    df_main['Etiqueta'] = 'correcta'\n\n# Verificar distribuciones\nprint(f\"\\nDistribuci√≥n de Sentiment:\")\nprint(df_main['Sentiment'].value_counts())\nprint(f\"\\nDistribuci√≥n de Etiqueta:\")\nprint(df_main['Etiqueta'].value_counts())\n\n# DIVISI√ìN PARA TAREA 1: DETECCI√ìN DE CONSISTENCIA (Etiqueta: correcta/incorrecta)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIVISI√ìN PARA TAREA 1: DETECCI√ìN DE CONSISTENCIA\")\nprint(\"=\" * 80)\n\n# Primero dividir en train (70%) y temp (30%)\nX_consistency = df_main['Sentence'].values\ny_consistency = df_main['Etiqueta'].values\n\nX_train_cons, X_temp_cons, y_train_cons, y_temp_cons = train_test_split(\n    X_consistency, y_consistency,\n    test_size=0.30,\n    random_state=42,\n    stratify=y_consistency\n)\n\n# Luego dividir temp en validation (15%) y test (15%)\nX_val_cons, X_test_cons, y_val_cons, y_test_cons = train_test_split(\n    X_temp_cons, y_temp_cons,\n    test_size=0.50,\n    random_state=42,\n    stratify=y_temp_cons\n)\n\n# Crear DataFrames\nconsistency_train = pd.DataFrame({\n    'Sentence': X_train_cons,\n    'Etiqueta': y_train_cons\n})\nconsistency_val = pd.DataFrame({\n    'Sentence': X_val_cons,\n    'Etiqueta': y_val_cons\n})\nconsistency_test = pd.DataFrame({\n    'Sentence': X_test_cons,\n    'Etiqueta': y_test_cons\n})\n\nprint(f\"\\n‚úì Splits creados para Detecci√≥n de Consistencia:\")\nprint(f\"  Train: {len(consistency_train)} ({len(consistency_train)/len(df_main)*100:.1f}%)\")\nprint(f\"  Validation: {len(consistency_val)} ({len(consistency_val)/len(df_main)*100:.1f}%)\")\nprint(f\"  Test: {len(consistency_test)} ({len(consistency_test)/len(df_main)*100:.1f}%)\")\n\n# Verificar balanceo en cada split\nprint(f\"\\nDistribuci√≥n de clases en cada split:\")\nprint(f\"\\nTrain:\")\nprint(consistency_train['Etiqueta'].value_counts(normalize=True).mul(100).round(2))\nprint(f\"\\nValidation:\")\nprint(consistency_val['Etiqueta'].value_counts(normalize=True).mul(100).round(2))\nprint(f\"\\nTest:\")\nprint(consistency_test['Etiqueta'].value_counts(normalize=True).mul(100).round(2))\n\n# DIVISI√ìN PARA TAREA 2: AN√ÅLISIS DE SENTIMIENTO (Sentiment: positive/negative/neutral)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIVISI√ìN PARA TAREA 2: AN√ÅLISIS DE SENTIMIENTO\")\nprint(\"=\" * 80)\n\n# Primero dividir en train (70%) y temp (30%)\nX_sentiment = df_main['Sentence'].values\ny_sentiment = df_main['Sentiment'].values\n\nX_train_sent, X_temp_sent, y_train_sent, y_temp_sent = train_test_split(\n    X_sentiment, y_sentiment,\n    test_size=0.30,\n    random_state=42,\n    stratify=y_sentiment\n)\n\n# Luego dividir temp en validation (15%) y test (15%)\nX_val_sent, X_test_sent, y_val_sent, y_test_sent = train_test_split(\n    X_temp_sent, y_temp_sent,\n    test_size=0.50,\n    random_state=42,\n    stratify=y_temp_sent\n)\n\n# Crear DataFrames\nsentiment_train = pd.DataFrame({\n    'Sentence': X_train_sent,\n    'Sentiment': y_train_sent\n})\nsentiment_val = pd.DataFrame({\n    'Sentence': X_val_sent,\n    'Sentiment': y_val_sent\n})\nsentiment_test = pd.DataFrame({\n    'Sentence': X_test_sent,\n    'Sentiment': y_test_sent\n})\n\nprint(f\"\\n‚úì Splits creados para An√°lisis de Sentimiento:\")\nprint(f\"  Train: {len(sentiment_train)} ({len(sentiment_train)/len(df_main)*100:.1f}%)\")\nprint(f\"  Validation: {len(sentiment_val)} ({len(sentiment_val)/len(df_main)*100:.1f}%)\")\nprint(f\"  Test: {len(sentiment_test)} ({len(sentiment_test)/len(df_main)*100:.1f}%)\")\n\n# Verificar balanceo en cada split\nprint(f\"\\nDistribuci√≥n de clases en cada split:\")\nprint(f\"\\nTrain:\")\nprint(sentiment_train['Sentiment'].value_counts(normalize=True).mul(100).round(2))\nprint(f\"\\nValidation:\")\nprint(sentiment_val['Sentiment'].value_counts(normalize=True).mul(100).round(2))\nprint(f\"\\nTest:\")\nprint(sentiment_test['Sentiment'].value_counts(normalize=True).mul(100).round(2))\n\n# GUARDAR SPLITS EN ARCHIVOS CSV\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GUARDANDO SPLITS EN ARCHIVOS CSV\")\nprint(\"=\" * 80)\n\n# Guardar splits de consistencia\nconsistency_train.to_csv('data_processed/consistency_train.csv', index=False)\nconsistency_val.to_csv('data_processed/consistency_val.csv', index=False)\nconsistency_test.to_csv('data_processed/consistency_test.csv', index=False)\n\nprint(f\"\\n‚úì Splits de Consistencia guardados:\")\nprint(f\"  data_processed/consistency_train.csv\")\nprint(f\"  data_processed/consistency_val.csv\")\nprint(f\"  data_processed/consistency_test.csv\")\n\n# Guardar splits de sentimiento\nsentiment_train.to_csv('data_processed/sentiment_train.csv', index=False)\nsentiment_val.to_csv('data_processed/sentiment_val.csv', index=False)\nsentiment_test.to_csv('data_processed/sentiment_test.csv', index=False)\n\nprint(f\"\\n‚úì Splits de Sentimiento guardados:\")\nprint(f\"  data_processed/sentiment_train.csv\")\nprint(f\"  data_processed/sentiment_val.csv\")\nprint(f\"  data_processed/sentiment_test.csv\")\n\n# VISUALIZACI√ìN DE LA DISTRIBUCI√ìN\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VISUALIZACI√ìN DE LA DISTRIBUCI√ìN DE SPLITS\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nfig.suptitle('Distribuci√≥n de Clases en Train/Val/Test', fontsize=16, fontweight='bold')\n\n# Fila 1: Consistencia\nsplits_cons = [\n    (consistency_train, 'Train (Consistencia)'),\n    (consistency_val, 'Validation (Consistencia)'),\n    (consistency_test, 'Test (Consistencia)')\n]\n\nfor idx, (split_df, title) in enumerate(splits_cons):\n    counts = split_df['Etiqueta'].value_counts()\n    colors = ['#2ecc71' if label == 'correcta' else '#e74c3c' for label in counts.index]\n    axes[0, idx].bar(counts.index, counts.values, color=colors)\n    axes[0, idx].set_title(title)\n    axes[0, idx].set_ylabel('Frecuencia')\n    axes[0, idx].set_xlabel('Etiqueta')\n\n    # A√±adir porcentajes\n    for i, (label, count) in enumerate(counts.items()):\n        pct = count / len(split_df) * 100\n        axes[0, idx].text(i, count, f'{pct:.1f}%', ha='center', va='bottom')\n\n# Fila 2: Sentimiento\nsplits_sent = [\n    (sentiment_train, 'Train (Sentimiento)'),\n    (sentiment_val, 'Validation (Sentimiento)'),\n    (sentiment_test, 'Test (Sentimiento)')\n]\n\nfor idx, (split_df, title) in enumerate(splits_sent):\n    counts = split_df['Sentiment'].value_counts()\n    axes[1, idx].bar(range(len(counts)), counts.values)\n    axes[1, idx].set_title(title)\n    axes[1, idx].set_ylabel('Frecuencia')\n    axes[1, idx].set_xlabel('Sentimiento')\n    axes[1, idx].set_xticks(range(len(counts)))\n    axes[1, idx].set_xticklabels(counts.index, rotation=45)\n\n    # A√±adir porcentajes\n    for i, (label, count) in enumerate(counts.items()):\n        pct = count / len(split_df) * 100\n        axes[1, idx].text(i, count, f'{pct:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/04_train_val_test_splits.png', dpi=300, bbox_inches='tight')\nprint(\"\\n‚úì Visualizaci√≥n guardada: charts/04_train_val_test_splits.png\")\nplt.show()\n\n# RESUMEN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RESUMEN FINAL\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì TAREA 1 COMPLETADA\")\nprint(f\"\\nArchivos generados:\")\nprint(f\"  1. Splits para Detecci√≥n de Consistencia (3 archivos)\")\nprint(f\"  2. Splits para An√°lisis de Sentimiento (3 archivos)\")\nprint(f\"  3. Visualizaci√≥n de distribuciones (1 gr√°fico)\")\nprint(f\"\\nTotal de splits creados: 6 archivos CSV\")\nprint(f\"Proporci√≥n: 70% Train, 15% Validation, 15% Test\")\nprint(f\"Stratificaci√≥n: ‚úì Aplicada en ambas tareas\")\nprint(f\"\\nLos splits est√°n listos para ser usados en las siguientes tareas.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ksujy24s61k",
   "source": "## 11. TAREA 2: Shallow Learning - Detecci√≥n de Consistencia con BoW\n\n**Objetivo**: Entrenar y comparar 3 clasificadores tradicionales para detectar consistencia usando representaci√≥n Bag of Words.\n\nEn esta secci√≥n vamos a:\n1. Cargar los datos vectorizados con BoW (ya generados en la secci√≥n 5)\n2. Aplicar los splits Train/Val/Test de la TAREA 1\n3. Entrenar 3 clasificadores: Logistic Regression, Random Forest, SVM\n4. Optimizar hiperpar√°metros con GridSearchCV\n5. Evaluar y comparar rendimiento\n\n**Tarea**: Detecci√≥n de Consistencia (correcta vs incorrecta)  \n**Representaci√≥n**: Bag of Words (BoW)  \n**Clasificadores**: Logistic Regression, Random Forest, LinearSVC",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dzs1hp3vkjr",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\n\n# Establecer semilla\nnp.random.seed(42)\n\nprint(\"=\" * 80)\nprint(\"TAREA 2: SHALLOW LEARNING - DETECCI√ìN DE CONSISTENCIA CON BoW\")\nprint(\"=\" * 80)\n\n# PASO 1: CARGAR DATOS BoW Y SPLITS\nprint(\"\\n1. CARGANDO DATOS BoW\")\nprint(\"-\" * 80)\n\n# Cargar datos BoW completos (generados en secci√≥n 5)\n# NOTA: Si no existe datos_vectorizados_final.csv, necesitamos generarlo\ntry:\n    df_bow = pd.read_csv('data_processed/datos_vectorizados_final.csv')\n    print(f\"‚úì Datos BoW cargados: {df_bow.shape}\")\n    print(f\"  Caracter√≠sticas: {df_bow.shape[1] - 2}\")  # -2 por Sentiment e Idioma\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontr√≥ 'datos_vectorizados_final.csv'\")\n    print(\"Por favor, ejecuta primero la secci√≥n 5 (Bag of Words)\")\n    raise\n\n# Cargar splits de consistencia\nprint(\"\\n2. CARGANDO SPLITS DE CONSISTENCIA\")\nprint(\"-\" * 80)\n\ntry:\n    consistency_train = pd.read_csv('data_processed/consistency_train.csv')\n    consistency_val = pd.read_csv('data_processed/consistency_val.csv')\n    consistency_test = pd.read_csv('data_processed/consistency_test.csv')\n    \n    print(f\"‚úì Splits cargados:\")\n    print(f\"  Train: {len(consistency_train)} muestras\")\n    print(f\"  Validation: {len(consistency_val)} muestras\")\n    print(f\"  Test: {len(consistency_test)} muestras\")\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontraron los splits de consistencia\")\n    print(\"Por favor, ejecuta primero la TAREA 1 (Divisi√≥n Train/Val/Test)\")\n    raise\n\n# Verificar que tenemos la columna Etiqueta en los datos BoW\n# Necesitamos fusionar los datos BoW con las etiquetas de consistencia\nprint(\"\\n3. PREPARANDO DATOS PARA ENTRENAMIENTO\")\nprint(\"-\" * 80)\n\n# Cargar el dataset original con etiquetas\ndf_original = pd.read_csv('data/initial_data.csv')\n\n# Verificar/crear columna Etiqueta\nif 'Etiqueta' not in df_original.columns:\n    print(\"NOTA: Creando columna 'Etiqueta' = 'correcta' (todas correctas)\")\n    df_original['Etiqueta'] = 'correcta'\n\n# Ahora necesitamos hacer match entre los datos BoW y las etiquetas\n# Asumimos que el orden es el mismo\nprint(f\"Verificando concordancia de tama√±os...\")\nprint(f\"  BoW shape: {df_bow.shape[0]}\")\nprint(f\"  Original shape: {df_original.shape[0]}\")\n\n# Agregar columna Etiqueta a df_bow\nif df_bow.shape[0] == df_original.shape[0]:\n    df_bow['Etiqueta'] = df_original['Etiqueta'].values\n    print(\"‚úì Etiquetas agregadas a datos BoW\")\nelse:\n    print(\"‚ö†Ô∏è  ADVERTENCIA: Tama√±os no coinciden. Esto puede causar problemas.\")\n\n# Preparar conjuntos de entrenamiento, validaci√≥n y test\n# Necesitamos hacer merge con las oraciones de los splits\n\ndef prepare_bow_data(split_df, df_bow_full):\n    \"\"\"Prepara datos BoW para un split espec√≠fico\"\"\"\n    # Crear una copia del split con √≠ndice basado en Sentence\n    split_sentences = split_df['Sentence'].values\n    \n    # Encontrar √≠ndices correspondientes en df_original\n    df_original_indexed = df_original.reset_index(drop=True)\n    indices = []\n    \n    for sentence in split_sentences:\n        # Buscar la oraci√≥n en el dataset original\n        matches = df_original_indexed[df_original_indexed['Sentence'] == sentence].index\n        if len(matches) > 0:\n            indices.append(matches[0])\n    \n    # Extraer caracter√≠sticas BoW correspondientes\n    X = df_bow_full.iloc[indices].drop(['Sentiment', 'Idioma', 'Etiqueta'], axis=1, errors='ignore').values\n    y = split_df['Etiqueta'].values\n    \n    return X, y\n\nprint(\"\\nPreparando splits con caracter√≠sticas BoW...\")\nX_train, y_train = prepare_bow_data(consistency_train, df_bow)\nX_val, y_val = prepare_bow_data(consistency_val, df_bow)\nX_test, y_test = prepare_bow_data(consistency_test, df_bow)\n\nprint(f\"‚úì Datos preparados:\")\nprint(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n\n# PASO 2: ENTRENAR CLASIFICADORES CON GRIDSEARCH\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. ENTRENAMIENTO Y OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\nprint(\"=\" * 80)\n\n# Definir modelos y grids de hiperpar√°metros\nmodels = {\n    'Logistic Regression': {\n        'model': LogisticRegression(max_iter=1000, random_state=42),\n        'params': {\n            'C': [0.1, 1, 10]\n        }\n    },\n    'Random Forest': {\n        'model': RandomForestClassifier(random_state=42),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [10, 20, None]\n        }\n    },\n    'LinearSVC': {\n        'model': LinearSVC(random_state=42, max_iter=2000),\n        'params': {\n            'C': [0.1, 1, 10]\n        }\n    }\n}\n\n# Entrenar y evaluar cada modelo\nresults = []\nbest_models = {}\n\nfor model_name, config in models.items():\n    print(f\"\\n--- {model_name} ---\")\n    print(f\"Hiperpar√°metros a probar: {config['params']}\")\n    \n    # GridSearchCV\n    start_time = time.time()\n    grid_search = GridSearchCV(\n        config['model'],\n        config['params'],\n        cv=5,\n        scoring='f1_weighted',\n        n_jobs=-1,\n        verbose=1\n    )\n    \n    grid_search.fit(X_train, y_train)\n    training_time = time.time() - start_time\n    \n    # Mejor modelo\n    best_model = grid_search.best_estimator_\n    best_models[model_name] = best_model\n    \n    print(f\"‚úì Entrenamiento completado en {training_time:.2f}s\")\n    print(f\"Mejores hiperpar√°metros: {grid_search.best_params_}\")\n    \n    # Evaluar en validation set\n    y_val_pred = best_model.predict(X_val)\n    \n    # Calcular m√©tricas\n    accuracy = accuracy_score(y_val, y_val_pred)\n    precision = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    recall = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    f1 = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    \n    results.append({\n        'Modelo': model_name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1,\n        'Tiempo_Entrenamiento': training_time,\n        'Mejores_Params': str(grid_search.best_params_)\n    })\n    \n    print(f\"M√©tricas en Validation:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\")\n\n# PASO 3: COMPARAR RESULTADOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"5. COMPARACI√ìN DE RESULTADOS\")\nprint(\"=\" * 80)\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nTabla comparativa:\")\nprint(results_df.to_string(index=False))\n\n# Identificar mejor modelo\nbest_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Modelo']\nprint(f\"\\n‚úì Mejor modelo seg√∫n F1-Score: {best_model_name}\")\n\n# PASO 4: EVALUACI√ìN FINAL EN TEST SET\nprint(\"\\n\" + \"=\" * 80)\nprint(\"6. EVALUACI√ìN FINAL EN TEST SET\")\nprint(\"=\" * 80)\n\nbest_model_final = best_models[best_model_name]\ny_test_pred = best_model_final.predict(X_test)\n\nprint(f\"\\nResultados del mejor modelo ({best_model_name}) en Test:\")\nprint(classification_report(y_test, y_test_pred))\n\n# PASO 5: MATRICES DE CONFUSI√ìN\nprint(\"\\n\" + \"=\" * 80)\nprint(\"7. MATRICES DE CONFUSI√ìN\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Matrices de Confusi√≥n - BoW (Validation Set)', fontsize=16, fontweight='bold')\n\nfor idx, (model_name, model) in enumerate(best_models.items()):\n    y_val_pred = model.predict(X_val)\n    cm = confusion_matrix(y_val, y_val_pred)\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n                xticklabels=['correcta', 'incorrecta'],\n                yticklabels=['correcta', 'incorrecta'])\n    axes[idx].set_title(f'{model_name}\\nF1: {results_df[results_df[\"Modelo\"]==model_name][\"F1-Score\"].values[0]:.4f}')\n    axes[idx].set_ylabel('True Label')\n    axes[idx].set_xlabel('Predicted Label')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/05_bow_consistency_confusion_matrices.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Matrices de confusi√≥n guardadas: charts/05_bow_consistency_confusion_matrices.png\")\nplt.show()\n\n# PASO 6: GR√ÅFICO COMPARATIVO\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nfig.suptitle('Comparaci√≥n de Modelos - BoW (Consistencia)', fontsize=16, fontweight='bold')\n\nx_pos = np.arange(len(results_df))\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\nwidth = 0.2\n\nfor i, metric in enumerate(metrics):\n    ax.bar(x_pos + i*width, results_df[metric], width, label=metric)\n\nax.set_ylabel('Score')\nax.set_xlabel('Modelo')\nax.set_xticks(x_pos + width * 1.5)\nax.set_xticklabels(results_df['Modelo'], rotation=15, ha='right')\nax.legend()\nax.set_ylim([0, 1.1])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/05_bow_consistency_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Gr√°fico comparativo guardado: charts/05_bow_consistency_comparison.png\")\nplt.show()\n\n# PASO 7: GUARDAR MEJOR MODELO\nprint(\"\\n\" + \"=\" * 80)\nprint(\"8. GUARDANDO MEJOR MODELO\")\nprint(\"=\" * 80)\n\nmodel_path = 'models/bow_consistency_best.pkl'\nwith open(model_path, 'wb') as f:\n    pickle.dump(best_model_final, f)\nprint(f\"‚úì Mejor modelo guardado: {model_path}\")\n\n# Guardar tabla de resultados\nresults_df.to_csv('models/bow_consistency_results.csv', index=False)\nprint(f\"‚úì Resultados guardados: models/bow_consistency_results.csv\")\n\n# RESUMEN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RESUMEN FINAL - TAREA 2\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì TAREA 2 COMPLETADA\")\nprint(f\"\\nMejor modelo: {best_model_name}\")\nprint(f\"F1-Score (validation): {results_df[results_df['Modelo']==best_model_name]['F1-Score'].values[0]:.4f}\")\nprint(f\"\\nArchivos generados:\")\nprint(f\"  1. Matrices de confusi√≥n (3 modelos)\")\nprint(f\"  2. Gr√°fico comparativo de m√©tricas\")\nprint(f\"  3. Mejor modelo guardado (pickle)\")\nprint(f\"  4. Tabla de resultados (CSV)\")\nprint(f\"\\nJustificaci√≥n de hiperpar√°metros:\")\nprint(f\"  - Logistic Regression: C controla la regularizaci√≥n (mayor C = menos regularizaci√≥n)\")\nprint(f\"  - Random Forest: n_estimators (n√∫mero de √°rboles), max_depth (profundidad m√°xima)\")\nprint(f\"  - LinearSVC: C controla el trade-off entre margen y error de clasificaci√≥n\")\nprint(f\"\\nLos hiperpar√°metros fueron optimizados mediante GridSearchCV con validaci√≥n cruzada 5-fold.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cag8x4xmled",
   "source": "## 12. TAREA 3: Shallow Learning - An√°lisis de Sentimiento con TF-IDF\n\n**Objetivo**: Entrenar y comparar 3 clasificadores para an√°lisis de sentimiento multiclase usando representaci√≥n TF-IDF.\n\nEn esta secci√≥n vamos a:\n1. Cargar los datos vectorizados con TF-IDF (ya generados en la secci√≥n 6)\n2. Aplicar los splits Train/Val/Test de sentimiento de la TAREA 1\n3. Entrenar 3 clasificadores: Logistic Regression (multinomial), Random Forest, Multinomial Naive Bayes\n4. Optimizar hiperpar√°metros con GridSearchCV\n5. Evaluar con m√©tricas macro/micro/weighted (3 clases)\n6. An√°lisis especial de la clase minoritaria \"negative\"\n\n**Tarea**: An√°lisis de Sentimiento (positive/negative/neutral)  \n**Representaci√≥n**: TF-IDF  \n**Clasificadores**: Logistic Regression, Random Forest, Multinomial Naive Bayes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wbou8wwu34",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport time\n\n# Establecer semilla\nnp.random.seed(42)\n\nprint(\"=\" * 80)\nprint(\"TAREA 3: SHALLOW LEARNING - AN√ÅLISIS DE SENTIMIENTO CON TF-IDF\")\nprint(\"=\" * 80)\n\n# PASO 1: CARGAR DATOS TF-IDF Y SPLITS\nprint(\"\\n1. CARGANDO DATOS TF-IDF\")\nprint(\"-\" * 80)\n\ntry:\n    df_tfidf = pd.read_csv('data_processed/datos_tfidf_final.csv')\n    print(f\"‚úì Datos TF-IDF cargados: {df_tfidf.shape}\")\n    print(f\"  Caracter√≠sticas: {df_tfidf.shape[1] - 1}\")  # -1 por Sentiment\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontr√≥ 'datos_tfidf_final.csv'\")\n    print(\"Por favor, ejecuta primero la secci√≥n 6 (TF-IDF)\")\n    raise\n\n# Cargar splits de sentimiento\nprint(\"\\n2. CARGANDO SPLITS DE SENTIMIENTO\")\nprint(\"-\" * 80)\n\ntry:\n    sentiment_train = pd.read_csv('data_processed/sentiment_train.csv')\n    sentiment_val = pd.read_csv('data_processed/sentiment_val.csv')\n    sentiment_test = pd.read_csv('data_processed/sentiment_test.csv')\n    \n    print(f\"‚úì Splits cargados:\")\n    print(f\"  Train: {len(sentiment_train)} muestras\")\n    print(f\"  Validation: {len(sentiment_val)} muestras\")\n    print(f\"  Test: {len(sentiment_test)} muestras\")\n    \n    # Mostrar distribuci√≥n de clases\n    print(f\"\\nDistribuci√≥n de clases en Train:\")\n    print(sentiment_train['Sentiment'].value_counts())\n    print(f\"\\nDistribuci√≥n de clases en Validation:\")\n    print(sentiment_val['Sentiment'].value_counts())\n    print(f\"\\nDistribuci√≥n de clases en Test:\")\n    print(sentiment_test['Sentiment'].value_counts())\n    \nexcept FileNotFoundError:\n    print(\"ERROR: No se encontraron los splits de sentimiento\")\n    print(\"Por favor, ejecuta primero la TAREA 1 (Divisi√≥n Train/Val/Test)\")\n    raise\n\n# PASO 2: PREPARAR DATOS\nprint(\"\\n3. PREPARANDO DATOS PARA ENTRENAMIENTO\")\nprint(\"-\" * 80)\n\n# Cargar dataset original\ndf_original = pd.read_csv('data/initial_data.csv')\n\ndef prepare_tfidf_data(split_df, df_tfidf_full):\n    \"\"\"Prepara datos TF-IDF para un split espec√≠fico\"\"\"\n    split_sentences = split_df['Sentence'].values\n    df_original_indexed = df_original.reset_index(drop=True)\n    indices = []\n    \n    for sentence in split_sentences:\n        matches = df_original_indexed[df_original_indexed['Sentence'] == sentence].index\n        if len(matches) > 0:\n            indices.append(matches[0])\n    \n    # Extraer caracter√≠sticas TF-IDF correspondientes\n    X = df_tfidf_full.iloc[indices].drop(['Sentiment'], axis=1, errors='ignore').values\n    y = split_df['Sentiment'].values\n    \n    return X, y\n\nprint(\"Preparando splits con caracter√≠sticas TF-IDF...\")\nX_train, y_train = prepare_tfidf_data(sentiment_train, df_tfidf)\nX_val, y_val = prepare_tfidf_data(sentiment_val, df_tfidf)\nX_test, y_test = prepare_tfidf_data(sentiment_test, df_tfidf)\n\nprint(f\"‚úì Datos preparados:\")\nprint(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n\n# An√°lisis de clases\nprint(f\"\\nAn√°lisis de distribuci√≥n de clases:\")\nunique, counts = np.unique(y_train, return_counts=True)\nfor label, count in zip(unique, counts):\n    pct = count / len(y_train) * 100\n    print(f\"  {label}: {count} ({pct:.2f}%)\")\n\n# PASO 3: ENTRENAR CLASIFICADORES CON GRIDSEARCH\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. ENTRENAMIENTO Y OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\nprint(\"=\" * 80)\n\n# Definir modelos y grids de hiperpar√°metros\nmodels = {\n    'Logistic Regression': {\n        'model': LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42),\n        'params': {\n            'C': [0.1, 1, 10],\n            'solver': ['lbfgs', 'saga']\n        }\n    },\n    'Random Forest': {\n        'model': RandomForestClassifier(random_state=42),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [15, 25, None]\n        }\n    },\n    'Multinomial NB': {\n        'model': MultinomialNB(),\n        'params': {\n            'alpha': [0.1, 0.5, 1.0]\n        }\n    }\n}\n\n# Entrenar y evaluar cada modelo\nresults = []\nbest_models = {}\n\nfor model_name, config in models.items():\n    print(f\"\\n--- {model_name} ---\")\n    print(f\"Hiperpar√°metros a probar: {config['params']}\")\n    \n    # GridSearchCV\n    start_time = time.time()\n    grid_search = GridSearchCV(\n        config['model'],\n        config['params'],\n        cv=5,\n        scoring='f1_weighted',\n        n_jobs=-1,\n        verbose=1\n    )\n    \n    grid_search.fit(X_train, y_train)\n    training_time = time.time() - start_time\n    \n    # Mejor modelo\n    best_model = grid_search.best_estimator_\n    best_models[model_name] = best_model\n    \n    print(f\"‚úì Entrenamiento completado en {training_time:.2f}s\")\n    print(f\"Mejores hiperpar√°metros: {grid_search.best_params_}\")\n    \n    # Evaluar en validation set\n    y_val_pred = best_model.predict(X_val)\n    \n    # Calcular m√©tricas con diferentes promedios\n    accuracy = accuracy_score(y_val, y_val_pred)\n    precision_macro = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n    recall_macro = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n    f1_macro = f1_score(y_val, y_val_pred, average='macro', zero_division=0)\n    \n    precision_micro = precision_score(y_val, y_val_pred, average='micro', zero_division=0)\n    recall_micro = recall_score(y_val, y_val_pred, average='micro', zero_division=0)\n    f1_micro = f1_score(y_val, y_val_pred, average='micro', zero_division=0)\n    \n    precision_weighted = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    recall_weighted = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    f1_weighted = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n    \n    results.append({\n        'Modelo': model_name,\n        'Accuracy': accuracy,\n        'F1_Macro': f1_macro,\n        'F1_Micro': f1_micro,\n        'F1_Weighted': f1_weighted,\n        'Precision_Macro': precision_macro,\n        'Recall_Macro': recall_macro,\n        'Tiempo_Entrenamiento': training_time,\n        'Mejores_Params': str(grid_search.best_params_)\n    })\n    \n    print(f\"M√©tricas en Validation:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  F1-Macro: {f1_macro:.4f}\")\n    print(f\"  F1-Micro: {f1_micro:.4f}\")\n    print(f\"  F1-Weighted: {f1_weighted:.4f}\")\n\n# PASO 4: COMPARAR RESULTADOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"5. COMPARACI√ìN DE RESULTADOS\")\nprint(\"=\" * 80)\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nTabla comparativa:\")\nprint(results_df[['Modelo', 'Accuracy', 'F1_Macro', 'F1_Micro', 'F1_Weighted', 'Tiempo_Entrenamiento']].to_string(index=False))\n\n# Identificar mejor modelo\nbest_model_name = results_df.loc[results_df['F1_Weighted'].idxmax(), 'Modelo']\nprint(f\"\\n‚úì Mejor modelo seg√∫n F1-Weighted: {best_model_name}\")\n\n# PASO 5: EVALUACI√ìN DETALLADA POR CLASE\nprint(\"\\n\" + \"=\" * 80)\nprint(\"6. EVALUACI√ìN DETALLADA POR CLASE (Validation Set)\")\nprint(\"=\" * 80)\n\nfor model_name, model in best_models.items():\n    print(f\"\\n--- {model_name} ---\")\n    y_val_pred = model.predict(X_val)\n    print(classification_report(y_val, y_val_pred, zero_division=0))\n\n# PASO 6: AN√ÅLISIS DE CLASE MINORITARIA\nprint(\"\\n\" + \"=\" * 80)\nprint(\"7. AN√ÅLISIS ESPECIAL DE CLASE MINORITARIA (negative)\")\nprint(\"=\" * 80)\n\nbest_model_final = best_models[best_model_name]\ny_val_pred = best_model_final.predict(X_val)\n\n# Obtener m√©tricas por clase\nreport_dict = classification_report(y_val, y_val_pred, output_dict=True, zero_division=0)\n\nprint(f\"\\nM√©tricas del mejor modelo ({best_model_name}) para cada clase:\")\nprint(f\"\\n{'Clase':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\nprint(\"-\" * 65)\n\nfor label in ['negative', 'neutral', 'positive']:\n    if label in report_dict:\n        metrics = report_dict[label]\n        print(f\"{label:<15} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f} {metrics['f1-score']:<12.4f} {int(metrics['support']):<10}\")\n\n# Analizar errores en clase \"negative\"\nprint(f\"\\nAn√°lisis de errores en clase 'negative':\")\nnegative_indices = np.where(y_val == 'negative')[0]\nnegative_predictions = y_val_pred[negative_indices]\nnegative_true = y_val[negative_indices]\n\nerrors = np.sum(negative_predictions != negative_true)\ntotal = len(negative_true)\naccuracy_negative = 1 - (errors / total)\n\nprint(f\"  Total de muestras 'negative': {total}\")\nprint(f\"  Correctamente clasificadas: {total - errors}\")\nprint(f\"  Incorrectamente clasificadas: {errors}\")\nprint(f\"  Accuracy en clase 'negative': {accuracy_negative:.4f}\")\n\n# Ver a qu√© clases se confunden las 'negative'\nif errors > 0:\n    print(f\"\\nConfusi√≥n de clase 'negative':\")\n    unique_preds, counts = np.unique(negative_predictions[negative_predictions != negative_true], return_counts=True)\n    for pred_class, count in zip(unique_preds, counts):\n        print(f\"  Clasificadas como '{pred_class}': {count} ({count/errors*100:.1f}% de los errores)\")\n\n# PASO 7: EVALUACI√ìN FINAL EN TEST SET\nprint(\"\\n\" + \"=\" * 80)\nprint(\"8. EVALUACI√ìN FINAL EN TEST SET\")\nprint(\"=\" * 80)\n\ny_test_pred = best_model_final.predict(X_test)\n\nprint(f\"\\nResultados del mejor modelo ({best_model_name}) en Test:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\n\n# PASO 8: MATRICES DE CONFUSI√ìN\nprint(\"\\n\" + \"=\" * 80)\nprint(\"9. MATRICES DE CONFUSI√ìN\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Matrices de Confusi√≥n - TF-IDF (Validation Set)', fontsize=16, fontweight='bold')\n\nlabels_order = ['negative', 'neutral', 'positive']\n\nfor idx, (model_name, model) in enumerate(best_models.items()):\n    y_val_pred = model.predict(X_val)\n    cm = confusion_matrix(y_val, y_val_pred, labels=labels_order)\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=axes[idx],\n                xticklabels=labels_order,\n                yticklabels=labels_order)\n    axes[idx].set_title(f'{model_name}\\nF1-Weighted: {results_df[results_df[\"Modelo\"]==model_name][\"F1_Weighted\"].values[0]:.4f}')\n    axes[idx].set_ylabel('True Label')\n    axes[idx].set_xlabel('Predicted Label')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/06_tfidf_sentiment_confusion_matrices.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Matrices de confusi√≥n guardadas: charts/06_tfidf_sentiment_confusion_matrices.png\")\nplt.show()\n\n# PASO 9: GR√ÅFICO COMPARATIVO\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nfig.suptitle('Comparaci√≥n de Modelos - TF-IDF (Sentimiento)', fontsize=16, fontweight='bold')\n\n# Gr√°fico 1: M√©tricas generales\nx_pos = np.arange(len(results_df))\nmetrics = ['Accuracy', 'F1_Macro', 'F1_Micro', 'F1_Weighted']\nwidth = 0.2\n\nfor i, metric in enumerate(metrics):\n    axes[0].bar(x_pos + i*width, results_df[metric], width, label=metric)\n\naxes[0].set_ylabel('Score')\naxes[0].set_xlabel('Modelo')\naxes[0].set_xticks(x_pos + width * 1.5)\naxes[0].set_xticklabels(results_df['Modelo'], rotation=15, ha='right')\naxes[0].legend()\naxes[0].set_ylim([0, 1.1])\naxes[0].grid(axis='y', alpha=0.3)\naxes[0].set_title('M√©tricas Generales')\n\n# Gr√°fico 2: F1-Score por clase (mejor modelo)\nclasses = ['negative', 'neutral', 'positive']\nf1_scores = []\n\nfor class_label in classes:\n    if class_label in report_dict:\n        f1_scores.append(report_dict[class_label]['f1-score'])\n    else:\n        f1_scores.append(0)\n\ncolors = ['#e74c3c', '#95a5a6', '#2ecc71']\naxes[1].bar(classes, f1_scores, color=colors)\naxes[1].set_ylabel('F1-Score')\naxes[1].set_xlabel('Clase')\naxes[1].set_ylim([0, 1.1])\naxes[1].grid(axis='y', alpha=0.3)\naxes[1].set_title(f'F1-Score por Clase ({best_model_name})')\n\n# A√±adir valores sobre las barras\nfor i, (label, score) in enumerate(zip(classes, f1_scores)):\n    axes[1].text(i, score + 0.02, f'{score:.3f}', ha='center', va='bottom')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/06_tfidf_sentiment_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Gr√°fico comparativo guardado: charts/06_tfidf_sentiment_comparison.png\")\nplt.show()\n\n# PASO 10: GUARDAR MEJOR MODELO\nprint(\"\\n\" + \"=\" * 80)\nprint(\"10. GUARDANDO MEJOR MODELO\")\nprint(\"=\" * 80)\n\nmodel_path = 'models/tfidf_sentiment_best.pkl'\nwith open(model_path, 'wb') as f:\n    pickle.dump(best_model_final, f)\nprint(f\"‚úì Mejor modelo guardado: {model_path}\")\n\n# Guardar tabla de resultados\nresults_df.to_csv('models/tfidf_sentiment_results.csv', index=False)\nprint(f\"‚úì Resultados guardados: models/tfidf_sentiment_results.csv\")\n\n# RESUMEN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RESUMEN FINAL - TAREA 3\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì TAREA 3 COMPLETADA\")\nprint(f\"\\nMejor modelo: {best_model_name}\")\nprint(f\"F1-Weighted (validation): {results_df[results_df['Modelo']==best_model_name]['F1_Weighted'].values[0]:.4f}\")\nprint(f\"F1-Macro (validation): {results_df[results_df['Modelo']==best_model_name]['F1_Macro'].values[0]:.4f}\")\nprint(f\"\\nRendimiento por clase (mejor modelo):\")\nfor label in classes:\n    if label in report_dict:\n        print(f\"  {label}: F1={report_dict[label]['f1-score']:.4f}, Support={int(report_dict[label]['support'])}\")\n\nprint(f\"\\nArchivos generados:\")\nprint(f\"  1. Matrices de confusi√≥n (3 modelos)\")\nprint(f\"  2. Gr√°ficos comparativos (m√©tricas generales y por clase)\")\nprint(f\"  3. Mejor modelo guardado (pickle)\")\nprint(f\"  4. Tabla de resultados (CSV)\")\n\nprint(f\"\\nJustificaci√≥n de hiperpar√°metros:\")\nprint(f\"  - Logistic Regression: C (regularizaci√≥n), solver (optimizador para multinomial)\")\nprint(f\"  - Random Forest: n_estimators, max_depth (mayor profundidad para capturar complejidad multiclase)\")\nprint(f\"  - Multinomial NB: alpha (suavizado de Laplace, crucial para TF-IDF con ceros)\")\n\nprint(f\"\\nObservaciones sobre clase minoritaria 'negative':\")\nprint(f\"  - Es la clase m√°s dif√≠cil de predecir debido al desbalanceo\")\nprint(f\"  - F1-Score: {report_dict['negative']['f1-score']:.4f}\")\nprint(f\"  - Se recomienda considerar t√©cnicas de balanceo para mejorar rendimiento\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rqsyh2rc43g",
   "source": "## 13. TAREA 4: Deep Learning - Preparaci√≥n de Secuencias para LSTM/CNN\n\n**Objetivo**: Preparar datos de texto en formato secuencial para modelos de Deep Learning (LSTM/CNN).\n\nEn esta secci√≥n vamos a:\n1. Cargar datos preprocesados (tokens limpios sin lematizar)\n2. Crear un Tokenizer de Keras para convertir texto a secuencias num√©ricas\n3. Aplicar padding a las secuencias (max_length=100)\n4. Crear matrices de embeddings pre-entrenadas usando Word2Vec y FastText\n5. Aplicar splits Train/Val/Test de la TAREA 1\n6. Verificar cobertura del vocabulario\n\n**Nota importante**: Usamos tokens limpios (sin lematizar) porque los modelos Word2Vec y FastText fueron entrenados con palabras en su forma original.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hplorb0ls65",
   "source": "import pandas as pd\nimport numpy as np\nimport pickle\nimport ast\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import Word2Vec, FastText\n\n# Establecer semilla\nnp.random.seed(42)\n\nprint(\"=\" * 80)\nprint(\"TAREA 4: PREPARACI√ìN DE SECUENCIAS PARA DEEP LEARNING\")\nprint(\"=\" * 80)\n\n# PASO 1: CARGAR DATOS PREPROCESADOS\nprint(\"\\n1. CARGANDO DATOS PREPROCESADOS\")\nprint(\"-\" * 80)\n\ntry:\n    df_processed = pd.read_csv('data_processed/datos_preprocesados_completo.csv')\n    print(f\"‚úì Datos cargados: {df_processed.shape}\")\n    print(f\"Columnas disponibles: {list(df_processed.columns)}\")\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontr√≥ 'datos_preprocesados_completo.csv'\")\n    print(\"Por favor, ejecuta primero la secci√≥n 4 (Lemmatization y Stemming)\")\n    raise\n\n# Verificar que tenemos la columna text_clean\nif 'text_clean' not in df_processed.columns:\n    print(\"ERROR: No se encontr√≥ columna 'text_clean'\")\n    print(\"Por favor, verifica que el preprocesamiento se complet√≥ correctamente\")\n    raise\n\n# Verificar columna Etiqueta\nif 'Etiqueta' not in df_processed.columns:\n    print(\"NOTA: Creando columna 'Etiqueta' = 'correcta'\")\n    df_processed['Etiqueta'] = 'correcta'\n\nprint(f\"\\nPrimeras muestras de text_clean:\")\nprint(df_processed['text_clean'].head(3))\n\n# PASO 2: CREAR TOKENIZER Y CONVERTIR A SECUENCIAS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"2. CREANDO TOKENIZER DE KERAS\")\nprint(\"=\" * 80)\n\n# Par√°metros\nMAX_NUM_WORDS = 5000  # Vocabulario m√°ximo\nMAX_SEQUENCE_LENGTH = 100  # Longitud m√°xima de secuencias\nOOV_TOKEN = '<OOV>'  # Token para palabras fuera de vocabulario\n\n# Crear tokenizer\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=OOV_TOKEN)\ntokenizer.fit_on_texts(df_processed['text_clean'].values)\n\n# Estad√≠sticas del tokenizer\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1  # +1 por el √≠ndice 0 reservado\n\nprint(f\"‚úì Tokenizer creado\")\nprint(f\"  Vocabulario total: {len(word_index)} palabras\")\nprint(f\"  Vocabulario usado: {min(MAX_NUM_WORDS, vocab_size)} palabras\")\nprint(f\"  Token OOV: '{OOV_TOKEN}'\")\n\n# Mostrar algunas palabras del vocabulario\nprint(f\"\\nEjemplos de palabras en el vocabulario:\")\nsample_words = list(word_index.items())[:10]\nfor word, idx in sample_words:\n    print(f\"  {word}: {idx}\")\n\n# PASO 3: CONVERTIR TEXTOS A SECUENCIAS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"3. CONVERTIR TEXTOS A SECUENCIAS NUM√âRICAS\")\nprint(\"=\" * 80)\n\nsequences = tokenizer.texts_to_sequences(df_processed['text_clean'].values)\n\nprint(f\"‚úì Secuencias creadas: {len(sequences)} secuencias\")\n\n# Analizar longitudes de secuencias\nsequence_lengths = [len(seq) for seq in sequences]\nprint(f\"\\nEstad√≠sticas de longitud de secuencias:\")\nprint(f\"  Media: {np.mean(sequence_lengths):.2f}\")\nprint(f\"  Mediana: {np.median(sequence_lengths):.2f}\")\nprint(f\"  M√≠nima: {np.min(sequence_lengths)}\")\nprint(f\"  M√°xima: {np.max(sequence_lengths)}\")\nprint(f\"  Percentil 95: {np.percentile(sequence_lengths, 95):.2f}\")\n\n# Ejemplo de conversi√≥n\nprint(f\"\\nEjemplo de conversi√≥n texto -> secuencia:\")\nexample_text = df_processed['text_clean'].iloc[0]\nexample_seq = sequences[0]\nprint(f\"  Texto: {example_text[:100]}...\")\nprint(f\"  Secuencia: {example_seq[:20]}...\")\n\n# PASO 4: APLICAR PADDING\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. APLICAR PADDING A LAS SECUENCIAS\")\nprint(\"=\" * 80)\n\npadded_sequences = pad_sequences(\n    sequences,\n    maxlen=MAX_SEQUENCE_LENGTH,\n    padding='post',\n    truncating='post'\n)\n\nprint(f\"‚úì Padding aplicado\")\nprint(f\"  Forma final: {padded_sequences.shape}\")\nprint(f\"  Longitud m√°xima: {MAX_SEQUENCE_LENGTH}\")\nprint(f\"  Tipo de padding: post (al final)\")\nprint(f\"  Tipo de truncado: post (desde el final)\")\n\n# Ejemplo de secuencia paddeada\nprint(f\"\\nEjemplo de secuencia paddeada:\")\nprint(f\"  Original (len={len(example_seq)}): {example_seq[:15]}...\")\nprint(f\"  Padded (len={len(padded_sequences[0])}): {padded_sequences[0][:15]}...\")\n\n# PASO 5: CARGAR MODELOS DE EMBEDDINGS PRE-ENTRENADOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"5. CARGAR MODELOS DE EMBEDDINGS PRE-ENTRENADOS\")\nprint(\"=\" * 80)\n\n# Cargar Word2Vec\nprint(\"\\n--- Word2Vec ---\")\ntry:\n    w2v_model = Word2Vec.load('models/word2vec_model.model')\n    print(f\"‚úì Modelo Word2Vec cargado\")\n    print(f\"  Dimensi√≥n: {w2v_model.wv.vector_size}\")\n    print(f\"  Vocabulario: {len(w2v_model.wv)} palabras\")\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontr√≥ 'word2vec_model.model'\")\n    print(\"Por favor, ejecuta primero la secci√≥n 7 (Word Embeddings)\")\n    raise\n\n# Cargar FastText\nprint(\"\\n--- FastText ---\")\ntry:\n    ft_model = FastText.load('models/fasttext_model.model')\n    print(f\"‚úì Modelo FastText cargado\")\n    print(f\"  Dimensi√≥n: {ft_model.wv.vector_size}\")\n    print(f\"  Vocabulario: {len(ft_model.wv)} palabras\")\nexcept FileNotFoundError:\n    print(\"ERROR: No se encontr√≥ 'fasttext_model.model'\")\n    print(\"Por favor, ejecuta primero la secci√≥n 7 (Word Embeddings)\")\n    raise\n\n# PASO 6: CREAR MATRICES DE EMBEDDINGS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"6. CREAR MATRICES DE EMBEDDINGS PRE-ENTRENADAS\")\nprint(\"=\" * 80)\n\ndef create_embedding_matrix(word_index, embedding_model, embedding_dim, max_words):\n    \"\"\"\n    Crea matriz de embeddings a partir de un modelo pre-entrenado\n    \n    Args:\n        word_index: Diccionario palabra->√≠ndice del tokenizer\n        embedding_model: Modelo Word2Vec o FastText\n        embedding_dim: Dimensi√≥n de los embeddings\n        max_words: N√∫mero m√°ximo de palabras a incluir\n    \n    Returns:\n        embedding_matrix: Matriz numpy de forma (vocab_size, embedding_dim)\n    \"\"\"\n    vocab_size = min(len(word_index) + 1, max_words)\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n    \n    found_words = 0\n    missing_words = 0\n    \n    for word, idx in word_index.items():\n        if idx >= max_words:\n            continue\n        \n        try:\n            # Intentar obtener el vector de la palabra\n            embedding_vector = embedding_model.wv[word]\n            embedding_matrix[idx] = embedding_vector\n            found_words += 1\n        except KeyError:\n            # Palabra no encontrada, dejar como vector de ceros\n            missing_words += 1\n    \n    return embedding_matrix, found_words, missing_words\n\n# Crear matriz de Word2Vec\nprint(\"\\n--- Matriz de embeddings Word2Vec ---\")\nembedding_dim_w2v = w2v_model.wv.vector_size\nembedding_matrix_w2v, found_w2v, missing_w2v = create_embedding_matrix(\n    word_index, w2v_model, embedding_dim_w2v, MAX_NUM_WORDS\n)\n\nprint(f\"‚úì Matriz creada: {embedding_matrix_w2v.shape}\")\nprint(f\"  Palabras encontradas: {found_w2v}\")\nprint(f\"  Palabras no encontradas (OOV): {missing_w2v}\")\nprint(f\"  Cobertura: {found_w2v/(found_w2v+missing_w2v)*100:.2f}%\")\n\n# Crear matriz de FastText\nprint(\"\\n--- Matriz de embeddings FastText ---\")\nembedding_dim_ft = ft_model.wv.vector_size\nembedding_matrix_ft, found_ft, missing_ft = create_embedding_matrix(\n    word_index, ft_model, embedding_dim_ft, MAX_NUM_WORDS\n)\n\nprint(f\"‚úì Matriz creada: {embedding_matrix_ft.shape}\")\nprint(f\"  Palabras encontradas: {found_ft}\")\nprint(f\"  Palabras no encontradas (OOV): {missing_ft}\")\nprint(f\"  Cobertura: {found_ft/(found_ft+missing_ft)*100:.2f}%\")\n\n# PASO 7: PREPARAR SPLITS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"7. PREPARAR SPLITS TRAIN/VAL/TEST\")\nprint(\"=\" * 80)\n\n# Cargar dataset original para hacer match\ndf_original = pd.read_csv('data/initial_data.csv')\n\n# Cargar splits\nconsistency_train = pd.read_csv('data_processed/consistency_train.csv')\nconsistency_val = pd.read_csv('data_processed/consistency_val.csv')\nconsistency_test = pd.read_csv('data_processed/consistency_test.csv')\n\nsentiment_train = pd.read_csv('data_processed/sentiment_train.csv')\nsentiment_val = pd.read_csv('data_processed/sentiment_val.csv')\nsentiment_test = pd.read_csv('data_processed/sentiment_test.csv')\n\ndef get_split_indices(split_df, df_original_ref):\n    \"\"\"Obtiene √≠ndices del dataset original para un split\"\"\"\n    indices = []\n    split_sentences = split_df['Sentence'].values\n    df_original_indexed = df_original_ref.reset_index(drop=True)\n    \n    for sentence in split_sentences:\n        matches = df_original_indexed[df_original_indexed['Sentence'] == sentence].index\n        if len(matches) > 0:\n            indices.append(matches[0])\n    \n    return indices\n\n# Obtener √≠ndices para cada split\nprint(\"\\nObteniendo √≠ndices de splits...\")\n\nindices_consistency_train = get_split_indices(consistency_train, df_original)\nindices_consistency_val = get_split_indices(consistency_val, df_original)\nindices_consistency_test = get_split_indices(consistency_test, df_original)\n\nindices_sentiment_train = get_split_indices(sentiment_train, df_original)\nindices_sentiment_val = get_split_indices(sentiment_val, df_original)\nindices_sentiment_test = get_split_indices(sentiment_test, df_original)\n\nprint(f\"‚úì √çndices obtenidos:\")\nprint(f\"  Consistency - Train: {len(indices_consistency_train)}, Val: {len(indices_consistency_val)}, Test: {len(indices_consistency_test)}\")\nprint(f\"  Sentiment - Train: {len(indices_sentiment_train)}, Val: {len(indices_sentiment_val)}, Test: {len(indices_sentiment_test)}\")\n\n# PASO 8: GUARDAR TODOS LOS DATOS PREPARADOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"8. GUARDAR DATOS PREPARADOS\")\nprint(\"=\" * 80)\n\n# Guardar secuencias paddeadas\nnp.savez_compressed(\n    'data_processed/sequences_padded.npz',\n    sequences=padded_sequences,\n    # Guardar tambi√©n los √≠ndices de splits\n    consistency_train_idx=indices_consistency_train,\n    consistency_val_idx=indices_consistency_val,\n    consistency_test_idx=indices_consistency_test,\n    sentiment_train_idx=indices_sentiment_train,\n    sentiment_val_idx=indices_sentiment_val,\n    sentiment_test_idx=indices_sentiment_test\n)\nprint(\"‚úì Secuencias paddeadas guardadas: data_processed/sequences_padded.npz\")\n\n# Guardar tokenizer\nwith open('models/tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\nprint(\"‚úì Tokenizer guardado: models/tokenizer.pkl\")\n\n# Guardar matrices de embeddings\nnp.save('models/embedding_matrix_w2v.npy', embedding_matrix_w2v)\nprint(\"‚úì Matriz Word2Vec guardada: models/embedding_matrix_w2v.npy\")\n\nnp.save('models/embedding_matrix_ft.npy', embedding_matrix_ft)\nprint(\"‚úì Matriz FastText guardada: models/embedding_matrix_ft.npy\")\n\n# Guardar informaci√≥n de configuraci√≥n\nconfig = {\n    'max_num_words': MAX_NUM_WORDS,\n    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n    'vocab_size': vocab_size,\n    'embedding_dim_w2v': embedding_dim_w2v,\n    'embedding_dim_ft': embedding_dim_ft,\n    'w2v_coverage': found_w2v/(found_w2v+missing_w2v)*100,\n    'ft_coverage': found_ft/(found_ft+missing_ft)*100\n}\n\nwith open('models/sequences_config.pkl', 'wb') as f:\n    pickle.dump(config, f)\nprint(\"‚úì Configuraci√≥n guardada: models/sequences_config.pkl\")\n\n# PASO 9: VERIFICACI√ìN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"9. VERIFICACI√ìN Y RESUMEN\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì Ejemplo de carga y uso:\")\nprint(f\"\\n# Cargar secuencias\")\nprint(f\"data = np.load('data_processed/sequences_padded.npz')\")\nprint(f\"sequences = data['sequences']\")\nprint(f\"train_idx = data['consistency_train_idx']\")\nprint(f\"\\n# Cargar tokenizer\")\nprint(f\"with open('models/tokenizer.pkl', 'rb') as f:\")\nprint(f\"    tokenizer = pickle.load(f)\")\nprint(f\"\\n# Cargar matriz de embeddings\")\nprint(f\"embedding_matrix = np.load('models/embedding_matrix_w2v.npy')\")\n\n# Mostrar ejemplo pr√°ctico\nprint(f\"\\n‚úì Verificaci√≥n con datos de consistencia (train):\")\nX_train_cons = padded_sequences[indices_consistency_train]\ny_train_cons = consistency_train['Etiqueta'].values\nprint(f\"  X_train shape: {X_train_cons.shape}\")\nprint(f\"  y_train shape: {y_train_cons.shape}\")\nprint(f\"  Primera secuencia (primeros 20 tokens): {X_train_cons[0][:20]}\")\n\n# RESUMEN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RESUMEN FINAL - TAREA 4\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì TAREA 4 COMPLETADA\")\nprint(f\"\\nDatos preparados:\")\nprint(f\"  1. Secuencias num√©ricas paddeadas: {padded_sequences.shape}\")\nprint(f\"  2. Tokenizer de Keras (vocab={min(MAX_NUM_WORDS, vocab_size)})\")\nprint(f\"  3. Matriz Word2Vec: {embedding_matrix_w2v.shape} (cobertura: {found_w2v/(found_w2v+missing_w2v)*100:.2f}%)\")\nprint(f\"  4. Matriz FastText: {embedding_matrix_ft.shape} (cobertura: {found_ft/(found_ft+missing_ft)*100:.2f}%)\")\nprint(f\"  5. √çndices de splits guardados para ambas tareas\")\n\nprint(f\"\\nPar√°metros configurados:\")\nprint(f\"  - Vocabulario m√°ximo: {MAX_NUM_WORDS}\")\nprint(f\"  - Longitud de secuencia: {MAX_SEQUENCE_LENGTH}\")\nprint(f\"  - Dimensi√≥n embeddings: {embedding_dim_w2v}\")\nprint(f\"  - Token OOV: {OOV_TOKEN}\")\n\nprint(f\"\\nArchivos generados:\")\nprint(f\"  1. data_processed/sequences_padded.npz\")\nprint(f\"  2. models/tokenizer.pkl\")\nprint(f\"  3. models/embedding_matrix_w2v.npy\")\nprint(f\"  4. models/embedding_matrix_ft.npy\")\nprint(f\"  5. models/sequences_config.pkl\")\n\nprint(f\"\\nCobertura de vocabulario:\")\nprint(f\"  - Word2Vec: {found_w2v} palabras encontradas, {missing_w2v} OOV ({found_w2v/(found_w2v+missing_w2v)*100:.2f}% cobertura)\")\nprint(f\"  - FastText: {found_ft} palabras encontradas, {missing_ft} OOV ({found_ft/(found_ft+missing_ft)*100:.2f}% cobertura)\")\nprint(f\"  - FastText tiene mejor cobertura por usar subword information\")\n\nprint(f\"\\nLos datos est√°n listos para entrenar modelos LSTM y CNN en las siguientes tareas.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oqyxw00e2lg",
   "source": "## 14. TAREA 5: Deep Learning - LSTM con Word2Vec (Consistencia)\n\n**Objetivo**: Entrenar modelos LSTM para detecci√≥n de consistencia comparando 3 configuraciones de embeddings.\n\nEn esta secci√≥n vamos a:\n1. Cargar secuencias preparadas y matriz de embeddings Word2Vec\n2. Crear arquitectura LSTM para clasificaci√≥n binaria (correcta/incorrecta)\n3. Entrenar con **3 configuraciones de embeddings**:\n   - **Frozen**: Embeddings congelados (trainable=False) - usa conocimiento pre-entrenado\n   - **Fine-tuned**: Embeddings entrenables (trainable=True) - adapta embeddings a la tarea\n   - **From scratch**: Sin inicializaci√≥n pre-entrenada - aprende desde cero\n4. Comparar rendimiento, curvas de aprendizaje y tiempos de entrenamiento\n5. Analizar cu√°ndo usar cada configuraci√≥n\n\n**Tarea**: Detecci√≥n de Consistencia (correcta vs incorrecta)  \n**Arquitectura**: LSTM  \n**Embeddings**: Word2Vec (100 dimensiones)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zplhnogi7ad",
   "source": "import pandas as pd\nimport numpy as np\nimport pickle\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, History\nfrom tensorflow.keras.utils import to_categorical\n\n# Establecer semillas para reproducibilidad\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nprint(\"=\" * 80)\nprint(\"TAREA 5: LSTM CON WORD2VEC - DETECCI√ìN DE CONSISTENCIA\")\nprint(\"=\" * 80)\n\n# PASO 1: CARGAR DATOS PREPARADOS\nprint(\"\\n1. CARGANDO DATOS PREPARADOS\")\nprint(\"-\" * 80)\n\n# Cargar secuencias paddeadas\ndata = np.load('data_processed/sequences_padded.npz')\nsequences = data['sequences']\nprint(f\"‚úì Secuencias cargadas: {sequences.shape}\")\n\n# Cargar √≠ndices de splits para consistencia\ntrain_idx = data['consistency_train_idx']\nval_idx = data['consistency_val_idx']\ntest_idx = data['consistency_test_idx']\n\nprint(f\"‚úì √çndices de splits cargados:\")\nprint(f\"  Train: {len(train_idx)} muestras\")\nprint(f\"  Validation: {len(val_idx)} muestras\")\nprint(f\"  Test: {len(test_idx)} muestras\")\n\n# Cargar matriz de embeddings Word2Vec\nembedding_matrix_w2v = np.load('models/embedding_matrix_w2v.npy')\nprint(f\"‚úì Matriz de embeddings Word2Vec cargada: {embedding_matrix_w2v.shape}\")\n\n# Cargar configuraci√≥n\nwith open('models/sequences_config.pkl', 'rb') as f:\n    config = pickle.load(f)\n\nMAX_SEQUENCE_LENGTH = config['max_sequence_length']\nVOCAB_SIZE = config['vocab_size']\nEMBEDDING_DIM = config['embedding_dim_w2v']\n\nprint(f\"\\nConfiguraci√≥n:\")\nprint(f\"  Vocab size: {VOCAB_SIZE}\")\nprint(f\"  Sequence length: {MAX_SEQUENCE_LENGTH}\")\nprint(f\"  Embedding dim: {EMBEDDING_DIM}\")\n\n# PASO 2: PREPARAR DATOS DE ENTRENAMIENTO\nprint(\"\\n2. PREPARAR DATOS PARA ENTRENAMIENTO\")\nprint(\"-\" * 80)\n\n# Cargar etiquetas\nconsistency_train = pd.read_csv('data_processed/consistency_train.csv')\nconsistency_val = pd.read_csv('data_processed/consistency_val.csv')\nconsistency_test = pd.read_csv('data_processed/consistency_test.csv')\n\n# Extraer X e y\nX_train = sequences[train_idx]\nX_val = sequences[val_idx]\nX_test = sequences[test_idx]\n\n# Convertir etiquetas a binario (correcta=0, incorrecta=1)\nlabel_map = {'correcta': 0, 'incorrecta': 1}\ny_train = np.array([label_map.get(label, 0) for label in consistency_train['Etiqueta'].values])\ny_val = np.array([label_map.get(label, 0) for label in consistency_val['Etiqueta'].values])\ny_test = np.array([label_map.get(label, 0) for label in consistency_test['Etiqueta'].values])\n\nprint(f\"‚úì Datos preparados:\")\nprint(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\nprint(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")\n\nprint(f\"\\nDistribuci√≥n de clases en train:\")\nunique, counts = np.unique(y_train, return_counts=True)\nfor label, count in zip(unique, counts):\n    label_name = 'correcta' if label == 0 else 'incorrecta'\n    print(f\"  {label_name}: {count} ({count/len(y_train)*100:.2f}%)\")\n\n# PASO 3: DEFINIR ARQUITECTURA LSTM\nprint(\"\\n\" + \"=\" * 80)\nprint(\"3. DEFINIR ARQUITECTURA LSTM\")\nprint(\"=\" * 80)\n\ndef create_lstm_model(vocab_size, embedding_dim, sequence_length, \n                      embedding_matrix=None, trainable=True):\n    \"\"\"\n    Crea modelo LSTM para clasificaci√≥n binaria\n    \n    Args:\n        vocab_size: Tama√±o del vocabulario\n        embedding_dim: Dimensi√≥n de embeddings\n        sequence_length: Longitud de secuencias\n        embedding_matrix: Matriz de embeddings pre-entrenados (opcional)\n        trainable: Si los embeddings son entrenables\n    \n    Returns:\n        model: Modelo Keras compilado\n    \"\"\"\n    model = Sequential([\n        # Capa de Embedding\n        Embedding(\n            input_dim=vocab_size,\n            output_dim=embedding_dim,\n            input_length=sequence_length,\n            weights=[embedding_matrix] if embedding_matrix is not None else None,\n            trainable=trainable,\n            name='embedding'\n        ),\n        \n        # Capa LSTM\n        LSTM(64, return_sequences=False, name='lstm'),\n        \n        # Dropout para regularizaci√≥n\n        Dropout(0.3, name='dropout'),\n        \n        # Capa densa intermedia\n        Dense(32, activation='relu', name='dense'),\n        \n        # Capa de salida (clasificaci√≥n binaria)\n        Dense(1, activation='sigmoid', name='output')\n    ])\n    \n    # Compilar modelo\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nprint(\"‚úì Arquitectura LSTM definida:\")\nprint(\"\\n  Embedding Layer ‚Üí LSTM(64) ‚Üí Dropout(0.3) ‚Üí Dense(32) ‚Üí Dense(1)\")\nprint(\"\\nPar√°metros:\")\nprint(\"  - LSTM units: 64\")\nprint(\"  - Dropout rate: 0.3\")\nprint(\"  - Dense units: 32\")\nprint(\"  - Activation: sigmoid (clasificaci√≥n binaria)\")\nprint(\"  - Optimizer: Adam (lr=0.001)\")\nprint(\"  - Loss: binary_crossentropy\")\n\n# PASO 4: ENTRENAR MODELOS CON DIFERENTES CONFIGURACIONES\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. ENTRENAR MODELOS - 3 CONFIGURACIONES\")\nprint(\"=\" * 80)\n\n# Configuraci√≥n de entrenamiento\nEPOCHS = 20\nBATCH_SIZE = 32\nEARLY_STOPPING_PATIENCE = 3\n\n# Callback de early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=EARLY_STOPPING_PATIENCE,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# Diccionario para almacenar resultados\nresults = {}\nhistories = {}\n\n# CONFIGURACI√ìN 1: FROZEN EMBEDDINGS\nprint(\"\\n\" + \"-\" * 80)\nprint(\"CONFIGURACI√ìN 1: EMBEDDINGS CONGELADOS (FROZEN)\")\nprint(\"-\" * 80)\nprint(\"Los embeddings NO se actualizan durante el entrenamiento\")\nprint(\"Ventaja: M√°s r√°pido, menos par√°metros, usa conocimiento pre-entrenado\")\n\nmodel_frozen = create_lstm_model(\n    vocab_size=VOCAB_SIZE,\n    embedding_dim=EMBEDDING_DIM,\n    sequence_length=MAX_SEQUENCE_LENGTH,\n    embedding_matrix=embedding_matrix_w2v,\n    trainable=False  # FROZEN\n)\n\nprint(f\"\\nTotal de par√°metros:\")\nmodel_frozen.summary()\n\nprint(\"\\nEntrenando modelo FROZEN...\")\nstart_time = time.time()\nhistory_frozen = model_frozen.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopping],\n    verbose=1\n)\ntraining_time_frozen = time.time() - start_time\n\nprint(f\"‚úì Entrenamiento completado en {training_time_frozen:.2f}s\")\n\n# Evaluar en validation\ny_val_pred_frozen = (model_frozen.predict(X_val) > 0.5).astype(int).flatten()\nacc_frozen = accuracy_score(y_val, y_val_pred_frozen)\nf1_frozen = f1_score(y_val, y_val_pred_frozen, average='binary')\n\nresults['frozen'] = {\n    'model': model_frozen,\n    'history': history_frozen,\n    'accuracy': acc_frozen,\n    'f1_score': f1_frozen,\n    'training_time': training_time_frozen\n}\nhistories['frozen'] = history_frozen\n\nprint(f\"Resultados en Validation:\")\nprint(f\"  Accuracy: {acc_frozen:.4f}\")\nprint(f\"  F1-Score: {f1_frozen:.4f}\")\n\n# CONFIGURACI√ìN 2: FINE-TUNED EMBEDDINGS\nprint(\"\\n\" + \"-\" * 80)\nprint(\"CONFIGURACI√ìN 2: EMBEDDINGS FINE-TUNED\")\nprint(\"-\" * 80)\nprint(\"Los embeddings se actualizan durante el entrenamiento\")\nprint(\"Ventaja: Adapta embeddings a la tarea espec√≠fica, mejor rendimiento\")\n\nmodel_finetuned = create_lstm_model(\n    vocab_size=VOCAB_SIZE,\n    embedding_dim=EMBEDDING_DIM,\n    sequence_length=MAX_SEQUENCE_LENGTH,\n    embedding_matrix=embedding_matrix_w2v,\n    trainable=True  # FINE-TUNED\n)\n\nprint(f\"\\nTotal de par√°metros:\")\nmodel_finetuned.summary()\n\nprint(\"\\nEntrenando modelo FINE-TUNED...\")\nstart_time = time.time()\nhistory_finetuned = model_finetuned.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopping],\n    verbose=1\n)\ntraining_time_finetuned = time.time() - start_time\n\nprint(f\"‚úì Entrenamiento completado en {training_time_finetuned:.2f}s\")\n\n# Evaluar en validation\ny_val_pred_finetuned = (model_finetuned.predict(X_val) > 0.5).astype(int).flatten()\nacc_finetuned = accuracy_score(y_val, y_val_pred_finetuned)\nf1_finetuned = f1_score(y_val, y_val_pred_finetuned, average='binary')\n\nresults['finetuned'] = {\n    'model': model_finetuned,\n    'history': history_finetuned,\n    'accuracy': acc_finetuned,\n    'f1_score': f1_finetuned,\n    'training_time': training_time_finetuned\n}\nhistories['finetuned'] = history_finetuned\n\nprint(f\"Resultados en Validation:\")\nprint(f\"  Accuracy: {acc_finetuned:.4f}\")\nprint(f\"  F1-Score: {f1_finetuned:.4f}\")\n\n# CONFIGURACI√ìN 3: FROM SCRATCH\nprint(\"\\n\" + \"-\" * 80)\nprint(\"CONFIGURACI√ìN 3: FROM SCRATCH (SIN PRE-ENTRENAMIENTO)\")\nprint(\"-\" * 80)\nprint(\"Los embeddings se inicializan aleatoriamente y se aprenden desde cero\")\nprint(\"Ventaja: No depende de embeddings externos, √∫til con vocabulario muy espec√≠fico\")\n\nmodel_scratch = create_lstm_model(\n    vocab_size=VOCAB_SIZE,\n    embedding_dim=EMBEDDING_DIM,\n    sequence_length=MAX_SEQUENCE_LENGTH,\n    embedding_matrix=None,  # Sin pre-entrenamiento\n    trainable=True\n)\n\nprint(f\"\\nTotal de par√°metros:\")\nmodel_scratch.summary()\n\nprint(\"\\nEntrenando modelo FROM SCRATCH...\")\nstart_time = time.time()\nhistory_scratch = model_scratch.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopping],\n    verbose=1\n)\ntraining_time_scratch = time.time() - start_time\n\nprint(f\"‚úì Entrenamiento completado en {training_time_scratch:.2f}s\")\n\n# Evaluar en validation\ny_val_pred_scratch = (model_scratch.predict(X_val) > 0.5).astype(int).flatten()\nacc_scratch = accuracy_score(y_val, y_val_pred_scratch)\nf1_scratch = f1_score(y_val, y_val_pred_scratch, average='binary')\n\nresults['scratch'] = {\n    'model': model_scratch,\n    'history': history_scratch,\n    'accuracy': acc_scratch,\n    'f1_score': f1_scratch,\n    'training_time': training_time_scratch\n}\nhistories['scratch'] = history_scratch\n\nprint(f\"Resultados en Validation:\")\nprint(f\"  Accuracy: {acc_scratch:.4f}\")\nprint(f\"  F1-Score: {f1_scratch:.4f}\")\n\n# PASO 5: COMPARAR RESULTADOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"5. COMPARACI√ìN DE RESULTADOS\")\nprint(\"=\" * 80)\n\ncomparison_df = pd.DataFrame({\n    'Configuraci√≥n': ['Frozen', 'Fine-tuned', 'From Scratch'],\n    'Accuracy': [acc_frozen, acc_finetuned, acc_scratch],\n    'F1-Score': [f1_frozen, f1_finetuned, f1_scratch],\n    'Tiempo (s)': [training_time_frozen, training_time_finetuned, training_time_scratch]\n})\n\nprint(\"\\nTabla comparativa:\")\nprint(comparison_df.to_string(index=False))\n\n# Identificar mejor configuraci√≥n\nbest_config = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Configuraci√≥n']\nprint(f\"\\n‚úì Mejor configuraci√≥n seg√∫n F1-Score: {best_config}\")\n\n# PASO 6: VISUALIZAR CURVAS DE APRENDIZAJE\nprint(\"\\n\" + \"=\" * 80)\nprint(\"6. CURVAS DE APRENDIZAJE\")\nprint(\"=\" * 80)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nfig.suptitle('Curvas de Aprendizaje - LSTM Word2Vec (Consistencia)', fontsize=16, fontweight='bold')\n\nconfigs = ['frozen', 'finetuned', 'scratch']\ntitles = ['Frozen', 'Fine-tuned', 'From Scratch']\n\nfor idx, (config, title) in enumerate(zip(configs, titles)):\n    history = histories[config]\n    \n    # Plot loss\n    axes[idx].plot(history.history['loss'], label='Train Loss', linewidth=2)\n    axes[idx].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    axes[idx].set_title(f'{title}\\nF1: {results[config][\"f1_score\"]:.4f}')\n    axes[idx].set_xlabel('Epoch')\n    axes[idx].set_ylabel('Loss')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.97])\nplt.savefig('charts/07_lstm_w2v_learning_curves.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Curvas guardadas: charts/07_lstm_w2v_learning_curves.png\")\nplt.show()\n\n# PASO 7: EVALUACI√ìN EN TEST SET\nprint(\"\\n\" + \"=\" * 80)\nprint(\"7. EVALUACI√ìN EN TEST SET\")\nprint(\"=\" * 80)\n\nbest_config_key = best_config.lower().replace(' ', '')\nbest_model = results[best_config_key]['model']\n\ny_test_pred = (best_model.predict(X_test) > 0.5).astype(int).flatten()\n\nprint(f\"\\nResultados del mejor modelo ({best_config}) en Test:\")\nprint(classification_report(y_test, y_test_pred, target_names=['correcta', 'incorrecta']))\n\n# Matriz de confusi√≥n\ncm = confusion_matrix(y_test, y_test_pred)\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n            xticklabels=['correcta', 'incorrecta'],\n            yticklabels=['correcta', 'incorrecta'])\nax.set_title(f'Matriz de Confusi√≥n - {best_config} (Test Set)')\nax.set_ylabel('True Label')\nax.set_xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig('charts/07_lstm_w2v_confusion_matrix.png', dpi=300, bbox_inches='tight')\nprint(\"‚úì Matriz de confusi√≥n guardada: charts/07_lstm_w2v_confusion_matrix.png\")\nplt.show()\n\n# PASO 8: GUARDAR MODELOS\nprint(\"\\n\" + \"=\" * 80)\nprint(\"8. GUARDAR MODELOS\")\nprint(\"=\" * 80)\n\nmodel_frozen.save('models/w2v_lstm_frozen.h5')\nprint(\"‚úì Modelo frozen guardado: models/w2v_lstm_frozen.h5\")\n\nmodel_finetuned.save('models/w2v_lstm_finetuned.h5')\nprint(\"‚úì Modelo fine-tuned guardado: models/w2v_lstm_finetuned.h5\")\n\nmodel_scratch.save('models/w2v_lstm_scratch.h5')\nprint(\"‚úì Modelo from scratch guardado: models/w2v_lstm_scratch.h5\")\n\n# Guardar resultados\ncomparison_df.to_csv('models/w2v_lstm_results.csv', index=False)\nprint(\"‚úì Resultados guardados: models/w2v_lstm_results.csv\")\n\n# RESUMEN FINAL\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RESUMEN FINAL - TAREA 5\")\nprint(\"=\" * 80)\n\nprint(f\"\\n‚úì TAREA 5 COMPLETADA\")\nprint(f\"\\nMejor configuraci√≥n: {best_config}\")\nprint(f\"F1-Score (test): {f1_score(y_test, y_test_pred, average='binary'):.4f}\")\n\nprint(f\"\\nComparaci√≥n de configuraciones:\")\nprint(comparison_df.to_string(index=False))\n\nprint(f\"\\nAn√°lisis y recomendaciones:\")\nprint(f\"\\n1. FROZEN (Embeddings congelados):\")\nprint(f\"   ‚Ä¢ M√°s r√°pido de entrenar\")\nprint(f\"   ‚Ä¢ Menos par√°metros (menor riesgo de overfitting)\")\nprint(f\"   ‚Ä¢ √ötil cuando: dataset peque√±o, recursos limitados\")\n\nprint(f\"\\n2. FINE-TUNED (Embeddings ajustables):\")\nprint(f\"   ‚Ä¢ Mejor rendimiento en la tarea espec√≠fica\")\nprint(f\"   ‚Ä¢ Adapta embeddings al dominio\")\nprint(f\"   ‚Ä¢ √ötil cuando: dataset mediano/grande, task-specific vocabulary\")\n\nprint(f\"\\n3. FROM SCRATCH (Sin pre-entrenamiento):\")\nprint(f\"   ‚Ä¢ Aprende todo desde cero\")\nprint(f\"   ‚Ä¢ Requiere m√°s datos y tiempo\")\nprint(f\"   ‚Ä¢ √ötil cuando: vocabulario muy espec√≠fico, embeddings no relevantes\")\n\nprint(f\"\\nArchivos generados:\")\nprint(f\"  1. 3 modelos entrenados (.h5)\")\nprint(f\"  2. Curvas de aprendizaje\")\nprint(f\"  3. Matriz de confusi√≥n\")\nprint(f\"  4. Tabla de resultados (CSV)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}