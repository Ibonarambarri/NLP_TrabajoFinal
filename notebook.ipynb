{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad860f77",
   "metadata": {},
   "source": [
    "PRIMER PASO: BARAJEAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa38f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original cargado: 5842 noticias\n",
      "Distribución de sentimientos:\n",
      "Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "\n",
      "Noticias positivas disponibles: 1852\n",
      "Noticias negativas disponibles: 860\n",
      "\n",
      "Creando 5842 mezclas de sentimientos opuestos...\n",
      "  Procesadas 1000/5842...\n",
      "  Procesadas 2000/5842...\n",
      "  Procesadas 3000/5842...\n",
      "  Procesadas 4000/5842...\n",
      "  Procesadas 5000/5842...\n",
      "\n",
      "============================================================\n",
      "ESTADÍSTICAS DEL DATASET FINAL\n",
      "============================================================\n",
      "Total de filas: 11684\n",
      "Noticias correctas: 5842 (50.0%)\n",
      "Noticias incorrectas: 5842 (50.0%)\n",
      "\n",
      "Distribución de sentimientos en noticias CORRECTAS:\n",
      "Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de sentimientos en noticias INCORRECTAS:\n",
      "Sentiment\n",
      "positive    2939\n",
      "negative    2903\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "EJEMPLOS\n",
      "============================================================\n",
      "\n",
      "[NOTICIA CORRECTA]\n",
      "Sentimiento: neutral\n",
      "The presentation material can be viewed on the company 's website in English after the conference .\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 1]\n",
      "Sentimiento: negative\n",
      "End Of Day Scan: Stochastic Overbought $JDST $FREE $BLRX $LPG $AFSI $DVAX $HDGE $AHT $ALR $ASMB  www, Tallink claims the watertight doors of both Vana Tallinn and Regina Baltica, are fully in working order .\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 2]\n",
      "Sentimiento: positive\n",
      "Lloyds to cut 945 jobs as part of three-year restructuring strategy, bypasses Elcoteq Tallinn.\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 3]\n",
      "Sentimiento: positive\n",
      "4 million liters, Export declined by 6 percent to 16, 8 mln ) for the first nine months of 2007 from 37.\n",
      "\n",
      "============================================================\n",
      "✓ Dataset guardado como 'data/dataset_mezclado_final.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Leer el dataset original desde la carpeta data\n",
    "df = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "print(f\"Dataset original cargado: {len(df)} noticias\")\n",
    "print(f\"Distribución de sentimientos:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Crear una copia para las noticias correctas (50%)\n",
    "df_correctas = df.copy()\n",
    "df_correctas['Etiqueta'] = 'correcta'\n",
    "\n",
    "# Separar por sentimientos para mezclas\n",
    "df_positive = df[df['Sentiment'] == 'positive']\n",
    "df_negative = df[df['Sentiment'] == 'negative']\n",
    "\n",
    "print(f\"\\nNoticias positivas disponibles: {len(df_positive)}\")\n",
    "print(f\"Noticias negativas disponibles: {len(df_negative)}\")\n",
    "\n",
    "# Función para extraer fragmentos de una noticia\n",
    "def extraer_fragmentos(noticia):\n",
    "    \"\"\"\n",
    "    Extrae fragmentos de una noticia dividiéndola por comas o puntos\n",
    "    \"\"\"\n",
    "    # Primero intentar dividir por comas\n",
    "    fragmentos = [p.strip() for p in noticia.split(',') if p.strip()]\n",
    "    \n",
    "    # Si no hay suficientes fragmentos, dividir por puntos\n",
    "    if len(fragmentos) <= 1:\n",
    "        fragmentos = [p.strip() for p in noticia.split('.') if p.strip()]\n",
    "    \n",
    "    return fragmentos\n",
    "\n",
    "# Función para crear mezclas de sentimientos OPUESTOS\n",
    "def crear_mezclas_opuestas(df_positive, df_negative, num_mezclas):\n",
    "    \"\"\"\n",
    "    Crea mezclas incorrectas combinando noticias positivas con negativas\n",
    "    \"\"\"\n",
    "    noticias_mezcladas = []\n",
    "    \n",
    "    print(f\"\\nCreando {num_mezclas} mezclas de sentimientos opuestos...\")\n",
    "    \n",
    "    for i in range(num_mezclas):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Procesadas {i + 1}/{num_mezclas}...\")\n",
    "        \n",
    "        # Seleccionar una noticia positiva y una negativa\n",
    "        idx_pos = random.randint(0, len(df_positive) - 1)\n",
    "        idx_neg = random.randint(0, len(df_negative) - 1)\n",
    "        \n",
    "        noticia_pos = df_positive.iloc[idx_pos]['Sentence']\n",
    "        noticia_neg = df_negative.iloc[idx_neg]['Sentence']\n",
    "        \n",
    "        # Extraer fragmentos\n",
    "        frag_pos = extraer_fragmentos(noticia_pos)\n",
    "        frag_neg = extraer_fragmentos(noticia_neg)\n",
    "        \n",
    "        # Seleccionar 1-2 fragmentos de cada una\n",
    "        if len(frag_pos) > 0 and len(frag_neg) > 0:\n",
    "            num_pos = min(random.randint(1, 2), len(frag_pos))\n",
    "            num_neg = min(random.randint(1, 2), len(frag_neg))\n",
    "            \n",
    "            fragmentos_seleccionados = (\n",
    "                random.sample(frag_pos, num_pos) + \n",
    "                random.sample(frag_neg, num_neg)\n",
    "            )\n",
    "            \n",
    "            # Mezclar el orden\n",
    "            random.shuffle(fragmentos_seleccionados)\n",
    "            \n",
    "            noticia_mezclada = ', '.join(fragmentos_seleccionados)\n",
    "            if noticia_mezclada and noticia_mezclada[-1] not in '.!?':\n",
    "                noticia_mezclada += '.'\n",
    "            \n",
    "            # Asignar sentimiento aleatorio entre los dos opuestos\n",
    "            sentiment = random.choice(['positive', 'negative'])\n",
    "            \n",
    "            noticias_mezcladas.append({\n",
    "                'Sentence': noticia_mezclada,\n",
    "                'Sentiment': sentiment,\n",
    "                'Etiqueta': 'incorrecta'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(noticias_mezcladas)\n",
    "\n",
    "# Crear el mismo número de noticias incorrectas que correctas\n",
    "num_mezclas = len(df_correctas)\n",
    "df_incorrectas = crear_mezclas_opuestas(df_positive, df_negative, num_mezclas)\n",
    "\n",
    "# Combinar ambos datasets\n",
    "df_final = pd.concat([df_correctas, df_incorrectas], ignore_index=True)\n",
    "\n",
    "# Mezclar aleatoriamente el dataset final\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ESTADÍSTICAS DEL DATASET FINAL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de filas: {len(df_final)}\")\n",
    "print(f\"Noticias correctas: {len(df_final[df_final['Etiqueta'] == 'correcta'])} ({len(df_final[df_final['Etiqueta'] == 'correcta'])/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Noticias incorrectas: {len(df_final[df_final['Etiqueta'] == 'incorrecta'])} ({len(df_final[df_final['Etiqueta'] == 'incorrecta'])/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución de sentimientos en noticias CORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'correcta']['Sentiment'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribución de sentimientos en noticias INCORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'incorrecta']['Sentiment'].value_counts())\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EJEMPLOS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n[NOTICIA CORRECTA]\")\n",
    "ejemplo_correcta = df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_correcta[:300] + (\"...\" if len(ejemplo_correcta) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 1]\")\n",
    "ejemplo_inc1 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_inc1[:300] + (\"...\" if len(ejemplo_inc1) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 2]\")\n",
    "ejemplo_inc2 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentiment']}\")\n",
    "print(ejemplo_inc2[:300] + (\"...\" if len(ejemplo_inc2) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 3]\")\n",
    "ejemplo_inc3 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentiment']}\")\n",
    "print(ejemplo_inc3[:300] + (\"...\" if len(ejemplo_inc3) > 300 else \"\"))\n",
    "\n",
    "# Guardar el dataset final\n",
    "output_path = 'data/dataset_mezclado_final.csv'\n",
    "df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Dataset guardado como '{output_path}'\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bee4a",
   "metadata": {},
   "source": [
    "TRADUCCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f280f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deep-translator in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (1.11.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from deep-translator) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from deep-translator) (2.32.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.6.15)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Iniciando traducción del dataset...\n",
      "Total de filas a procesar: 11684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/11684 [00:11<4:37:29,  1.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Traducir SOLO la columna 'Sentence' (primera columna)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Las columnas 'Sentiment' y 'Etiqueta' permanecen iguales\u001b[39;00m\n\u001b[32m     59\u001b[39m texto_original = df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtraducir_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midioma_elegido\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Añadir el idioma a la nueva columna\u001b[39;00m\n\u001b[32m     63\u001b[39m df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mIdioma\u001b[39m\u001b[33m'\u001b[39m] = nombres_idiomas[idioma_elegido]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtraducir_texto\u001b[39m\u001b[34m(texto, idioma_destino)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texto\n\u001b[32m     41\u001b[39m translator = GoogleTranslator(source=\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m, target=idioma_destino)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m traduccion = \u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m time.sleep(\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Pausa para evitar límites de la API\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m traduccion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\deep_translator\\google.py:67\u001b[39m, in \u001b[36mGoogleTranslator.translate\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.payload_key:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m._url_params[\u001b[38;5;28mself\u001b[39m.payload_key] = text\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_url_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Instalar librerías necesarias\n",
    "!pip install deep-translator pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Leer el dataset original\n",
    "df = pd.read_csv('data/dataset_mezclado_final.csv')\n",
    "\n",
    "# Definir los idiomas a los que traducir (códigos ISO)\n",
    "idiomas = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'eu', 'gl']  # inglés, francés, alemán, italiano, portugués, catalán, euskera, gallego\n",
    "\n",
    "# Diccionario para nombres completos de idiomas\n",
    "nombres_idiomas = {\n",
    "    'es': 'español',\n",
    "    'en': 'inglés',\n",
    "    'fr': 'francés',\n",
    "    'de': 'alemán',\n",
    "    'it': 'italiano',\n",
    "    'pt': 'portugués',\n",
    "    'ca': 'catalán',\n",
    "    'eu': 'euskera',\n",
    "    'gl': 'gallego'\n",
    "}\n",
    "\n",
    "# Crear una copia del dataframe\n",
    "df_traducido = df.copy()\n",
    "\n",
    "# Crear nueva columna para el idioma\n",
    "df_traducido['Idioma'] = ''\n",
    "\n",
    "# Función para traducir con manejo de errores\n",
    "def traducir_texto(texto, idioma_destino):\n",
    "    try:\n",
    "        if idioma_destino == 'es':  # Si es español, no traducir\n",
    "            return texto\n",
    "        translator = GoogleTranslator(source='es', target=idioma_destino)\n",
    "        traduccion = translator.translate(texto)\n",
    "        time.sleep(0.5)  # Pausa para evitar límites de la API\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        print(f\"Error traduciendo a {idioma_destino}: {e}\")\n",
    "        return texto  # Si falla, devolver el texto original\n",
    "\n",
    "# Traducir SOLO la primera columna (Sentence) de cada fila a un idioma aleatorio\n",
    "print(\"Iniciando traducción del dataset...\")\n",
    "print(f\"Total de filas a procesar: {len(df_traducido)}\")\n",
    "\n",
    "for idx in tqdm(range(len(df_traducido))):\n",
    "    # Seleccionar un idioma aleatorio (incluyendo español para mantener algunas filas originales)\n",
    "    idioma_elegido = random.choice(idiomas + ['es', 'es', 'es'])  # Mayor probabilidad de español\n",
    "    \n",
    "    # Traducir SOLO la columna 'Sentence' (primera columna)\n",
    "    # Las columnas 'Sentiment' y 'Etiqueta' permanecen iguales\n",
    "    texto_original = df_traducido.loc[idx, 'Sentence']\n",
    "    df_traducido.loc[idx, 'Sentence'] = traducir_texto(texto_original, idioma_elegido)\n",
    "    \n",
    "    # Añadir el idioma a la nueva columna\n",
    "    df_traducido.loc[idx, 'Idioma'] = nombres_idiomas[idioma_elegido]\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "output_path = 'data/dataset_multiidioma.csv'\n",
    "df_traducido.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n✓ Dataset traducido guardado en: {output_path}\")\n",
    "print(f\"Total de filas: {len(df_traducido)}\")\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "print(df_traducido.head())\n",
    "print(f\"\\nDistribución de idiomas:\")\n",
    "print(df_traducido['Idioma'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
