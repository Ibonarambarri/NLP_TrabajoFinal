{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad860f77",
   "metadata": {},
   "source": [
    "PRIMER PASO: BARAJEAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa38f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original cargado: 5842 noticias\n",
      "Distribución de sentimientos:\n",
      "Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "\n",
      "Noticias positivas disponibles: 1852\n",
      "Noticias negativas disponibles: 860\n",
      "\n",
      "Creando 5842 mezclas de sentimientos opuestos...\n",
      "  Procesadas 1000/5842...\n",
      "  Procesadas 2000/5842...\n",
      "  Procesadas 3000/5842...\n",
      "  Procesadas 4000/5842...\n",
      "  Procesadas 5000/5842...\n",
      "\n",
      "============================================================\n",
      "ESTADÍSTICAS DEL DATASET FINAL\n",
      "============================================================\n",
      "Total de filas: 11684\n",
      "Noticias correctas: 5842 (50.0%)\n",
      "Noticias incorrectas: 5842 (50.0%)\n",
      "\n",
      "Distribución de sentimientos en noticias CORRECTAS:\n",
      "Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de sentimientos en noticias INCORRECTAS:\n",
      "Sentiment\n",
      "positive    2939\n",
      "negative    2903\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "EJEMPLOS\n",
      "============================================================\n",
      "\n",
      "[NOTICIA CORRECTA]\n",
      "Sentimiento: neutral\n",
      "The presentation material can be viewed on the company 's website in English after the conference .\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 1]\n",
      "Sentimiento: negative\n",
      "End Of Day Scan: Stochastic Overbought $JDST $FREE $BLRX $LPG $AFSI $DVAX $HDGE $AHT $ALR $ASMB  www, Tallink claims the watertight doors of both Vana Tallinn and Regina Baltica, are fully in working order .\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 2]\n",
      "Sentimiento: positive\n",
      "Lloyds to cut 945 jobs as part of three-year restructuring strategy, bypasses Elcoteq Tallinn.\n",
      "\n",
      "[NOTICIA INCORRECTA - Ejemplo 3]\n",
      "Sentimiento: positive\n",
      "4 million liters, Export declined by 6 percent to 16, 8 mln ) for the first nine months of 2007 from 37.\n",
      "\n",
      "============================================================\n",
      "✓ Dataset guardado como 'data/dataset_mezclado_final.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Leer el dataset original desde la carpeta data\n",
    "df = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "print(f\"Dataset original cargado: {len(df)} noticias\")\n",
    "print(f\"Distribución de sentimientos:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Crear una copia para las noticias correctas (50%)\n",
    "df_correctas = df.copy()\n",
    "df_correctas['Etiqueta'] = 'correcta'\n",
    "\n",
    "# Separar por sentimientos para mezclas\n",
    "df_positive = df[df['Sentiment'] == 'positive']\n",
    "df_negative = df[df['Sentiment'] == 'negative']\n",
    "\n",
    "print(f\"\\nNoticias positivas disponibles: {len(df_positive)}\")\n",
    "print(f\"Noticias negativas disponibles: {len(df_negative)}\")\n",
    "\n",
    "# Función para extraer fragmentos de una noticia\n",
    "def extraer_fragmentos(noticia):\n",
    "    \"\"\"\n",
    "    Extrae fragmentos de una noticia dividiéndola por comas o puntos\n",
    "    \"\"\"\n",
    "    # Primero intentar dividir por comas\n",
    "    fragmentos = [p.strip() for p in noticia.split(',') if p.strip()]\n",
    "    \n",
    "    # Si no hay suficientes fragmentos, dividir por puntos\n",
    "    if len(fragmentos) <= 1:\n",
    "        fragmentos = [p.strip() for p in noticia.split('.') if p.strip()]\n",
    "    \n",
    "    return fragmentos\n",
    "\n",
    "# Función para crear mezclas de sentimientos OPUESTOS\n",
    "def crear_mezclas_opuestas(df_positive, df_negative, num_mezclas):\n",
    "    \"\"\"\n",
    "    Crea mezclas incorrectas combinando noticias positivas con negativas\n",
    "    \"\"\"\n",
    "    noticias_mezcladas = []\n",
    "    \n",
    "    print(f\"\\nCreando {num_mezclas} mezclas de sentimientos opuestos...\")\n",
    "    \n",
    "    for i in range(num_mezclas):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Procesadas {i + 1}/{num_mezclas}...\")\n",
    "        \n",
    "        # Seleccionar una noticia positiva y una negativa\n",
    "        idx_pos = random.randint(0, len(df_positive) - 1)\n",
    "        idx_neg = random.randint(0, len(df_negative) - 1)\n",
    "        \n",
    "        noticia_pos = df_positive.iloc[idx_pos]['Sentence']\n",
    "        noticia_neg = df_negative.iloc[idx_neg]['Sentence']\n",
    "        \n",
    "        # Extraer fragmentos\n",
    "        frag_pos = extraer_fragmentos(noticia_pos)\n",
    "        frag_neg = extraer_fragmentos(noticia_neg)\n",
    "        \n",
    "        # Seleccionar 1-2 fragmentos de cada una\n",
    "        if len(frag_pos) > 0 and len(frag_neg) > 0:\n",
    "            num_pos = min(random.randint(1, 2), len(frag_pos))\n",
    "            num_neg = min(random.randint(1, 2), len(frag_neg))\n",
    "            \n",
    "            fragmentos_seleccionados = (\n",
    "                random.sample(frag_pos, num_pos) + \n",
    "                random.sample(frag_neg, num_neg)\n",
    "            )\n",
    "            \n",
    "            # Mezclar el orden\n",
    "            random.shuffle(fragmentos_seleccionados)\n",
    "            \n",
    "            noticia_mezclada = ', '.join(fragmentos_seleccionados)\n",
    "            if noticia_mezclada and noticia_mezclada[-1] not in '.!?':\n",
    "                noticia_mezclada += '.'\n",
    "            \n",
    "            # Asignar sentimiento aleatorio entre los dos opuestos\n",
    "            sentiment = random.choice(['positive', 'negative'])\n",
    "            \n",
    "            noticias_mezcladas.append({\n",
    "                'Sentence': noticia_mezclada,\n",
    "                'Sentiment': sentiment,\n",
    "                'Etiqueta': 'incorrecta'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(noticias_mezcladas)\n",
    "\n",
    "# Crear el mismo número de noticias incorrectas que correctas\n",
    "num_mezclas = len(df_correctas)\n",
    "df_incorrectas = crear_mezclas_opuestas(df_positive, df_negative, num_mezclas)\n",
    "\n",
    "# Combinar ambos datasets\n",
    "df_final = pd.concat([df_correctas, df_incorrectas], ignore_index=True)\n",
    "\n",
    "# Mezclar aleatoriamente el dataset final\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ESTADÍSTICAS DEL DATASET FINAL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de filas: {len(df_final)}\")\n",
    "print(f\"Noticias correctas: {len(df_final[df_final['Etiqueta'] == 'correcta'])} ({len(df_final[df_final['Etiqueta'] == 'correcta'])/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Noticias incorrectas: {len(df_final[df_final['Etiqueta'] == 'incorrecta'])} ({len(df_final[df_final['Etiqueta'] == 'incorrecta'])/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución de sentimientos en noticias CORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'correcta']['Sentiment'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribución de sentimientos en noticias INCORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'incorrecta']['Sentiment'].value_counts())\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EJEMPLOS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n[NOTICIA CORRECTA]\")\n",
    "ejemplo_correcta = df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_correcta[:300] + (\"...\" if len(ejemplo_correcta) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 1]\")\n",
    "ejemplo_inc1 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_inc1[:300] + (\"...\" if len(ejemplo_inc1) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 2]\")\n",
    "ejemplo_inc2 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentiment']}\")\n",
    "print(ejemplo_inc2[:300] + (\"...\" if len(ejemplo_inc2) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 3]\")\n",
    "ejemplo_inc3 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentiment']}\")\n",
    "print(ejemplo_inc3[:300] + (\"...\" if len(ejemplo_inc3) > 300 else \"\"))\n",
    "\n",
    "# Guardar el dataset final\n",
    "output_path = 'data/dataset_mezclado_final.csv'\n",
    "df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Dataset guardado como '{output_path}'\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bee4a",
   "metadata": {},
   "source": [
    "TRADUCCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f280f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deep-translator in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (1.11.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from deep-translator) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from deep-translator) (2.32.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.6.15)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Iniciando traducción del dataset...\n",
      "Total de filas a procesar: 11684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/11684 [00:11<4:37:29,  1.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Traducir SOLO la columna 'Sentence' (primera columna)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Las columnas 'Sentiment' y 'Etiqueta' permanecen iguales\u001b[39;00m\n\u001b[32m     59\u001b[39m texto_original = df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtraducir_texto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midioma_elegido\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Añadir el idioma a la nueva columna\u001b[39;00m\n\u001b[32m     63\u001b[39m df_traducido.loc[idx, \u001b[33m'\u001b[39m\u001b[33mIdioma\u001b[39m\u001b[33m'\u001b[39m] = nombres_idiomas[idioma_elegido]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtraducir_texto\u001b[39m\u001b[34m(texto, idioma_destino)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texto\n\u001b[32m     41\u001b[39m translator = GoogleTranslator(source=\u001b[33m'\u001b[39m\u001b[33mes\u001b[39m\u001b[33m'\u001b[39m, target=idioma_destino)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m traduccion = \u001b[43mtranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m time.sleep(\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Pausa para evitar límites de la API\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m traduccion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\deep_translator\\google.py:67\u001b[39m, in \u001b[36mGoogleTranslator.translate\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.payload_key:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m._url_params[\u001b[38;5;28mself\u001b[39m.payload_key] = text\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_base_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_url_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Instalar librerías necesarias\n",
    "!pip install deep-translator pandas tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Leer el dataset original\n",
    "df = pd.read_csv('data/dataset_mezclado_final.csv')\n",
    "\n",
    "# Definir los idiomas a los que traducir (códigos ISO)\n",
    "idiomas = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'eu', 'gl']  # inglés, francés, alemán, italiano, portugués, catalán, euskera, gallego\n",
    "\n",
    "# Diccionario para nombres completos de idiomas\n",
    "nombres_idiomas = {\n",
    "    'es': 'español',\n",
    "    'en': 'inglés',\n",
    "    'fr': 'francés',\n",
    "    'de': 'alemán',\n",
    "    'it': 'italiano',\n",
    "    'pt': 'portugués',\n",
    "    'ca': 'catalán',\n",
    "    'eu': 'euskera',\n",
    "    'gl': 'gallego'\n",
    "}\n",
    "\n",
    "# Crear una copia del dataframe\n",
    "df_traducido = df.copy()\n",
    "\n",
    "# Crear nueva columna para el idioma\n",
    "df_traducido['Idioma'] = ''\n",
    "\n",
    "# Función para traducir con manejo de errores\n",
    "def traducir_texto(texto, idioma_destino):\n",
    "    try:\n",
    "        if idioma_destino == 'es':  # Si es español, no traducir\n",
    "            return texto\n",
    "        translator = GoogleTranslator(source='es', target=idioma_destino)\n",
    "        traduccion = translator.translate(texto)\n",
    "        time.sleep(0.5)  # Pausa para evitar límites de la API\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        print(f\"Error traduciendo a {idioma_destino}: {e}\")\n",
    "        return texto  # Si falla, devolver el texto original\n",
    "\n",
    "# Traducir SOLO la primera columna (Sentence) de cada fila a un idioma aleatorio\n",
    "print(\"Iniciando traducción del dataset...\")\n",
    "print(f\"Total de filas a procesar: {len(df_traducido)}\")\n",
    "\n",
    "for idx in tqdm(range(len(df_traducido))):\n",
    "    # Seleccionar un idioma aleatorio (incluyendo español para mantener algunas filas originales)\n",
    "    idioma_elegido = random.choice(idiomas + ['es', 'es', 'es'])  # Mayor probabilidad de español\n",
    "    \n",
    "    # Traducir SOLO la columna 'Sentence' (primera columna)\n",
    "    # Las columnas 'Sentiment' y 'Etiqueta' permanecen iguales\n",
    "    texto_original = df_traducido.loc[idx, 'Sentence']\n",
    "    df_traducido.loc[idx, 'Sentence'] = traducir_texto(texto_original, idioma_elegido)\n",
    "    \n",
    "    # Añadir el idioma a la nueva columna\n",
    "    df_traducido.loc[idx, 'Idioma'] = nombres_idiomas[idioma_elegido]\n",
    "\n",
    "\n",
    "\n",
    "# Guardar el nuevo dataset\n",
    "output_path = 'data/dataset_multiidioma.csv'\n",
    "df_traducido.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n✓ Dataset traducido guardado en: {output_path}\")\n",
    "print(f\"Total de filas: {len(df_traducido)}\")\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "print(df_traducido.head())\n",
    "print(f\"\\nDistribución de idiomas:\")\n",
    "print(df_traducido['Idioma'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549c3f4",
   "metadata": {},
   "source": [
    "1. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5423e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (2.2.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aleja\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Análisis Exploratorio de Datos...\n",
      "============================================================\n",
      "✨ RESUMEN DEL DATASET ✨\n",
      "============================================================\n",
      "\n",
      "1. ESTRUCTURA Y CONTEO DE VALORES NO NULOS (df.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11684 entries, 0 to 11683\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Sentence   11684 non-null  object\n",
      " 1   Sentiment  11684 non-null  object\n",
      " 2   Etiqueta   11684 non-null  object\n",
      " 3   Idioma     11684 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 365.2+ KB\n",
      "\n",
      "2. ESTADÍSTICAS DESCRIPTIVAS (df.describe(include='all')):\n",
      "------------------------------------------------------------\n",
      "         Sentence Sentiment  Etiqueta   Idioma\n",
      "count       11684     11684     11684    11684\n",
      "unique      11602         3         2        9\n",
      "top     700 Model  positive  correcta  español\n",
      "freq            3      4791      5842     3176\n",
      "\n",
      "3. DISTRIBUCIONES CLAVE (Conteo y Porcentaje):\n",
      "\n",
      "--- Distribución de: Sentiment ---\n",
      "           Conteo  Porcentaje (%)\n",
      "Sentiment                        \n",
      "positive     4791           41.00\n",
      "negative     3763           32.21\n",
      "neutral      3130           26.79\n",
      "\n",
      "--- Distribución de: Idioma ---\n",
      "           Conteo  Porcentaje (%)\n",
      "Idioma                           \n",
      "español      3176           27.18\n",
      "portugués    1126            9.64\n",
      "inglés       1100            9.41\n",
      "gallego      1063            9.10\n",
      "euskera      1057            9.05\n",
      "alemán       1056            9.04\n",
      "italiano     1048            8.97\n",
      "francés      1030            8.82\n",
      "catalán      1028            8.80\n",
      "\n",
      "4. ESTADÍSTICAS DE LONGITUD DE ORACIONES (En palabras):\n",
      "------------------------------------------------------------\n",
      "count    11684.00\n",
      "mean        23.53\n",
      "std         11.73\n",
      "min          1.00\n",
      "25%         15.00\n",
      "50%         22.00\n",
      "75%         30.00\n",
      "max         95.00\n",
      "Name: sentence_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carga y manejo del error de ruta\n",
    "print(\"Iniciando Análisis Exploratorio de Datos...\")\n",
    "try:\n",
    "    # Corrección: Intenta leer el archivo directamente desde el directorio actual\n",
    "    df = pd.read_csv('data/dataset_multiidioma.csv', encoding='utf-8')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: No se encontró 'dataset_multiidioma.csv'.\")\n",
    "    exit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"✨ RESUMEN DEL DATASET ✨\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 2. Estructura, Tipos de Datos y Nulos (Combinado en una sola llamada)\n",
    "print(\"\\n1. ESTRUCTURA Y CONTEO DE VALORES NO NULOS (df.info()):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n2. ESTADÍSTICAS DESCRIPTIVAS (df.describe(include='all')):\")\n",
    "print(\"-\" * 60)\n",
    "# Muestra estadísticas resumidas para todas las columnas\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# 3. Distribución de Variables Categóricas Clave\n",
    "print(\"\\n3. DISTRIBUCIONES CLAVE (Conteo y Porcentaje):\")\n",
    "for col in ['Sentiment', 'Idioma']:\n",
    "    # Combina Conteo Absoluto y Porcentaje en una sola tabla limpia\n",
    "    counts = df[col].value_counts().rename('Conteo')\n",
    "    percents = df[col].value_counts(normalize=True).mul(100).round(2).rename('Porcentaje (%)')\n",
    "    print(f\"\\n--- Distribución de: {col} ---\")\n",
    "    print(pd.concat([counts, percents], axis=1))\n",
    "\n",
    "# 4. Análisis de la Longitud de las Oraciones\n",
    "# Cálculo eficiente: cuenta palabras usando métodos vectorizados de pandas (.str)\n",
    "df['sentence_length'] = df['Sentence'].str.split().str.len()\n",
    "print(\"\\n4. ESTADÍSTICAS DE LONGITUD DE ORACIONES (En palabras):\")\n",
    "print(\"-\" * 60)\n",
    "# Utiliza .describe() para obtener media, mediana, min, max, std en una línea\n",
    "print(df['sentence_length'].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd023f0",
   "metadata": {},
   "source": [
    "2. Visualizaciones del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6c5dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1919, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_28348\\2986217461.py\", line 6, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py\", line 174, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py\", line 143, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# CÓDIGO DE ANÁLISIS EXPLORATORIO ESENCIAL (Sin comandos de instalación)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Importaciones\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:174\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    145\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    147\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# CÓDIGO DE ANÁLISIS EXPLORATORIO ESENCIAL (Sin comandos de instalación)\n",
    "# ====================================================================\n",
    "\n",
    "# Importaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- SIMULACIÓN DE DATOS (REEMPLAZA ESTA SECCIÓN CON TU CARGA DE DATOS) ---\n",
    "# Necesario para que el código corra sin errores si no tienes un archivo\n",
    "np.random.seed(42)\n",
    "data_size = 300\n",
    "data = {\n",
    "    'Sentiment': np.random.choice(['positive', 'negative', 'neutral'], size=data_size, p=[0.45, 0.35, 0.20]),\n",
    "    'Idioma': np.random.choice(['en', 'es', 'fr'], size=data_size, p=[0.6, 0.3, 0.1]),\n",
    "    'sentence_length': (np.random.normal(loc=15, scale=5, size=data_size)).astype(int).clip(min=5)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.loc[df['Sentiment'] == 'negative', 'sentence_length'] += 3\n",
    "df.loc[df['Sentiment'] == 'positive', 'sentence_length'] -= 1\n",
    "df['sentence_length'] = df['sentence_length'].clip(min=5) \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Configuración de estilo\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERANDO LAS 3 VISUALIZACIONES MÁS LÓGICAS PARA SENTIMIENTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear figura con 3 subplots en una sola fila\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Visualizaciones Clave: Sentimiento y Longitud de Oración', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribución de Sentimientos (Gráfico de Conteo)\n",
    "sns.countplot(x='Sentiment', data=df, ax=axes[0],\n",
    "              order=df['Sentiment'].value_counts().index)\n",
    "axes[0].set_title('1. Distribución de Sentimientos (Objetivo)', fontweight='bold')\n",
    "axes[0].set_xlabel('Sentimiento')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# 2. Distribución de Longitud de Oraciones (Histograma)\n",
    "sns.histplot(df['sentence_length'], bins=30, kde=True, ax=axes[1])\n",
    "media = df['sentence_length'].mean()\n",
    "axes[1].axvline(media, color='red', linestyle='--', label=f'Media: {media:.1f}')\n",
    "axes[1].set_title('2. Distribución de Longitud (Numérica)', fontweight='bold')\n",
    "axes[1].set_xlabel('Número de Palabras')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Longitud de Oraciones por Sentimiento (Boxplot)\n",
    "sns.boxplot(x='Sentiment', y='sentence_length', data=df, ax=axes[2])\n",
    "axes[2].set_title('3. Relación: Longitud vs. Sentimiento', fontweight='bold')\n",
    "axes[2].set_xlabel('Sentimiento')\n",
    "axes[2].set_ylabel('Número de Palabras')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "\n",
    "plt.savefig('analisis_esencial_sentimientos.png', dpi=300)\n",
    "print(\"\\n✓ Gráficos esenciales guardados como 'analisis_esencial_sentimientos.png'\")\n",
    "plt.show()\n",
    "\n",
    "# --- Estadísticas de la relación clave ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ESTADÍSTICAS DE LONGITUD POR SENTIMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "print(df.groupby('Sentiment')['sentence_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34d558",
   "metadata": {},
   "source": [
    "3. Preprocesamiento de Datos - Tokenización y Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d4cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando recursos NLTK esenciales...\n",
      "✓ Recursos NLTK (punkt, stopwords) listos.\n",
      "\n",
      "================================================================================\n",
      "PREPROCESAMIENTO DE DATOS SIMPLE Y ROBUSTO\n",
      "================================================================================\n",
      "\n",
      "1. APLICANDO PROCESO DE LIMPIEZA...\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Preprocesamiento completado en la columna 'tokens_processed'\n",
      "\n",
      "================================================================================\n",
      "RESULTADOS DEL PREPROCESAMIENTO Y ESTADÍSTICAS\n",
      "================================================================================\n",
      "\n",
      "✅ Columna de tokens procesados (Primeras 5 filas):\n",
      "                                            Sentence  \\\n",
      "0  The presentation material can be viewed on the...   \n",
      "1  Scansione di fine giornata: ipercomprato stoca...   \n",
      "2  EGUNERAZIOA 1-Lloydsek 945 lanpostu moztuko di...   \n",
      "3  Incap Contract Manufacturing Services Private ...   \n",
      "4  mas aproximando-se da linha de resistência de ...   \n",
      "\n",
      "                                    tokens_processed  \n",
      "0  [presentation, material, viewed, company, webs...  \n",
      "1  [scansione, fine, giornata, ipercomprato, stoc...  \n",
      "2  [egunerazioa, lloydsek, 945, lanpostu, moztuko...  \n",
      "3  [incap, contract, manufacturing, services, pri...  \n",
      "4  [mas, aproximando, linha, resistência, ruptura...  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ejemplo de transformación de la primera fila:\n",
      "  Texto Original: The presentation material can be viewed on the company 's website in English after the conference .\n",
      "  Tokens Finales: ['presentation', 'material', 'viewed', 'company', 'website', 'english', 'conference']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Estadísticas de Longitud:\n",
      "  Promedio de tokens originales: 27.06\n",
      "  Promedio de tokens procesados: 15.55\n",
      "  Reducción de ruido promedio: **42.53%**\n",
      "\n",
      "✓ Datos guardados en 'datos_preprocesados_simple.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# --- GESTIÓN DE NLTK ESENCIAL ---\n",
    "# El error está aquí, pero lo vamos a ignorar con el fallback en el código.\n",
    "try:\n",
    "    print(\"Verificando recursos NLTK esenciales...\")\n",
    "    # Intentamos descargar, pero el error de LookupError nos dice que a veces falla.\n",
    "    nltk.download('punkt', quiet=True, raise_on_error=False)\n",
    "    nltk.download('stopwords', quiet=True, raise_on_error=False)\n",
    "    print(\"✓ Recursos NLTK (punkt, stopwords) listos.\")\n",
    "except Exception:\n",
    "    print(\"ATENCIÓN: La descarga de recursos de NLTK falló. Usaremos tokenización simple.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# La importación de word_tokenize es necesaria, pero la usaremos dentro de un try/except.\n",
    "from nltk.tokenize import word_tokenize \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Función de tokenización robusta para usar como fallback\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Tokeniza de forma básica usando regex si NLTK falla.\"\"\"\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO DE DATOS SIMPLE Y ROBUSTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- SIMULACIÓN DE DATOS (REEMPLAZA ESTO CON TU CÓDIGO DE CARGA) ---\n",
    "try:\n",
    "    if 'df' not in locals():\n",
    "        print(\"Creando DataFrame de simulación (df)...\")\n",
    "        data = {\n",
    "            'Sentence': [\n",
    "                \"This is a great movie! Check out http://link.com #positive\",\n",
    "                \"¡Qué mal día! No me gustó nada. @user\",\n",
    "                \"neutral sentence with a question mark?\",\n",
    "                \"This is a very very long sentence with many unnecessary words and stop words.\",\n",
    "                \"La vida es bella. Es cierto.\",\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "except NameError:\n",
    "    pass\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Cargar stopwords de múltiples idiomas\n",
    "try:\n",
    "    STOPWORDS_ES = set(stopwords.words('spanish'))\n",
    "    STOPWORDS_EN = set(stopwords.words('english'))\n",
    "    STOPWORDS_ALL = STOPWORDS_ES.union(STOPWORDS_EN) \n",
    "except LookupError:\n",
    "    STOPWORDS_ALL = set()\n",
    "\n",
    "\n",
    "# Crear una copia del DataFrame para el proceso\n",
    "df_processed = df.copy()\n",
    "\n",
    "\n",
    "## 🎯 FUNCIÓN PRINCIPAL DE PREPROCESAMIENTO\n",
    "def preprocess_text(text, stopwords_set, min_len=2):\n",
    "    \"\"\"\n",
    "    Realiza limpieza, tokenización, minúsculas y eliminación de stopwords/puntuación\n",
    "    en una sola función.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return []\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. LIMPIEZA BÁSICA\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. TOKENIZACIÓN y CASE FOLDING\n",
    "    try:\n",
    "        # Intenta usar NLTK (más preciso)\n",
    "        tokens = word_tokenize(text.lower()) \n",
    "    except LookupError:\n",
    "        # Usa el tokenizer simple (robusto)\n",
    "        tokens = simple_tokenize(text)\n",
    "\n",
    "    # 3. ELIMINACIÓN DE PUNTUACIÓN, STOPWORDS y tokens muy cortos\n",
    "    tokens_final = [\n",
    "        token for token in tokens\n",
    "        if token not in string.punctuation \n",
    "        and token not in stopwords_set\n",
    "        and len(token) > min_len\n",
    "    ]\n",
    "    \n",
    "    return tokens_final\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 1. APLICAR PREPROCESAMIENTO Y CREAR COLUMNA DE COMPARACIÓN\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n1. APLICANDO PROCESO DE LIMPIEZA...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_processed['tokens_processed'] = df_processed['Sentence'].apply(\n",
    "    lambda x: preprocess_text(x, STOPWORDS_ALL)\n",
    ")\n",
    "\n",
    "# ⭐️ SOLUCIÓN AL LOOKUPERROR: Usar el tokenizer robusto (simple_tokenize) para\n",
    "# crear la columna de tokens originales, evitando la dependencia directa de NLTK.\n",
    "df_processed['tokens_original'] = df_processed['Sentence'].apply(\n",
    "     lambda x: simple_tokenize(str(x)) if not pd.isna(x) else []\n",
    ")\n",
    "\n",
    "print(\"✓ Preprocesamiento completado en la columna 'tokens_processed'\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 2. VISUALIZACIÓN Y RESULTADO FINAL\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DEL PREPROCESAMIENTO Y ESTADÍSTICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "## Muestra la columna de resultado\n",
    "print(\"\\n✅ Columna de tokens procesados (Primeras 5 filas):\")\n",
    "print(df_processed[['Sentence', 'tokens_processed']].head())\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "## Muestra un ejemplo de comparación\n",
    "if len(df_processed) > 0:\n",
    "    print(\"\\nEjemplo de transformación de la primera fila:\")\n",
    "    print(f\"  Texto Original: {df['Sentence'].iloc[0]}\")\n",
    "    print(f\"  Tokens Finales: {df_processed['tokens_processed'].iloc[0]}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "## Estadísticas de reducción\n",
    "df_processed['num_tokens_original'] = df_processed['tokens_original'].apply(len)\n",
    "df_processed['num_tokens_processed'] = df_processed['tokens_processed'].apply(len)\n",
    "\n",
    "mean_original = df_processed['num_tokens_original'].mean()\n",
    "mean_processed = df_processed['num_tokens_processed'].mean()\n",
    "reduction_percentage = ((mean_original - mean_processed) / mean_original * 100) if mean_original > 0 else 0\n",
    "\n",
    "print(f\"\\nEstadísticas de Longitud:\")\n",
    "print(f\"  Promedio de tokens originales: {mean_original:.2f}\")\n",
    "print(f\"  Promedio de tokens procesados: {mean_processed:.2f}\")\n",
    "print(f\"  Reducción de ruido promedio: **{reduction_percentage:.2f}%**\")\n",
    "\n",
    "# Guardar resultado final\n",
    "df_processed.to_csv('datos_preprocesados_simple.csv', index=False)\n",
    "print(f\"\\n✓ Datos guardados en 'datos_preprocesados_simple.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca1727",
   "metadata": {},
   "source": [
    "4. Lemmatization y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c11c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando recursos NLTK esenciales (wordnet)...\n",
      "✓ Recurso 'wordnet' listo.\n",
      "\n",
      "============================================================\n",
      "LEMMATIZATION Y STEMMING CORREGIDO\n",
      "============================================================\n",
      "\n",
      "1. STEMMING (Porter Stemmer - Inglés)\n",
      "------------------------------------------------------------\n",
      "✓ Stemming completado. Columna: 'tokens_stemmed'\n",
      "\n",
      "2. LEMMATIZATION (WordNet Lemmatizer)\n",
      "------------------------------------------------------------\n",
      "✓ Lemmatization completada. Columna: 'tokens_lemmatized'\n",
      "\n",
      "3. COMPARACIÓN DE RESULTADOS Y VOCABULARIO\n",
      "------------------------------------------------------------\n",
      "Token Original       Stemming             Lemmatization       \n",
      "------------------------------------------------------------\n",
      "presentation         present              presentation        \n",
      "material             materi               material            \n",
      "viewed               view                 view                \n",
      "\n",
      "Tamaño del vocabulario base: 30955\n",
      "Tamaño después de Stemming: 26963 (12.90% reducción)\n",
      "Tamaño después de Lemmatization: 29716 (4.00% reducción)\n",
      "\n",
      "✓ Datos guardados en 'datos_preprocesados_completo_corregido.csv'\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np # Necesario para la simulación\n",
    "\n",
    "# --- GESTIÓN NLTK ESENCIAL ---\n",
    "try:\n",
    "    print(\"Verificando recursos NLTK esenciales (wordnet)...\")\n",
    "    nltk.download('wordnet', quiet=True, raise_on_error=False)\n",
    "    print(\"✓ Recurso 'wordnet' listo.\")\n",
    "except Exception:\n",
    "    print(\"⚠️ ATENCIÓN: El recurso 'wordnet' de NLTK no está disponible.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEMMATIZATION Y STEMMING CORREGIDO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- SIMULACIÓN DEL DATAFRAME NECESARIO ---\n",
    "# ⚠️ ESTA SIMULACIÓN CREA EL DATAFRAME CON LA COLUMNA REQUERIDA. \n",
    "# Si estás ejecutando esto DESPUÉS del código de preprocesamiento anterior,\n",
    "# puedes ELIMINAR O COMENTAR esta sección.\n",
    "try:\n",
    "    if 'df_processed' not in locals() or 'tokens_processed' not in df_processed.columns:\n",
    "        print(\"Creando DataFrame de simulación con columna 'tokens_processed'...\")\n",
    "        df_processed = pd.DataFrame({\n",
    "            'tokens_processed': [\n",
    "                ['presentation', 'material', 'viewed', 'company', 'websites', 'running'],\n",
    "                ['scansione', 'fine', 'giornata', 'ipercomprato', 'testing'],\n",
    "                ['lloydsek', 'lanpostu', 'moztuko', 'ditu', 'runs'],\n",
    "                ['incap', 'contract', 'manufacturing', 'services', 'fairly'],\n",
    "                ['aproximando', 'linha', 'resistencia', 'culpa', 'acoes'],\n",
    "            ]\n",
    "        })\n",
    "except NameError:\n",
    "    pass\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Inicializar herramientas\n",
    "porter_stemmer = PorterStemmer()\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Crear lista de tokens de ejemplo para demostración\n",
    "sample_tokens = ['running', 'runs', 'ran', 'easily', 'fairly', 'testing', 'corpora']\n",
    "\n",
    "\n",
    "## 1. STEMMING (Porter Stemmer)\n",
    "print(\"\\n1. STEMMING (Porter Stemmer - Inglés)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ⭐️ CORRECCIÓN: Usamos 'tokens_processed' en lugar de 'tokens_no_stop'\n",
    "df_processed['tokens_stemmed'] = df_processed['tokens_processed'].apply(\n",
    "    lambda tokens: [porter_stemmer.stem(token) for token in tokens]\n",
    ")\n",
    "print(\"✓ Stemming completado. Columna: 'tokens_stemmed'\")\n",
    "\n",
    "\n",
    "## 2. LEMMATIZATION (WordNet Lemmatizer)\n",
    "print(\"\\n2. LEMMATIZATION (WordNet Lemmatizer)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ⭐️ CORRECCIÓN: Usamos 'tokens_processed' en lugar de 'tokens_no_stop'\n",
    "df_processed['tokens_lemmatized'] = df_processed['tokens_processed'].apply(\n",
    "    lambda tokens: [word_lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    ")\n",
    "print(\"✓ Lemmatization completada. Columna: 'tokens_lemmatized'\")\n",
    "\n",
    "\n",
    "## 3. COMPARACIÓN y VOCABULARIO (Usamos 'tokens_processed' como base)\n",
    "print(\"\\n3. COMPARACIÓN DE RESULTADOS Y VOCABULARIO\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Generamos la lista base para la comparación\n",
    "base_tokens = df_processed['tokens_processed']\n",
    "\n",
    "# Mostrar comparación del primer texto\n",
    "if not df_processed.empty:\n",
    "    original = base_tokens.iloc[0][:3]\n",
    "    stemmed = df_processed['tokens_stemmed'].iloc[0][:3]\n",
    "    lemmatized = df_processed['tokens_lemmatized'].iloc[0][:3]\n",
    "    \n",
    "    print(f\"{'Token Original':<20} {'Stemming':<20} {'Lemmatization':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(min(len(original), 3)):\n",
    "        print(f\"{original[i]:<20} {stemmed[i]:<20} {lemmatized[i]:<20}\")\n",
    "\n",
    "# Calcular y comparar el tamaño del vocabulario\n",
    "vocab_original = set(token for tokens in base_tokens for token in tokens)\n",
    "vocab_stemmed = set(token for tokens in df_processed['tokens_stemmed'] for token in tokens)\n",
    "vocab_lemmatized = set(token for tokens in df_processed['tokens_lemmatized'] for token in tokens)\n",
    "\n",
    "print(f\"\\nTamaño del vocabulario base: {len(vocab_original)}\")\n",
    "print(f\"Tamaño después de Stemming: {len(vocab_stemmed)} ({((len(vocab_original) - len(vocab_stemmed)) / len(vocab_original) * 100):.2f}% reducción)\")\n",
    "print(f\"Tamaño después de Lemmatization: {len(vocab_lemmatized)} ({((len(vocab_original) - len(vocab_lemmatized)) / len(vocab_original) * 100):.2f}% reducción)\")\n",
    "\n",
    "\n",
    "# 4. GUARDAR DATOS FINALES\n",
    "df_processed['text_stemmed'] = df_processed['tokens_stemmed'].apply(' '.join)\n",
    "df_processed['text_lemmatized'] = df_processed['tokens_lemmatized'].apply(' '.join)\n",
    "df_processed['text_processed_base'] = base_tokens.apply(' '.join)\n",
    "\n",
    "df_processed.to_csv('datos_preprocesados_completo_corregido.csv', index=False)\n",
    "print(f\"\\n✓ Datos guardados en 'datos_preprocesados_completo_corregido.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abc521",
   "metadata": {},
   "source": [
    "5. Representación Tradicional: Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9bf8988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1919, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\aleja\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_28348\\3197862149.py\", line 4, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py\", line 174, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py\", line 143, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"c:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Configuración de estilo\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:174\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colors.py:57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\scale.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\ticker.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    145\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    147\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aleja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración de estilo\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACIÓN: BAG OF WORDS (BoW) SIMPLIFICADA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 1. CONFIGURACIÓN Y TRANSFORMACIÓN ---\n",
    "print(\"\\n1. CREANDO MATRIZ BAG OF WORDS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# El estudiante debe elegir la columna que mejor funcionó (lemmatized, stemmed, processed)\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# Crear y configurar el vectorizador\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    # Parámetros esenciales para reducir el ruido\n",
    "    max_features=5000, \n",
    "    min_df=2, \n",
    "    max_df=0.8, \n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Ajustar (aprender vocabulario) y transformar (crear matriz)\n",
    "X_bow = bow_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"✓ Matriz BoW creada. Forma: {X_bow.shape}\")\n",
    "print(f\"Tamaño del Vocabulario Final: {X_bow.shape[1]}\")\n",
    "\n",
    "\n",
    "# --- 2. ANÁLISIS DE FRECUENCIAS GLOBALES ---\n",
    "print(\"\\n2. ANÁLISIS DE FRECUENCIAS (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular frecuencias de todos los términos en un solo paso\n",
    "word_freq = np.asarray(X_bow.sum(axis=0)).flatten()\n",
    "freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False).head(15)\n",
    "\n",
    "print(freq_df)\n",
    "\n",
    "# Visualización simple (usando Seaborn para un gráfico más estético)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='frequency', y='word', data=freq_df)\n",
    "plt.title(f'Top 15 Términos Más Frecuentes (BoW)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('bow_top_terms_simple.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3. ANÁLISIS POR SENTIMIENTO (Simplificado) ---\n",
    "print(\"\\n3. TÉRMINOS CLAVE POR SENTIMIENTO (Top 5 por Clase)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Recorrer cada sentimiento para obtener sus palabras clave\n",
    "for sentiment in df_processed['Sentiment'].unique():\n",
    "    # 1. Filtrar los documentos\n",
    "    docs_sentiment = df_processed[df_processed['Sentiment'] == sentiment][TEXT_COLUMN]\n",
    "    \n",
    "    # 2. Vectorizar solo esos documentos (usando el vocabulario ya aprendido)\n",
    "    X_sentiment = bow_vectorizer.transform(docs_sentiment)\n",
    "    \n",
    "    # 3. Calcular frecuencias\n",
    "    freq_sentiment = np.asarray(X_sentiment.sum(axis=0)).flatten()\n",
    "    \n",
    "    # 4. Crear DataFrame y mostrar TOP 5\n",
    "    freq_df_sentiment = pd.DataFrame({\n",
    "        'word': feature_names,\n",
    "        'frequency': freq_sentiment\n",
    "    }).sort_values('frequency', ascending=False).head(5)\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(freq_df_sentiment)\n",
    "\n",
    "\n",
    "# --- 4. PREPARACIÓN FINAL PARA EL MODELO ---\n",
    "print(\"\\n4. PREPARACIÓN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Guardar la matriz BoW con etiquetas (esencial para el entrenamiento)\n",
    "# Creamos un DataFrame a partir de la matriz dispersa\n",
    "X_dense = X_bow.toarray()\n",
    "bow_final_df = pd.DataFrame(X_dense, columns=feature_names)\n",
    "\n",
    "# Añadimos la columna de Sentimiento (Y) y el Idioma (si es necesario)\n",
    "bow_final_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "bow_final_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "bow_final_df.to_csv('datos_vectorizados_final.csv', index=False)\n",
    "print(f\"✓ Matriz de características (X) y Sentimiento (Y) guardada en 'datos_vectorizados_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f2760",
   "metadata": {},
   "source": [
    "6. Representación Tradicional: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bee5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VECTORIZACIÓN: TF-IDF FUNCIONAL (SIN VISUALIZACIÓN)\n",
      "============================================================\n",
      "\n",
      "1. CREANDO MATRIZ TF-IDF\n",
      "------------------------------------------------------------\n",
      "✓ Modelo TF-IDF creado. Forma: (11684, 5000)\n",
      "Tamaño del Vocabulario Final: 5000\n",
      "\n",
      "2. TÉRMINOS CON MAYOR PESO TF-IDF PROMEDIO (TOP 15)\n",
      "------------------------------------------------------------\n",
      "         term  tfidf_mean\n",
      "1625      eur    0.017926\n",
      "1696    euros    0.016214\n",
      "3400      per    0.011562\n",
      "68       2008    0.009391\n",
      "972   company    0.009384\n",
      "77       2009    0.009337\n",
      "1238      der    0.009115\n",
      "61       2007    0.008641\n",
      "1664     euro    0.008584\n",
      "1296      die    0.008503\n",
      "1621      eta    0.008433\n",
      "3657   profit    0.008405\n",
      "1251      des    0.008390\n",
      "3971    sales    0.007952\n",
      "3057      net    0.007597\n",
      "\n",
      "3. PREPARACIÓN FINAL DE DATOS\n",
      "------------------------------------------------------------\n",
      "✓ Matriz de características (X) y Sentimiento (Y) guardada en 'datos_tfidf_final.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ❌ Eliminadas: import matplotlib.pyplot as plt\n",
    "# ❌ Eliminadas: import seaborn as sns\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACIÓN: TF-IDF FUNCIONAL (SIN VISUALIZACIÓN)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Elige la columna de texto limpia y normalizada\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# --- 1. CREACIÓN Y TRANSFORMACIÓN TF-IDF ---\n",
    "print(\"\\n1. CREANDO MATRIZ TF-IDF\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Nota: El DataFrame df_processed debe estar definido\n",
    "try:\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "except NameError:\n",
    "    print(\"ERROR: El DataFrame 'df_processed' no está definido. Asegúrate de cargar los datos antes.\")\n",
    "    sys.exit() # Salir para evitar más errores\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"✓ Modelo TF-IDF creado. Forma: {X_tfidf.shape}\")\n",
    "print(f\"Tamaño del Vocabulario Final: {X_tfidf.shape[1]}\")\n",
    "\n",
    "\n",
    "# --- 2. ANÁLISIS DE TÉRMINOS CLAVE (SIN GRÁFICOS) ---\n",
    "print(\"\\n2. TÉRMINOS CON MAYOR PESO TF-IDF PROMEDIO (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular el peso promedio de cada término\n",
    "tfidf_means = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "tfidf_df = pd.DataFrame({\n",
    "    'term': tfidf_feature_names,\n",
    "    'tfidf_mean': tfidf_means\n",
    "}).sort_values('tfidf_mean', ascending=False).head(15)\n",
    "\n",
    "print(tfidf_df)\n",
    "\n",
    "# ❌ Eliminada la sección de plt.figure() y sns.barplot()\n",
    "\n",
    "\n",
    "# --- 3. PREPARACIÓN FINAL PARA EL MODELO ---\n",
    "print(\"\\n3. PREPARACIÓN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Guardar la matriz TF-IDF con etiquetas (X y Y)\n",
    "tfidf_matrix_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_feature_names)\n",
    "tfidf_matrix_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "\n",
    "tfidf_matrix_df.to_csv('datos_tfidf_final.csv', index=False)\n",
    "print(f\"✓ Matriz de características (X) y Sentimiento (Y) guardada en 'datos_tfidf_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d0e81",
   "metadata": {},
   "source": [
    "7. Word Embeddings No Contextuales - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b8cec",
   "metadata": {},
   "source": [
    "8. Word Embeddings No Contextuales - GloVe Pre-entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fce4ea",
   "metadata": {},
   "source": [
    "9. Word Embeddings No Contextuales - FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d617b",
   "metadata": {},
   "source": [
    "10. Word Embeddings Contextuales - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be1407",
   "metadata": {},
   "source": [
    "11. Embeddings Contextuales - Modelos Multilingües"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87a122",
   "metadata": {},
   "source": [
    "12. Comparación de Todas las Representaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6c1d6",
   "metadata": {},
   "source": [
    "13. Estructura Final de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe47b6",
   "metadata": {},
   "source": [
    "14. Evaluación de Calidad de Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
