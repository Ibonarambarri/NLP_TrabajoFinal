{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad860f77",
   "metadata": {},
   "source": [
    "## PRIMER PASO: BARAJEAR DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ksk6quf69i8",
   "metadata": {},
   "source": [
    "## SETUP - Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1wi59fg8n9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "!pip install deep-translator pandas tqdm seaborn nltk scikit-learn gensim matplotlib numpy transformers torch\n",
    "\n",
    "print(\"‚úì Todas las dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_folders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpetas para organizar los archivos del proyecto\n",
    "import os\n",
    "\n",
    "folders = ['data', 'data_processed', 'models', 'charts']\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "print(\"‚úì Estructura de carpetas creada:\")\n",
    "print(\"  üìÅ data/              - Datasets originales\")\n",
    "print(\"  üìÅ data_processed/    - Datos procesados (CSV)\")\n",
    "print(\"  üìÅ models/            - Modelos entrenados\")\n",
    "print(\"  üìÅ charts/            - Gr√°ficos generados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0d82a",
   "metadata": {},
   "source": [
    "## Mezclado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa38f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Cargar dataset original\n",
    "df = pd.read_csv('data/initial_data.csv')\n",
    "\n",
    "print(f\"Dataset original cargado: {len(df)} noticias\")\n",
    "print(f\"Distribuci√≥n de sentimientos:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Crear copia para noticias correctas (50%)\n",
    "df_correctas = df.copy()\n",
    "df_correctas['Etiqueta'] = 'correcta'\n",
    "\n",
    "# Separar noticias por sentimiento\n",
    "df_positive = df[df['Sentiment'] == 'positive']\n",
    "df_negative = df[df['Sentiment'] == 'negative']\n",
    "\n",
    "print(f\"\\nNoticias positivas disponibles: {len(df_positive)}\")\n",
    "print(f\"Noticias negativas disponibles: {len(df_negative)}\")\n",
    "\n",
    "# Extraer fragmentos de una noticia dividiendo por comas o puntos\n",
    "def extraer_fragmentos(noticia):\n",
    "    fragmentos = [p.strip() for p in noticia.split(',') if p.strip()]\n",
    "    \n",
    "    if len(fragmentos) <= 1:\n",
    "        fragmentos = [p.strip() for p in noticia.split('.') if p.strip()]\n",
    "    \n",
    "    return fragmentos\n",
    "\n",
    "# Crear noticias incorrectas mezclando fragmentos de sentimientos opuestos\n",
    "def crear_mezclas_opuestas(df_positive, df_negative, num_mezclas):\n",
    "    noticias_mezcladas = []\n",
    "    \n",
    "    print(f\"\\nCreando {num_mezclas} mezclas de sentimientos opuestos...\")\n",
    "    \n",
    "    for i in range(num_mezclas):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Procesadas {i + 1}/{num_mezclas}...\")\n",
    "        \n",
    "        # Seleccionar una noticia positiva y una negativa aleatoriamente\n",
    "        idx_pos = random.randint(0, len(df_positive) - 1)\n",
    "        idx_neg = random.randint(0, len(df_negative) - 1)\n",
    "        \n",
    "        noticia_pos = df_positive.iloc[idx_pos]['Sentence']\n",
    "        noticia_neg = df_negative.iloc[idx_neg]['Sentence']\n",
    "        \n",
    "        # Extraer fragmentos de cada noticia\n",
    "        frag_pos = extraer_fragmentos(noticia_pos)\n",
    "        frag_neg = extraer_fragmentos(noticia_neg)\n",
    "        \n",
    "        # Seleccionar 1-2 fragmentos de cada noticia\n",
    "        if len(frag_pos) > 0 and len(frag_neg) > 0:\n",
    "            num_pos = min(random.randint(1, 2), len(frag_pos))\n",
    "            num_neg = min(random.randint(1, 2), len(frag_neg))\n",
    "            \n",
    "            fragmentos_seleccionados = (\n",
    "                random.sample(frag_pos, num_pos) + \n",
    "                random.sample(frag_neg, num_neg)\n",
    "            )\n",
    "            \n",
    "            # Mezclar el orden de los fragmentos\n",
    "            random.shuffle(fragmentos_seleccionados)\n",
    "            \n",
    "            noticia_mezclada = ', '.join(fragmentos_seleccionados)\n",
    "            if noticia_mezclada and noticia_mezclada[-1] not in '.!?':\n",
    "                noticia_mezclada += '.'\n",
    "            \n",
    "            # Asignar sentimiento aleatorio\n",
    "            sentiment = random.choice(['positive', 'negative'])\n",
    "            \n",
    "            noticias_mezcladas.append({\n",
    "                'Sentence': noticia_mezclada,\n",
    "                'Sentiment': sentiment,\n",
    "                'Etiqueta': 'incorrecta'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(noticias_mezcladas)\n",
    "\n",
    "# Crear el mismo n√∫mero de noticias incorrectas que correctas\n",
    "num_mezclas = len(df_correctas)\n",
    "df_incorrectas = crear_mezclas_opuestas(df_positive, df_negative, num_mezclas)\n",
    "\n",
    "# Combinar datasets\n",
    "df_final = pd.concat([df_correctas, df_incorrectas], ignore_index=True)\n",
    "\n",
    "# Mezclar aleatoriamente\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Mostrar estad√≠sticas\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ESTAD√çSTICAS DEL DATASET FINAL\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total de filas: {len(df_final)}\")\n",
    "print(f\"Noticias correctas: {len(df_final[df_final['Etiqueta'] == 'correcta'])} ({len(df_final[df_final['Etiqueta'] == 'correcta'])/len(df_final)*100:.1f}%)\")\n",
    "print(f\"Noticias incorrectas: {len(df_final[df_final['Etiqueta'] == 'incorrecta'])} ({len(df_final[df_final['Etiqueta'] == 'incorrecta'])/len(df_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de sentimientos en noticias CORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'correcta']['Sentiment'].value_counts())\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de sentimientos en noticias INCORRECTAS:\")\n",
    "print(df_final[df_final['Etiqueta'] == 'incorrecta']['Sentiment'].value_counts())\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EJEMPLOS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n[NOTICIA CORRECTA]\")\n",
    "ejemplo_correcta = df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'correcta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_correcta[:300] + (\"...\" if len(ejemplo_correcta) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 1]\")\n",
    "ejemplo_inc1 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[0]['Sentiment']}\")\n",
    "print(ejemplo_inc1[:300] + (\"...\" if len(ejemplo_inc1) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 2]\")\n",
    "ejemplo_inc2 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[50]['Sentiment']}\")\n",
    "print(ejemplo_inc2[:300] + (\"...\" if len(ejemplo_inc2) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n[NOTICIA INCORRECTA - Ejemplo 3]\")\n",
    "ejemplo_inc3 = df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentence']\n",
    "print(f\"Sentimiento: {df_final[df_final['Etiqueta'] == 'incorrecta'].iloc[100]['Sentiment']}\")\n",
    "print(ejemplo_inc3[:300] + (\"...\" if len(ejemplo_inc3) > 300 else \"\"))\n",
    "\n",
    "# Guardar dataset final\n",
    "output_path = 'data/dataset_mezclado_final.csv'\n",
    "df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Dataset guardado como '{output_path}'\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bee4a",
   "metadata": {},
   "source": [
    "## TRADUCCI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir el dataset a m√∫ltiples idiomas usando Google Translator\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv('data/dataset_mezclado_final.csv')\n",
    "\n",
    "# Idiomas disponibles\n",
    "idiomas = ['en', 'fr', 'de', 'it', 'pt', 'ca', 'eu', 'gl']\n",
    "\n",
    "nombres_idiomas = {\n",
    "    'es': 'espa√±ol',\n",
    "    'en': 'ingl√©s',\n",
    "    'fr': 'franc√©s',\n",
    "    'de': 'alem√°n',\n",
    "    'it': 'italiano',\n",
    "    'pt': 'portugu√©s',\n",
    "    'ca': 'catal√°n',\n",
    "    'eu': 'euskera',\n",
    "    'gl': 'gallego'\n",
    "}\n",
    "\n",
    "df_traducido = df.copy()\n",
    "df_traducido['Idioma'] = ''\n",
    "\n",
    "# Traducir texto con manejo de errores\n",
    "def traducir_texto(texto, idioma_destino):\n",
    "    try:\n",
    "        if idioma_destino == 'es':\n",
    "            return texto\n",
    "        translator = GoogleTranslator(source='es', target=idioma_destino)\n",
    "        traduccion = translator.translate(texto)\n",
    "        time.sleep(0.5)\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        print(f\"Error traduciendo a {idioma_destino}: {e}\")\n",
    "        return texto\n",
    "\n",
    "# Traducir solo la columna 'Sentence' a un idioma aleatorio por fila\n",
    "print(\"Iniciando traducci√≥n del dataset...\")\n",
    "print(f\"Total de filas a procesar: {len(df_traducido)}\")\n",
    "\n",
    "for idx in tqdm(range(len(df_traducido))):\n",
    "    # Seleccionar idioma aleatorio (mayor probabilidad para espa√±ol)\n",
    "    idioma_elegido = random.choice(idiomas + ['es', 'es', 'es'])\n",
    "    \n",
    "    # Traducir la columna 'Sentence'\n",
    "    texto_original = df_traducido.loc[idx, 'Sentence']\n",
    "    df_traducido.loc[idx, 'Sentence'] = traducir_texto(texto_original, idioma_elegido)\n",
    "    df_traducido.loc[idx, 'Idioma'] = nombres_idiomas[idioma_elegido]\n",
    "\n",
    "# Guardar dataset traducido\n",
    "output_path = 'data/dataset_multiidioma.csv'\n",
    "df_traducido.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úì Dataset traducido guardado en: {output_path}\")\n",
    "print(f\"Total de filas: {len(df_traducido)}\")\n",
    "print(f\"\\nEstructura del dataset:\")\n",
    "print(df_traducido.head())\n",
    "print(f\"\\nDistribuci√≥n de idiomas:\")\n",
    "print(df_traducido['Idioma'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549c3f4",
   "metadata": {},
   "source": [
    "## 1. An√°lisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Iniciando An√°lisis Exploratorio de Datos...\")\n",
    "try:\n",
    "    df = pd.read_csv('data/dataset_multiidioma.csv', encoding='utf-8')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nERROR: No se encontr√≥ 'dataset_multiidioma.csv'.\")\n",
    "    exit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" RESUMEN DEL DATASET \")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica del dataset\n",
    "print(\"\\n1. ESTRUCTURA Y CONTEO DE VALORES NO NULOS:\")\n",
    "df.info()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n2. ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "print(\"-\" * 60)\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Distribuciones de variables categ√≥ricas\n",
    "print(\"\\n3. DISTRIBUCIONES CLAVE (Conteo y Porcentaje):\")\n",
    "for col in ['Sentiment', 'Etiqueta', 'Idioma']:\n",
    "    counts = df[col].value_counts().rename('Conteo')\n",
    "    percents = df[col].value_counts(normalize=True).mul(100).round(2).rename('Porcentaje (%)')\n",
    "    print(f\"\\n--- Distribuci√≥n de: {col} ---\")\n",
    "    print(pd.concat([counts, percents], axis=1))\n",
    "\n",
    "# Calcular longitud de oraciones en palabras\n",
    "df['sentence_length'] = df['Sentence'].str.split().str.len()\n",
    "print(\"\\n4. ESTAD√çSTICAS DE LONGITUD DE ORACIONES (En palabras):\")\n",
    "print(\"-\" * 60)\n",
    "print(df['sentence_length'].describe().round(2))\n",
    "\n",
    "# An√°lisis de valores nulos\n",
    "print(\"\\n5. AN√ÅLISIS DE VALORES NULOS:\")\n",
    "print(\"-\" * 60)\n",
    "nulos = df.isnull().sum()\n",
    "if nulos.sum() == 0:\n",
    "    print(\"‚úì No hay valores nulos en el dataset\")\n",
    "else:\n",
    "    print(nulos[nulos > 0])\n",
    "\n",
    "# An√°lisis de duplicados\n",
    "print(\"\\n6. AN√ÅLISIS DE DUPLICADOS:\")\n",
    "print(\"-\" * 60)\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {duplicados} ({duplicados/len(df)*100:.2f}%)\")\n",
    "if duplicados > 0:\n",
    "    print(f\"Filas √∫nicas: {len(df) - duplicados}\")\n",
    "\n",
    "# An√°lisis cruzado entre variables\n",
    "print(\"\\n7. AN√ÅLISIS CRUZADO: Sentiment vs Etiqueta\")\n",
    "print(\"-\" * 60)\n",
    "crosstab = pd.crosstab(df['Sentiment'], df['Etiqueta'], margins=True)\n",
    "print(\"\\nConteo absoluto:\")\n",
    "print(crosstab)\n",
    "print(\"\\nPorcentaje por fila:\")\n",
    "crosstab_pct = pd.crosstab(df['Sentiment'], df['Etiqueta'], normalize='index') * 100\n",
    "print(crosstab_pct.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd023f0",
   "metadata": {},
   "source": [
    "## 2. Visualizaciones del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones del dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZACIONES DEL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Distribuciones principales\n",
    "print(\"\\n1. DISTRIBUCIONES PRINCIPALES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Distribuciones del Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Gr√°fico de sentimientos\n",
    "sns.countplot(x='Sentiment', data=df, ax=axes[0], order=df['Sentiment'].value_counts().index)\n",
    "axes[0].set_title('Distribuci√≥n de Sentimientos')\n",
    "axes[0].set_xlabel('Sentimiento')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Gr√°fico de idiomas (top 5)\n",
    "top_idiomas = df['Idioma'].value_counts().head(5).index\n",
    "df_top_idiomas = df[df['Idioma'].isin(top_idiomas)]\n",
    "sns.countplot(data=df_top_idiomas, y='Idioma', order=top_idiomas, ax=axes[1], palette='viridis')\n",
    "axes[1].set_title('Top 5 Idiomas')\n",
    "axes[1].set_xlabel('Frecuencia')\n",
    "\n",
    "# Gr√°fico de etiquetas\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "sns.countplot(data=df, x='Etiqueta', ax=axes[2], palette=colors)\n",
    "axes[2].set_title('Distribuci√≥n de Etiquetas')\n",
    "axes[2].set_xlabel('Etiqueta')\n",
    "axes[2].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/01_distribuciones.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 01_distribuciones.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. An√°lisis cruzado\n",
    "print(\"\\n2. AN√ÅLISIS CRUZADO\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "fig.suptitle('Relaci√≥n Sentimiento vs Etiqueta', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Crear heatmap de porcentajes\n",
    "pivot_pct = pd.crosstab(df['Sentiment'], df['Etiqueta'], normalize='index') * 100\n",
    "sns.heatmap(pivot_pct, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax, \n",
    "            cbar_kws={'label': 'Porcentaje (%)'})\n",
    "ax.set_title('Porcentaje por Sentimiento')\n",
    "ax.set_xlabel('Etiqueta')\n",
    "ax.set_ylabel('Sentimiento')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/02_heatmap_sentiment_etiqueta.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 02_heatmap_sentiment_etiqueta.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. An√°lisis de longitud de oraciones\n",
    "print(\"\\n3. AN√ÅLISIS DE LONGITUD\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('An√°lisis de Longitud de Oraciones', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histograma de longitudes\n",
    "sns.histplot(df['sentence_length'], bins=30, kde=True, ax=axes[0])\n",
    "media = df['sentence_length'].mean()\n",
    "axes[0].axvline(media, color='red', linestyle='--', label=f'Media: {media:.1f}')\n",
    "axes[0].set_title('Distribuci√≥n de Longitud')\n",
    "axes[0].set_xlabel('N√∫mero de Palabras')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot por sentimiento\n",
    "sns.boxplot(x='Sentiment', y='sentence_length', data=df, ax=axes[1])\n",
    "axes[1].set_title('Longitud por Sentimiento')\n",
    "axes[1].set_xlabel('Sentimiento')\n",
    "axes[1].set_ylabel('N√∫mero de Palabras')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "plt.savefig('charts/03_analisis_longitud.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Guardado: 03_analisis_longitud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN DE VISUALIZACIONES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì 01_distribuciones.png\")\n",
    "print(\"‚úì 02_heatmap_sentiment_etiqueta.png\")\n",
    "print(\"‚úì 03_analisis_longitud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34d558",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Datos - Tokenizaci√≥n y Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "try:\n",
    "    print(\"Verificando recursos NLTK esenciales...\")\n",
    "    nltk.download('punkt', quiet=True, raise_on_error=False)\n",
    "    nltk.download('stopwords', quiet=True, raise_on_error=False)\n",
    "    print(\"‚úì Recursos NLTK (punkt, stopwords) listos.\")\n",
    "except Exception:\n",
    "    print(\"ATENCI√ìN: La descarga de recursos de NLTK fall√≥. Usaremos tokenizaci√≥n simple.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Tokenizaci√≥n simple como alternativa\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESAMIENTO DE DATOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data = 'data/dataset_multiidioma.csv'\n",
    "\n",
    "# Cargar stopwords en m√∫ltiples idiomas\n",
    "try:\n",
    "    STOPWORDS_ES = set(stopwords.words('spanish'))\n",
    "    STOPWORDS_EN = set(stopwords.words('english'))\n",
    "    STOPWORDS_ALL = STOPWORDS_ES.union(STOPWORDS_EN) \n",
    "except LookupError:\n",
    "    STOPWORDS_ALL = set()\n",
    "\n",
    "df_processed = pd.read_csv(data).copy()\n",
    "\n",
    "# Preprocesar texto: limpieza, tokenizaci√≥n y eliminaci√≥n de stopwords\n",
    "def preprocess_text(text, stopwords_set, min_len=2):\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return []\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Eliminar URLs y menciones\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenizar y convertir a min√∫sculas\n",
    "    try:\n",
    "        tokens = word_tokenize(text.lower()) \n",
    "    except LookupError:\n",
    "        tokens = simple_tokenize(text)\n",
    "\n",
    "    # Eliminar puntuaci√≥n, stopwords y tokens cortos\n",
    "    tokens_final = [\n",
    "        token for token in tokens\n",
    "        if token not in string.punctuation \n",
    "        and token not in stopwords_set\n",
    "        and len(token) > min_len\n",
    "    ]\n",
    "    \n",
    "    return tokens_final\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "print(\"\\n1. APLICANDO PROCESO DE LIMPIEZA...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_processed['tokens_processed'] = df_processed['Sentence'].apply(\n",
    "    lambda x: preprocess_text(x, STOPWORDS_ALL)\n",
    ")\n",
    "\n",
    "df_processed['tokens_original'] = df_processed['Sentence'].apply(\n",
    "     lambda x: simple_tokenize(str(x)) if not pd.isna(x) else []\n",
    ")\n",
    "\n",
    "print(\"‚úì Preprocesamiento completado en la columna 'tokens_processed'\")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTADOS DEL PREPROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ Columna de tokens procesados (Primeras 5 filas):\")\n",
    "print(df_processed[['Sentence', 'tokens_processed']].head())\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ejemplo de transformaci√≥n\n",
    "if len(df_processed) > 0:\n",
    "    print(\"\\nEjemplo de transformaci√≥n de la primera fila:\")\n",
    "    print(f\"  Texto Original: {df['Sentence'].iloc[0]}\")\n",
    "    print(f\"  Tokens Finales: {df_processed['tokens_processed'].iloc[0]}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular estad√≠sticas de reducci√≥n\n",
    "df_processed['num_tokens_original'] = df_processed['tokens_original'].apply(len)\n",
    "df_processed['num_tokens_processed'] = df_processed['tokens_processed'].apply(len)\n",
    "\n",
    "mean_original = df_processed['num_tokens_original'].mean()\n",
    "mean_processed = df_processed['num_tokens_processed'].mean()\n",
    "reduction_percentage = ((mean_original - mean_processed) / mean_original * 100) if mean_original > 0 else 0\n",
    "\n",
    "print(f\"\\nEstad√≠sticas de Longitud:\")\n",
    "print(f\"  Promedio de tokens originales: {mean_original:.2f}\")\n",
    "print(f\"  Promedio de tokens procesados: {mean_processed:.2f}\")\n",
    "print(f\"  Reducci√≥n de ruido promedio: **{reduction_percentage:.2f}%**\")\n",
    "\n",
    "# Guardar resultado\n",
    "df_processed.to_csv('data_processed/datos_preprocesados_simple.csv', index=False)\n",
    "print(f\"\\n‚úì Datos guardados en 'data_processed/datos_preprocesados_simple.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca1727",
   "metadata": {},
   "source": [
    "## 4. Lemmatization y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c11c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "try:\n",
    "    print(\"Verificando recursos NLTK esenciales (wordnet)...\")\n",
    "    nltk.download('wordnet', quiet=True, raise_on_error=False)\n",
    "    print(\"‚úì Recurso 'wordnet' listo.\")\n",
    "except Exception:\n",
    "    print(\"ATENCI√ìN: El recurso 'wordnet' de NLTK no est√° disponible.\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEMMATIZATION Y STEMMING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar datos\n",
    "df_processed = pd.read_csv('data_processed/datos_preprocesados_simple.csv').copy()\n",
    "\n",
    "# Convertir strings de listas a listas reales\n",
    "print(\"\\n1. CONVIRTIENDO TOKENS (STRING ‚Üí LISTA)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def convert_to_list(value):\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            result = ast.literal_eval(value)\n",
    "            if isinstance(result, list):\n",
    "                return result\n",
    "        except:\n",
    "            return value.split()\n",
    "    return []\n",
    "\n",
    "# Aplicar conversi√≥n a las columnas de tokens\n",
    "df_processed['tokens_processed'] = df_processed['tokens_processed'].apply(convert_to_list)\n",
    "df_processed['tokens_original'] = df_processed['tokens_original'].apply(convert_to_list)\n",
    "\n",
    "# Verificar conversi√≥n\n",
    "sample = df_processed['tokens_processed'].iloc[0]\n",
    "if isinstance(sample, list) and len(sample) > 0 and isinstance(sample[0], str):\n",
    "    vocab_real = len(set(token for tokens in df_processed['tokens_processed'] for token in tokens))\n",
    "    print(f\"‚úì Conversi√≥n exitosa\")\n",
    "    print(f\"  Tipo: {type(sample)}\")\n",
    "    print(f\"  Ejemplo: {sample[:5]}\")\n",
    "    print(f\"  Vocabulario real: {vocab_real} palabras\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ADVERTENCIA: La conversi√≥n puede tener problemas\")\n",
    "\n",
    "# Inicializar herramientas de stemming y lemmatization\n",
    "porter_stemmer = PorterStemmer()\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Aplicar stemming\n",
    "print(\"\\n2. STEMMING (Porter Stemmer)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_processed['tokens_stemmed'] = df_processed['tokens_processed'].apply(\n",
    "    lambda tokens: [porter_stemmer.stem(token) for token in tokens] if isinstance(tokens, list) else []\n",
    ")\n",
    "print(\"‚úì Stemming completado\")\n",
    "\n",
    "# Aplicar lemmatization\n",
    "print(\"\\n3. LEMMATIZATION (WordNet Lemmatizer)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_processed['tokens_lemmatized'] = df_processed['tokens_processed'].apply(\n",
    "    lambda tokens: [word_lemmatizer.lemmatize(token, pos='v') for token in tokens] if isinstance(tokens, list) else []\n",
    ")\n",
    "print(\"‚úì Lemmatization completada\")\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\n4. COMPARACI√ìN DE RESULTADOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if not df_processed.empty and len(df_processed['tokens_processed'].iloc[0]) > 0:\n",
    "    original = df_processed['tokens_processed'].iloc[0][:3]\n",
    "    stemmed = df_processed['tokens_stemmed'].iloc[0][:3]\n",
    "    lemmatized = df_processed['tokens_lemmatized'].iloc[0][:3]\n",
    "    \n",
    "    print(f\"{'Token Original':<20} {'Stemming':<20} {'Lemmatization':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(min(len(original), 3)):\n",
    "        print(f\"{original[i]:<20} {stemmed[i]:<20} {lemmatized[i]:<20}\")\n",
    "\n",
    "# Calcular tama√±o de vocabulario despu√©s de cada t√©cnica\n",
    "vocab_original = set(token for tokens in df_processed['tokens_processed'] for token in tokens if isinstance(tokens, list))\n",
    "vocab_stemmed = set(token for tokens in df_processed['tokens_stemmed'] for token in tokens if isinstance(tokens, list))\n",
    "vocab_lemmatized = set(token for tokens in df_processed['tokens_lemmatized'] for token in tokens if isinstance(tokens, list))\n",
    "\n",
    "print(f\"\\nTama√±o del vocabulario base: {len(vocab_original)}\")\n",
    "print(f\"Tama√±o despu√©s de Stemming: {len(vocab_stemmed)} ({((len(vocab_original) - len(vocab_stemmed)) / len(vocab_original) * 100):.2f}% reducci√≥n)\")\n",
    "print(f\"Tama√±o despu√©s de Lemmatization: {len(vocab_lemmatized)} ({((len(vocab_original) - len(vocab_lemmatized)) / len(vocab_original) * 100):.2f}% reducci√≥n)\")\n",
    "\n",
    "# Convertir tokens a texto para compatibilidad\n",
    "df_processed['text_stemmed'] = df_processed['tokens_stemmed'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "df_processed['text_lemmatized'] = df_processed['tokens_lemmatized'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "df_processed['text_processed_base'] = df_processed['tokens_processed'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "df_processed.to_csv('data_processed/datos_preprocesados_completo.csv', index=False)\n",
    "print(f\"\\n‚úì Datos guardados en 'data_processed/datos_preprocesados_completo.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abc521",
   "metadata": {},
   "source": [
    "## 5. Representaci√≥n Tradicional: Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACI√ìN: BAG OF WORDS (BoW)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar columna de texto procesado\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# Crear vectorizador con par√°metros para reducir ruido\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=5000, \n",
    "    min_df=2, \n",
    "    max_df=0.8, \n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Crear matriz BoW\n",
    "X_bow = bow_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úì Matriz BoW creada. Forma: {X_bow.shape}\")\n",
    "print(f\"Tama√±o del Vocabulario Final: {X_bow.shape[1]}\")\n",
    "\n",
    "# Calcular frecuencias de t√©rminos\n",
    "print(\"\\n2. AN√ÅLISIS DE FRECUENCIAS (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "word_freq = np.asarray(X_bow.sum(axis=0)).flatten()\n",
    "freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False).head(15)\n",
    "\n",
    "print(freq_df)\n",
    "\n",
    "# Visualizar t√©rminos m√°s frecuentes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='frequency', y='word', data=freq_df)\n",
    "plt.title(f'Top 15 T√©rminos M√°s Frecuentes (BoW)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/bow_top_terms.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analizar t√©rminos clave por sentimiento\n",
    "print(\"\\n3. T√âRMINOS CLAVE POR SENTIMIENTO (Top 5 por Clase)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sentiment in df_processed['Sentiment'].unique():\n",
    "    # Filtrar documentos por sentimiento\n",
    "    docs_sentiment = df_processed[df_processed['Sentiment'] == sentiment][TEXT_COLUMN]\n",
    "    \n",
    "    # Vectorizar documentos\n",
    "    X_sentiment = bow_vectorizer.transform(docs_sentiment)\n",
    "    \n",
    "    # Calcular frecuencias\n",
    "    freq_sentiment = np.asarray(X_sentiment.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Mostrar top 5 t√©rminos\n",
    "    freq_df_sentiment = pd.DataFrame({\n",
    "        'word': feature_names,\n",
    "        'frequency': freq_sentiment\n",
    "    }).sort_values('frequency', ascending=False).head(5)\n",
    "    \n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(freq_df_sentiment)\n",
    "\n",
    "# Preparar datos finales para el modelo\n",
    "print(\"\\n4. PREPARACI√ìN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Convertir matriz dispersa a DataFrame\n",
    "X_dense = X_bow.toarray()\n",
    "bow_final_df = pd.DataFrame(X_dense, columns=feature_names)\n",
    "\n",
    "# A√±adir columnas de etiquetas\n",
    "bow_final_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "bow_final_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "# Guardar\n",
    "bow_final_df.to_csv('data_processed/datos_vectorizados_final.csv', index=False)\n",
    "print(f\"‚úì Matriz de caracter√≠sticas guardada en 'data_processed/datos_vectorizados_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f2760",
   "metadata": {},
   "source": [
    "## 6. Representaci√≥n Tradicional: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTORIZACI√ìN: TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar columna de texto procesado\n",
    "TEXT_COLUMN = 'text_lemmatized'\n",
    "\n",
    "# Crear vectorizador TF-IDF\n",
    "print(\"\\n1. CREANDO MATRIZ TF-IDF\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Crear matriz TF-IDF\n",
    "try:\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_processed[TEXT_COLUMN])\n",
    "except NameError:\n",
    "    print(\"ERROR: El DataFrame 'df_processed' no est√° definido. Aseg√∫rate de cargar los datos antes.\")\n",
    "    sys.exit()\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úì Modelo TF-IDF creado. Forma: {X_tfidf.shape}\")\n",
    "print(f\"Tama√±o del Vocabulario Final: {X_tfidf.shape[1]}\")\n",
    "\n",
    "# Analizar t√©rminos con mayor peso TF-IDF\n",
    "print(\"\\n2. T√âRMINOS CON MAYOR PESO TF-IDF PROMEDIO (TOP 15)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calcular peso promedio de cada t√©rmino\n",
    "tfidf_means = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "tfidf_df = pd.DataFrame({\n",
    "    'term': tfidf_feature_names,\n",
    "    'tfidf_mean': tfidf_means\n",
    "}).sort_values('tfidf_mean', ascending=False).head(15)\n",
    "\n",
    "print(tfidf_df)\n",
    "\n",
    "# Preparar datos finales\n",
    "print(\"\\n3. PREPARACI√ìN FINAL DE DATOS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Convertir matriz dispersa a DataFrame\n",
    "tfidf_matrix_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_feature_names)\n",
    "tfidf_matrix_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "\n",
    "# Guardar\n",
    "tfidf_matrix_df.to_csv('data_processed/datos_tfidf_final.csv', index=False)\n",
    "print(f\"‚úì Matriz de caracter√≠sticas guardada en 'data_processed/datos_tfidf_final.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d0e81",
   "metadata": {},
   "source": [
    "## 7. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings usando Word2Vec y FastText\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORD EMBEDDINGS NO CONTEXTUALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar datos procesados\n",
    "df_processed = pd.read_csv('data_processed/datos_preprocesados_completo.csv')\n",
    "\n",
    "# Convertir tokens a listas\n",
    "import ast\n",
    "\n",
    "def ensure_list(value):\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except:\n",
    "            return value.split()\n",
    "    return []\n",
    "\n",
    "df_processed['tokens_lemmatized'] = df_processed['tokens_lemmatized'].apply(ensure_list)\n",
    "sentences = df_processed['tokens_lemmatized'].tolist()\n",
    "\n",
    "sample = sentences[0]\n",
    "print(f\"\\nVerificaci√≥n inicial:\")\n",
    "print(f\"  Tipo: {type(sample)}\")\n",
    "print(f\"  Ejemplo: {sample[:5] if len(sample) > 0 else 'vac√≠o'}\")\n",
    "\n",
    "# WORD2VEC\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. WORD2VEC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nEntrenando modelo Word2Vec...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "vocab_size = len(w2v_model.wv)\n",
    "print(f\"‚úì Modelo entrenado\")\n",
    "print(f\"  Vocabulario: {vocab_size} palabras\")\n",
    "print(f\"  Dimensi√≥n: {w2v_model.wv.vector_size}\")\n",
    "\n",
    "# Calcular palabras fuera de vocabulario (OOV)\n",
    "oov_count = sum(1 for tokens in sentences for token in tokens if token not in w2v_model.wv)\n",
    "total_tokens = sum(len(tokens) for tokens in sentences)\n",
    "oov_percentage = (oov_count / total_tokens * 100) if total_tokens > 0 else 0\n",
    "\n",
    "print(f\"\\nAn√°lisis OOV:\")\n",
    "print(f\"  Palabras fuera de vocabulario: {oov_count:,}/{total_tokens:,}\")\n",
    "print(f\"  Porcentaje OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Generar vectores de documentos promediando los vectores de palabras\n",
    "def get_document_vector_w2v(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "print(\"\\nGenerando vectores de documentos...\")\n",
    "doc_vectors_w2v = df_processed['tokens_lemmatized'].apply(\n",
    "    lambda tokens: get_document_vector_w2v(tokens, w2v_model)\n",
    ")\n",
    "\n",
    "X_w2v = np.vstack(doc_vectors_w2v.values)\n",
    "w2v_df = pd.DataFrame(X_w2v, columns=[f'w2v_{i}' for i in range(X_w2v.shape[1])])\n",
    "w2v_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "w2v_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "w2v_df.to_csv('data_processed/datos_word2vec.csv', index=False)\n",
    "w2v_model.save('models/word2vec_model.model')\n",
    "print(\"‚úì Vectores Word2Vec guardados\")\n",
    "\n",
    "# Ejemplo de palabras similares\n",
    "if vocab_size > 1000:\n",
    "    try:\n",
    "        test_words = ['company', 'profit', 'loss', 'market']\n",
    "        print(\"\\nEjemplos de palabras similares:\")\n",
    "        for word in test_words:\n",
    "            if word in w2v_model.wv:\n",
    "                similar = w2v_model.wv.most_similar(word, topn=3)\n",
    "                print(f\"  {word}: {[w for w, s in similar]}\")\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# FASTTEXT\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. FASTTEXT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nEntrenando modelo FastText...\")\n",
    "ft_model = FastText(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "vocab_size_ft = len(ft_model.wv)\n",
    "print(f\"‚úì Modelo entrenado\")\n",
    "print(f\"  Vocabulario: {vocab_size_ft} palabras\")\n",
    "print(f\"  Dimensi√≥n: {ft_model.wv.vector_size}\")\n",
    "\n",
    "print(f\"\\nVentaja de FastText:\")\n",
    "print(f\"  ‚úì Todas las palabras tienen representaci√≥n (incluso OOV)\")\n",
    "print(f\"  ‚úì Usa subwords para palabras desconocidas\")\n",
    "print(f\"  Total de tokens procesados: {total_tokens:,}\")\n",
    "\n",
    "# Generar vectores de documentos (FastText no tiene problema con OOV)\n",
    "def get_document_vector_ft(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "print(\"\\nGenerando vectores de documentos...\")\n",
    "doc_vectors_ft = df_processed['tokens_lemmatized'].apply(\n",
    "    lambda tokens: get_document_vector_ft(tokens, ft_model)\n",
    ")\n",
    "\n",
    "X_ft = np.vstack(doc_vectors_ft.values)\n",
    "ft_df = pd.DataFrame(X_ft, columns=[f'ft_{i}' for i in range(X_ft.shape[1])])\n",
    "ft_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "ft_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "ft_df.to_csv('data_processed/datos_fasttext.csv', index=False)\n",
    "ft_model.save('models/fasttext_model.model')\n",
    "print(\"‚úì Vectores FastText guardados\")\n",
    "\n",
    "# Ejemplo de palabras similares\n",
    "if vocab_size_ft > 1000:\n",
    "    try:\n",
    "        test_words = ['company', 'profit', 'loss', 'market']\n",
    "        print(\"\\nEjemplos de palabras similares:\")\n",
    "        for word in test_words:\n",
    "            similar = ft_model.wv.most_similar(word, topn=3)\n",
    "            print(f\"  {word}: {[w for w, s in similar]}\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN DE EMBEDDINGS NO CONTEXTUALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úì Word2Vec: {vocab_size:,} palabras, {oov_percentage:.2f}% OOV\")\n",
    "print(f\"‚úì FastText: {vocab_size_ft:,} palabras, 0% OOV (usa subwords)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert_section",
   "metadata": {},
   "source": [
    "## 8. Word Embeddings Contextuales con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bert_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings contextuales usando BERT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORD EMBEDDINGS CONTEXTUALES - BERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar datos\n",
    "df_processed = pd.read_csv('data_processed/datos_preprocesados_completo.csv')\n",
    "\n",
    "# Cargar modelo BERT multiling√ºe\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "print(f\"\\nCargando modelo: {model_name}\")\n",
    "print(\"(Este proceso puede tardar unos minutos la primera vez)\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Usar GPU si est√° disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úì Modelo cargado en: {device}\")\n",
    "print(f\"  Vocabulario: {len(tokenizer)} tokens\")\n",
    "print(f\"  Dimensi√≥n de embeddings: {model.config.hidden_size}\")\n",
    "\n",
    "# Obtener embedding de BERT usando el token [CLS]\n",
    "def get_bert_embedding(text, tokenizer, model, device, max_length=128):\n",
    "    # Tokenizar texto\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                      padding=True, max_length=max_length)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Obtener embeddings sin calcular gradientes\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Usar el embedding del token [CLS] que representa toda la oraci√≥n\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embedding.flatten()\n",
    "\n",
    "# Generar embeddings para todos los documentos\n",
    "print(f\"\\nGenerando embeddings para {len(df_processed)} documentos...\")\n",
    "print(\"(Esto puede tardar varios minutos)\")\n",
    "\n",
    "embeddings_list = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(df_processed), batch_size)):\n",
    "    batch = df_processed['Sentence'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    for text in batch:\n",
    "        embedding = get_bert_embedding(str(text), tokenizer, model, device)\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "# Crear DataFrame con embeddings\n",
    "X_bert = np.vstack(embeddings_list)\n",
    "bert_df = pd.DataFrame(X_bert, columns=[f'bert_{i}' for i in range(X_bert.shape[1])])\n",
    "bert_df['Sentiment'] = df_processed['Sentiment'].values\n",
    "bert_df['Idioma'] = df_processed['Idioma'].values\n",
    "\n",
    "# Guardar\n",
    "bert_df.to_csv('data_processed/datos_bert.csv', index=False)\n",
    "print(f\"\\n‚úì Embeddings BERT guardados\")\n",
    "print(f\"  Forma de la matriz: {X_bert.shape}\")\n",
    "print(f\"  Archivo: datos_bert.csv\")\n",
    "\n",
    "print(f\"\\nAn√°lisis OOV:\")\n",
    "print(f\"  ‚úì BERT no tiene palabras OOV\")\n",
    "print(f\"  ‚úì Usa subword tokenization (WordPiece)\")\n",
    "print(f\"  ‚úì Embeddings contextuales (var√≠a seg√∫n contexto)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARACI√ìN: NO CONTEXTUALES vs CONTEXTUALES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWord2Vec/FastText (No Contextuales):\")\n",
    "print(\"  ‚Ä¢ Cada palabra tiene UN SOLO vector\")\n",
    "print(\"  ‚Ä¢ No considera contexto\")\n",
    "print(\"  ‚Ä¢ Vocabulario limitado (problemas OOV)\")\n",
    "print(\"\\nBERT (Contextual):\")\n",
    "print(\"  ‚Ä¢ Cada palabra tiene M√öLTIPLES vectores seg√∫n contexto\")\n",
    "print(\"  ‚Ä¢ Considera contexto completo de la oraci√≥n\")\n",
    "print(\"  ‚Ä¢ Sin problemas OOV (subword tokenization)\")\n",
    "print(\"  ‚Ä¢ M√°s costoso computacionalmente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
